---
title: Connecting the Popularity Adjusted Block Model to the Generalized Random Dot Product Graph
subtitle: SDSS 2021 Lightning Presentation
author: John Koo
date: 'June 2021'
output: 
  beamer_presentation:
    fig_crop: no
    theme: 'default'
    colortheme: 'beaver'
    includes:
      in_header: page_headers.tex
header-includes:
- \usepackage{setspace}
- \usepackage{float}
- \usepackage{mathtools}
- \usepackage{natbib}
- \usepackage[linesnumbered,ruled,vlined]{algorithm2e} 
- \setcitestyle{numbers,square,comma}
- \usepackage{verbatim}
- \usepackage{amsthm}
- \usepackage{comment}
- \setbeamertemplate{itemize items}[circle]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE,
                      fig.align = 'center',
                      fig.lp = '')
options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')
library(ggplot2)
import::from(magrittr, `%>%`)
theme_set(theme_bw())

source('~/dev/pabm-grdpg/functions.R')
set.seed(314159)
```

## Overview

<style type="text/css">
.caption {
    font-size: x-small;
}
</style>

1. Block Models and the Popularity Adjusted Block Model

2. Generalized Random Dot Product Graphs

3. Connecting the PABM to the GRDPG

4. Community Detection for the PABM

# Block Models

## Networks

```{r out.width = '50%'}
knitr::include_graphics('Sna_large.png')
```

\tiny

By Screenshot taken by User:DarwinPeacock - Screenshot of free software GUESS, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=6057981

## Bernoulli Graphs

::: columns

:::: {.column width=62.5%}

Let $G = (V, E)$ be an undirected and unweighted graph with $|V| = n$.

$G$ is described by adjacency matrix $A$ such that
$A_{ij} = \begin{cases} 
1 & \exists \text{ edge between } i \text{ and } j \\
0 & \text{else}
\end{cases}$

$A_{ji} = A_{ij}$ and $A_{ii} = 0$ $\forall i, j \in [n]$.

$A \sim BernoulliGraph(P)$ iff:

1. $P \in [0, 1]^{n \times n}$ describes edge probabilities between pairs of 
vertices.
2. $A_{ij} \stackrel{ind}{\sim} Bernoulli(P_{ij})$ for each $i < j$.

::::

:::: {.column width=37.5%}

```{r, fig.height = 2, fig.width = 2}
n <- 2 ** 5
p <- 1 / log(n)
P <- matrix(p, nrow = n, ncol = n)
A <- draw.graph(P)
qgraph::qgraph(A, vsize = 4)
```

::::

:::

## Block Models

Suppose each vertex $v_1, ..., v_n$ has hidden labels $z_1, ..., z_n \in [K]$,  
and each $P_{ij}$ depends on labels $z_i$ and $z_j$.  
Then $A \sim BernoulliGraph(P)$ is a *block model*.

Example: Stochastic Block Model (Lorrain and White, 1971) with two communities

::: columns

:::: column

* $z_1, ..., z_n \in \{1, 2\}$
* $P_{ij} = \begin{cases} 
p & z_i = z_j = 1 \\
q & z_i = z_j = 2 \\
r & z_i \neq z_j
\end{cases}$

* To make this an assortative SBM, set $p q > r^2$.
* In this example, $p = 1/2$, $q = 1/4$, and $r = 1/8$.

::::

:::: column

```{r, fig.height = 3, fig.width = 4, out.width = '100%'}
n1 <- 2 ** 5
n2 <- 2 ** 5
n <- n1 + n2
z <- c(rep(1, n1), rep(2, n2))
p <- 1/2
q <- 1/4
r <- 1/8
P <- matrix(r, nrow = n, ncol = n)
P[seq(n1), seq(n1)] <- p
P[seq(n1 + 1, n), seq(n1 + 1, n)] <- q
A <- draw.graph(P)
qgraph::qgraph(A, vsize = 4, groups = factor(z))
```

::::

:::

## Popularity Adjusted Block Model

Definition based on Noroozi, Rimal, and Pensky (2020); 
model first proposed by Sengupta and Chen (2017).

$A \sim PABM(\{\lambda^{(kl)}\}_K)$ iff 

1. w.l.o.g., organize $P$ such that each block 
$P^{(kl)} \in [0, 1]^{n_k \times n_l}$ contains edge 
probabilities between communities $k$ and $l$.
2. Organize parameters as vectors such that 
$\lambda^{(kl)} \in \mathbb{R}^{n_k}$ 
are the popularity parameters of members of community $k$ to community $l$.  
$\{\lambda^{(kl)}\}_K$ is the set of $K^2$ popularity vectors.
3. Then we can write each block of $P$ as 
$P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top$.
4. Sample $A \sim BernoulliGraph(P)$.

Example: $K = 2$

$$P = \begin{bmatrix}
P^{(11)} & P^{(12)} \\
P^{(21)} & P^{(22)}
\end{bmatrix} = 
\begin{bmatrix}
\lambda^{(11)} (\lambda^{(11)})^\top & \lambda^{(12)} (\lambda^{(21)})^\top \\
\lambda^{(21)} (\lambda^{(12)})^\top & \lambda^{(22)} (\lambda^{(22)})^\top
\end{bmatrix}$$

# Generalized Ranodm Dot Product Graphs

## Generalized Random Dot Product Graph

Generalized Random Dot Product Graph $A \sim GRDPG_{p, q}(X)$  
(Rubin-Delanchy, Cape, Tang, Priebe, 2020)

* Latent vectors $x_1, ..., x_n \in \mathbb{R}^{p+q}$ such that 
$x_i^\top I_{p, q} x_j \in [0, 1]$ and $I_{p, q} = blockdiag(I_p, -I_q)$
* $A \sim BernoulliGraph(X I_{p, q} X^\top)$ where 
$X = \begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix}^\top$

If latent vectors $X_1, ..., X_n \stackrel{iid}{\sim} F$, then we write 
$(A, X) \sim GRDPG_{p, q}(F, n)$.

## (Generalized) Random Dot Product Graph Model

### Recovery/Estimation

Want to estimate $X$ given $A$, or alternatively, 
interpoint distances, inner products, or angles.

### Adjacency Spectral Embedding

To embed in $\mathbb{R}^{p+q}$, 

1. Compute $A \approx \hat{V} \hat{\Lambda} \hat{V}^\top$ 
where $\hat{\Lambda} \in \mathbb{R}^{(p+q) \times (p+q)}$ and 
$\hat{V} \in \mathbb{R}^{n \times (p+q)}$ by using 
$p$ most positive and $q$ most negative eigenvalues.

2. Let $\hat{X} = \hat{V} |\hat{\Lambda}|^{1/2}$.

\vspace*{.5\baselineskip}

$$\max_i \|\hat{X}_i - Q_n X_i \| \stackrel{a.s.}{\to} 0
\text{ (Rubin-Delanchy et al., 2020)}$$

$$Q_n \in \mathbb{O}(p, q)$$

## Connecting Block Models to the GRDPG

::: columns

:::: column

```{r, out.width = '100%', fig.height = 4, fig.width = 4}
par(mar = rep(3, 4))
P.eigen = eigen(P)
X <- P.eigen$vectors[, 1:2] %*% diag(P.eigen$values[1:2] ** .5)
plot(X, asp = 1, col = z, xlab = NA, ylab = NA, main = 'SBM: Point masses')
```

::::

:::: column

```{r, out.width = '100%', fig.height = 4, fig.width = 4}
par(mar = rep(1.75, 4))
Pz <- generate.P.beta(n)
P <- Pz$P
z <- Pz$clustering
X <- embedding(P)
pairs(X, col = z, asp = 1, pch = '.', main = 'PABM: Orthogonal subspaces')
```

::::

:::

# Connecting the PABM to the GRDPG

## Connecting the PABM to the GRDPG ($K = 2$)

**Theorem** (KTT): $A \sim PABM(\{\lambda^{(kl)}\}_2)$ is equivalent to 
$A \sim GRDPG_{3, 1}(X U)$ for block diagonal $X$ 
constructed from $\{\lambda^{(kl)}\}_2$ and 
predetermined $U \in \mathbb{O}(4)$.

Proof: Decompose $P$ as follows

$$X = \begin{bmatrix}
\lambda^{(11)} & \lambda^{(12)} & 0 & 0 \\
0 & 0 & \lambda^{(21)} & \lambda^{(22)}
\end{bmatrix}$$

$$U = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 0 & 1 / \sqrt{2} & 1 / \sqrt{2} \\
0 & 0 & 1 / \sqrt{2} & -1 / \sqrt{2} \\
0 & 1 & 0 & 0
\end{bmatrix}$$

$$P = (X U) I_{3, 1} (X U)^\top = \begin{bmatrix}
\lambda^{(11)} (\lambda^{(11)})^\top & \lambda^{(12)} (\lambda^{(21)})^\top \\
\lambda^{(21)} (\lambda^{(12)})^\top & \lambda^{(22)} (\lambda^{(22)})^\top
\end{bmatrix}$$

## Connecting the PABM to the GRDPG ($K \geq 2$)

**Theorem** (KTT): $A \sim PABM(\{\lambda^{(kl)}\}_K)$ is equivalent to 
$A \sim GRDPG_{p, q}(X U)$ such that

* $p = K (K + 1) / 2$
* $q = K (K - 1) / 2$
* $U$ is orthogonal and predetermined for each $K$
* $X$ is block diagonal and composed of $\{\lambda^{(kl)}\}_K$  
$\implies$ if $x_i^\top$ and $x_j^\top$ are two rows of $X U$ corresponding 
to different communities, then $x_i^\top x_j = 0$.

**Remark** (non-uniqueness of the latent configuration):  
$A \sim GRDPG_{p, q}(X U) \implies A \sim GRDPG_{p, q}(X U Q)$ 
$\forall Q \in \mathbb{O}(p, q)$

# Community Detection for the PABM

## Sparse Subspace Clustering

**Corollary**: $X$ is block diagonal by community and 
$U$ is orthogonal $\implies$ 
each community corresponds to a subspace in $\mathbb{R}^{K^2}$.

Subspace property holds even with linear transformation 
$Q \in \mathbb{O}(p, q)$.

Noroozi et al. observed that the rank of $P$ is $K^2$ 
and the columns of $P$ belonging to each community has rank $K$ 
to justify SSC for the PABM.

$$\arg\min_{c_i} \|c_i\|_1 \text{ subject to } A_{\cdot, i} = A c_i
\text{ and } c_i^{(i)} = 0$$

GRDPG-based approach: Apply SSC to the ASE of $A$.

$$\arg\min_{c_i} \|c_i\|_1 \text{ subject to } 
\hat{x}_i = \hat{X} c_i \text{ and } c_i^{(i)} = 0$$
$$A \approx \hat{X} I_{p, q} \hat{X}^\top$$

## Sparse Subspace Clustering

**Theorem** (KTT): 
If $P = V \Lambda V^\top$ and $B = n V V^\top$, 
then $B_{ij} = 0$ $\forall i, j$ in different communities.

**Theorem** (KTT): 

Let 

* $P_n$ describe the edge probability matrix of the PABM with $n$ vertices, and 
$A_n \sim BernoulliGraph(P_n)$.
* $\hat{V}_n$ be the matrix of eigenvectors of $A_n$ corresponding to the 
$K (K + 1) / 2$ most positive and $K (K - 1) / 2$ most negative eigenvalues. 

Then 

* $\exists N < \infty$ such that when $n > N$, 
$\sqrt{n} \hat{V}_n$ obeys the Subspace Detection Property with probability 1.

## Orthogonal Spectral Clustering

**Theorem** (KTT): 
If $P = V \Lambda V^\top$ and $B = n V V^\top$, 
then $B_{ij} = 0$ $\forall i, j$ in different communities.

Orthogonal Spectral Clustering algorithm: 

1. Let $V$ be the eigenvectors of $A$ corresponding to the $K (K+1)/2$ most 
positive and $K (K-1) / 2$ most negative eigenvalues.
2. Compute $B = |n V V^\top|$ applying $|\cdot|$ entry-wise.
3. Construct graph $G$ using $B$ as its similarity matrix.
4. Partition $G$ into $K$ disconnected subgraphs.

**Theorem** (KTT): 
Let $\hat{B}_n$ with entries $\hat{B}_n^{(ij)}$ be the affinity matrix from OSC. 
Then $\forall$ pairs $(i, j)$ belonging to different communities 
and sparsity factor satisfying $n \rho_n = \omega\{(\log n)^{4c}\}$, 

$$
\max_{i, j} \hat{B}^{(ij)} = 
O_P \Big( \frac{(\log n)^c}{\sqrt{n \rho_n}} \Big)
$$

## Simulation Results

```{r clust_err_k, fig.width = 10, fig.height = 3, out.width = '100%'}
setwd('..')
clustering.df <- readr::read_csv('clustering-k.csv')
ssc.df <- readr::read_csv('clustering-ssc-k.csv')
clustering.df %>%
  dplyr::group_by(n, K) %>%
  dplyr::summarise(
    med.err = median(error),
    first.q = quantile(error, .25),
    third.q = quantile(error, .75),
    med.err.ssc = median(error.ssc),
    first.q.ssc = quantile(error.ssc, .25),
    third.q.ssc = quantile(error.ssc, .75),
    med.err.ep = median(error.ep, na.rm = TRUE),
    first.q.ep = quantile(error.ep, .25, na.rm = TRUE),
    third.q.ep = quantile(error.ep, .75, na.rm = TRUE),
    med.err.louvain = median(error.louvain),
    first.q.louvain = quantile(error.louvain, .25),
    third.q.louvain = quantile(error.louvain, .75)
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::inner_join(
    ssc.df %>% 
      dplyr::group_by(n, K) %>% 
      dplyr::summarise(med.err.ssc2 = median(error.ssc2),
                       first.q.ssc2 = quantile(error.ssc2, .25),
                       third.q.ssc2 = quantile(error.ssc2, .75)) %>% 
      dplyr::ungroup()
  ) %>% 
  ggplot() +
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096)) +
  # scale_x_continuous(breaks = c(128, 256, 512, 1024, 2048, 4096)) + 
  scale_y_log10() +
  labs(y = 'community detection error rate', 
       colour = NULL) +
  geom_line(aes(x = n, y = med.err,
                colour = 'OSC')) +
  geom_errorbar(aes(x = n, ymin = first.q, ymax = third.q,
                    colour = 'OSC'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc,
                colour = 'SSC-ASE')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc, ymax = third.q.ssc,
                    colour = 'SSC-ASE'), width = .1) + 
  # geom_line(aes(x = n, y = med.err.ep,
  #               colour = 'MM-EP')) + 
  # geom_errorbar(aes(x = n, ymin = first.q.ep, ymax = third.q.ep,
  #                   colour = 'MM-EP'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc2,
                colour = 'SSC-A')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc2, ymax = third.q.ssc2,
                    colour = 'SSC-A'), width = .1) + 
  geom_line(aes(x = n, y = med.err.louvain,
                colour = 'MM-Louvain')) + 
  geom_errorbar(aes(x = n, ymin = first.q.louvain, ymax = third.q.louvain,
                    colour = 'MM-Louvain'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  facet_wrap(~ K, labeller = 'label_both')
```

\tiny

IQR of community detection error rates using OSC (blue) compared against SSC on the ASE of A (purple), MM (red), and SSC on the adjacency matrix (green). Communities are approximately balanced. Simulations were repeated 50 times for each sample size.