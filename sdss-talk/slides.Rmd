---
title: Connecting the Popularity Adjusted Block Model to the Generalized Random Dot Product Graph
subtitle: SDSS 2021 Lightning Presentation
author: John Koo, Indiana University
date: 'June 2021'
output: 
  beamer_presentation:
    fig_crop: no
    theme: 'default'
    colortheme: 'beaver'
    includes:
      in_header: page_headers.tex
header-includes:
- \usepackage{setspace}
- \usepackage{float}
- \usepackage{mathtools}
- \usepackage{natbib}
- \usepackage[linesnumbered,ruled,vlined]{algorithm2e} 
- \setcitestyle{numbers,square,comma}
- \usepackage{verbatim}
- \usepackage{amsthm}
- \usepackage{comment}
- \setbeamertemplate{itemize items}[circle]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE,
                      fig.align = 'center',
                      fig.lp = '')
options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')
library(ggplot2)
import::from(magrittr, `%>%`)
theme_set(theme_bw())

source('~/dev/pabm-grdpg/functions.R')
set.seed(314159)
```

## Contributors

::: columns

:::: {.column width='33%'}

```{r, out.width = '100px'}
knitr::include_graphics('john_koo.jpeg')
```

John Koo,  
PhD Student in Statistical Science,  
Indiana University

::::

:::: {.column width='33%'}

```{r, out.width = '100px'}
knitr::include_graphics('minh_tang.jpg')
```

Minh Tang,  
Assistant Professor of Statistics,  
NC State University

::::

:::: {.column width='33%'}

```{r, out.width = '100px'}
knitr::include_graphics('mtrosset.jpg')
```

Michael Trosset,  
Professor of Statistics,  
Indiana University

::::

:::

## Overview

<style type="text/css">
.caption {
    font-size: x-small;
}
</style>

1. Block Models and the Popularity Adjusted Block Model

2. Generalized Random Dot Product Graphs

3. Connecting the PABM to the GRDPG

4. Community Detection for the PABM

# Block Models

## Networks

```{r out.width = '50%'}
knitr::include_graphics('Sna_large.png')
```

\tiny

Screenshot taken by User:DarwinPeacock - Screenshot of free software GUESS, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=6057981

## Bernoulli Graphs

::: columns

:::: {.column width=62.5%}

Let $G = (V, E)$ be an undirected and unweighted graph with $|V| = n$.

\vspace*{1\baselineskip}

$G$ is described by adjacency matrix $A$ such that
$A_{ij} = \begin{cases} 
1 & \exists \text{ edge between } i \text{ and } j \\
0 & \text{else}
\end{cases}$

$A_{ji} = A_{ij}$ and $A_{ii} = 0$ $\forall i, j \in [n]$.

\vspace*{1\baselineskip}

$A \sim BernoulliGraph(P)$ iff:

1. $P \in [0, 1]^{n \times n}$ describes edge probabilities between pairs of 
vertices.
2. $A_{ij} \stackrel{ind}{\sim} Bernoulli(P_{ij})$ for each $i < j$.

::::

:::: {.column width=37.5%}

```{r, fig.height = 2, fig.width = 2}
n <- 2 ** 5
p <- 1 / log(n)
P <- matrix(p, nrow = n, ncol = n)
A <- draw.graph(P)
qgraph::qgraph(A, vsize = 4)
```

::::

:::

## Block Models

Suppose each vertex $v_1, ..., v_n$ has hidden labels $z_1, ..., z_n \in [K]$,  
and each $P_{ij}$ depends on labels $z_i$ and $z_j$.  
Then $A \sim BernoulliGraph(P)$ is a *block model*.

Example: Stochastic Block Model (Lorrain and White, 1971) with two communities

::: columns

:::: column

* $z_1, ..., z_n \in \{1, 2\}$
* $P_{ij} = \begin{cases} 
p & z_i = z_j = 1 \\
q & z_i = z_j = 2 \\
r & z_i \neq z_j
\end{cases}$

* To make this an assortative SBM, set $p q > r^2$.
* In this example, $p = 1/2$, $q = 1/4$, and $r = 1/8$.

::::

:::: column

```{r, fig.height = 3, fig.width = 4, out.width = '100%'}
n1 <- 2 ** 5
n2 <- 2 ** 5
n <- n1 + n2
z <- c(rep(1, n1), rep(2, n2))
p <- 1/2
q <- 1/4
r <- 1/8
P <- matrix(r, nrow = n, ncol = n)
P[seq(n1), seq(n1)] <- p
P[seq(n1 + 1, n), seq(n1 + 1, n)] <- q
A <- draw.graph(P)
qgraph::qgraph(A, vsize = 4, groups = factor(z))
```

::::

:::

## Popularity Adjusted Block Model

Definition based on Noroozi, Rimal, and Pensky (2020); 
model first proposed by Sengupta and Chen (2017).

$A \sim PABM(\{\lambda^{(kl)}\}_K)$ iff 

1. w.l.o.g., organize $P$ such that each block 
$P^{(kl)} \in [0, 1]^{n_k \times n_l}$ contains edge 
probabilities between communities $k$ and $l$.
2. Organize parameters as vectors such that 
$\lambda^{(kl)} \in \mathbb{R}^{n_k}$ 
are the popularity parameters of members of community $k$ to community $l$.  
$\{\lambda^{(kl)}\}_K$ is the set of $K^2$ popularity vectors.
3. Then we can write each block of $P$ as 
$P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top$.
4. Sample $A \sim BernoulliGraph(P)$.

Example: $K = 2$

$$P = \begin{bmatrix}
P^{(11)} & P^{(12)} \\
P^{(21)} & P^{(22)}
\end{bmatrix} = 
\begin{bmatrix}
\lambda^{(11)} (\lambda^{(11)})^\top & \lambda^{(12)} (\lambda^{(21)})^\top \\
\lambda^{(21)} (\lambda^{(12)})^\top & \lambda^{(22)} (\lambda^{(22)})^\top
\end{bmatrix}$$

## Community Detection in Block Models

Likelihood

$$L = \prod_{i<j} \prod_{k, l}^K 
\big(p_{k, l, i, j}^{A_{ij}} 
(1 - p_{k, l, i, j})^{1 - A_{ij}} \big)^{z_{ik} z_{jl}}$$

* ML method for community detection: $\hat{\vec{z}} = \arg\max_{\vec{z}} L$

* NP-complete
  * Expectation-Maximization
  * Bayesian methods
  * **Spectral methods**

# Generalized Random Dot Product Graphs

## Generalized Random Dot Product Graph

Generalized Random Dot Product Graph $A \sim GRDPG_{p, q}(X)$  
(Rubin-Delanchy, Cape, Tang, Priebe, 2020)

* Latent vectors $x_1, ..., x_n \in \mathbb{R}^{p+q}$ such that 
$x_i^\top I_{p, q} x_j \in [0, 1]$ and $I_{p, q} = blockdiag(I_p, -I_q)$
* $A \sim BernoulliGraph(X I_{p, q} X^\top)$ where 
$X = \begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix}^\top$

If latent vectors $X_1, ..., X_n \stackrel{iid}{\sim} F$, then we write 
$(A, X) \sim GRDPG_{p, q}(F, n)$.

## (Generalized) Random Dot Product Graph Model

### Recovery/Estimation

Want to estimate $X$ given $A$.

### Adjacency Spectral Embedding

To embed in $\mathbb{R}^{p+q}$, 

1. Compute $A \approx \hat{V} \hat{\Lambda} \hat{V}^\top$ 
where $\hat{\Lambda} \in \mathbb{R}^{(p+q) \times (p+q)}$ and 
$\hat{V} \in \mathbb{R}^{n \times (p+q)}$ by using 
$p$ most positive and $q$ most negative eigenvalues.

2. Let $\hat{X} = \hat{V} |\hat{\Lambda}|^{1/2}$.

\vspace*{.5\baselineskip}

Rubin-Delanchy et al., 2020:  
$$\max_i \|\hat{X}_i - Q_n X_i \| \stackrel{a.s.}{\to} 0$$

$$Q_n \in \mathbb{O}(p, q)$$

## Connecting Block Models to the GRDPG

::: columns

:::: column

```{r, out.width = '100%', fig.height = 4, fig.width = 4}
par(mar = rep(3, 4))
P.eigen = eigen(P)
X <- P.eigen$vectors[, 1:2] %*% diag(P.eigen$values[1:2] ** .5)
plot(X, asp = 1, col = z, xlab = NA, ylab = NA, main = 'SBM: Point masses')
```

::::

:::: column

```{r, out.width = '100%', fig.height = 4, fig.width = 4}
par(mar = rep(1.75, 4))
Pz <- generate.P.beta(n)
P <- Pz$P
z <- Pz$clustering
X <- embedding(P)
pairs(X, col = z, asp = 1, pch = '.', main = 'PABM: Orthogonal subspaces')
```

::::

:::

# Connecting the PABM to the GRDPG

## Connecting the PABM to the GRDPG ($K = 2$)

**Theorem** (KTT): $A \sim PABM(\{\lambda^{(kl)}\}_2)$ is equivalent to 
$A \sim GRDPG_{3, 1}(X U)$ for block diagonal $X$ 
constructed from $\{\lambda^{(kl)}\}_2$ and 
predetermined $U \in \mathbb{O}(4)$.

Proof: Decompose $P$ as follows

$$X = \begin{bmatrix}
\lambda^{(11)} & \lambda^{(12)} & 0 & 0 \\
0 & 0 & \lambda^{(21)} & \lambda^{(22)}
\end{bmatrix}$$

$$U = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 0 & 1 / \sqrt{2} & 1 / \sqrt{2} \\
0 & 0 & 1 / \sqrt{2} & -1 / \sqrt{2} \\
0 & 1 & 0 & 0
\end{bmatrix}$$

$$P = (X U) I_{3, 1} (X U)^\top = \begin{bmatrix}
\lambda^{(11)} (\lambda^{(11)})^\top & \lambda^{(12)} (\lambda^{(21)})^\top \\
\lambda^{(21)} (\lambda^{(12)})^\top & \lambda^{(22)} (\lambda^{(22)})^\top
\end{bmatrix}$$

## Connecting the PABM to the GRDPG ($K \geq 2$)

**Theorem** (KTT): $A \sim PABM(\{\lambda^{(kl)}\}_K)$ is equivalent to 
$A \sim GRDPG_{p, q}(X U)$ such that

* $p = K (K + 1) / 2$
* $q = K (K - 1) / 2$
* $U$ is orthogonal and predetermined for each $K$
* $X$ is block diagonal and composed of $\{\lambda^{(kl)}\}_K$  
$\implies$ if $x_i^\top$ and $x_j^\top$ are two rows of $X U$ corresponding 
to different communities, then $x_i^\top x_j = 0$.

**Remark** (non-uniqueness of the latent configuration):  
$A \sim GRDPG_{p, q}(X U) \implies A \sim GRDPG_{p, q}(X U Q)$ 
$\forall Q \in \mathbb{O}(p, q)$

**Corollary**: $X$ is block diagonal by community and 
$U$ is orthogonal $\implies$ 
each community corresponds to a subspace in $\mathbb{R}^{K^2}$.

Subspace property holds even with linear transformation 
$Q \in \mathbb{O}(p, q)$.

# Community Detection for the PABM

## Sparse Subspace Clustering

*Sparse Subspace Clustering* (Elhamifar and Vidal, 2009):  
Let the rows of $X \in \mathbb{R}^{n \times d}$ are vectors that lie on or near a small number of low-dimensional subspaces. SSC constructs a graph from $X$ as follows:

1. For each $i \in [n]$, solve the optimization problem  
$c_i = \arg\min_{c} \|c\|_1$ subject to $x_i = X c$ and $c^{(i)} = 0$.
2. Construct a similarity graph using $B = |C| + |C^\top|$ as its affinity matrix, where $C = \begin{bmatrix} c_1 & \cdots & c_n \end{bmatrix}$.

If $X$ obeys the *Subspace Detection Property*, then it produces a similarity graph $B$ such that $B_{ij} = 0$ $\forall z_i \neq z_j$.

## Sparse Subspace Clustering for Community Detection

Noroozi et al. observed that the rank of $P$ is $K^2$ 
and the columns of $P$ belonging to each community has rank $K$ 
to justify SSC for the PABM.

$$c_i = \arg\min_{c} \|c\|_1 \text{ subject to } A_{\cdot, i} = A c
\text{ and } c^{(i)} = 0$$

GRDPG-based approach: Apply SSC to the ASE of $A$.

$$c_i = \arg\min_{c} \|c\|_1 \text{ subject to } 
\hat{x}_i = \hat{X} c \text{ and } c^{(i)} = 0$$
$$A \approx \hat{X} I_{p, q} \hat{X}^\top$$

## Sparse Subspace Clustering

**Theorem** (KTT): 

Let 

* $P_n$ describe the edge probability matrix of the PABM with $n$ vertices, and 
$A_n \sim BernoulliGraph(P_n)$.
* $\hat{V}_n$ be the matrix of eigenvectors of $A_n$ corresponding to the 
$K (K + 1) / 2$ most positive and $K (K - 1) / 2$ most negative eigenvalues. 

Then 

* $\exists N < \infty$ such that when $n > N$, 
$\sqrt{n} \hat{V}_n$ obeys the Subspace Detection Property with probability 1.

## Orthogonal Spectral Clustering

**Theorem** (KTT): 
If $P = V \Lambda V^\top$ is the spectral decomposition of $P$ for the PABM and 
$V$ has rows $v_i^\top$, then $v_i^\top v_j = 0$ $\forall z_i \neq z_j$.

**Theorem** (KTT): 
If $A \approx \hat{V} \hat{\Lambda} \hat{V}^\top$ is the spectral decomposition 
of $A$ for the PABM using the $K (K+1) / 2$ most positive and $K (K-1) / 2$ 
most negative eigenvalues, and we denote $\hat{v}_i^\top$ as the rows of 
$\hat{V}$, then 

$$\max_{i, j : z_i \neq z_j} \hat{v}_i^\top \hat{v}_j = 
O_P \Big( \frac{(\log n)^c}{\sqrt{n \rho_n}} \Big)$$

Orthogonal Spectral Clustering (KTT): 

1. Let $V$ be the eigenvectors of $A$ corresponding to the $K (K+1)/2$ most 
positive and $K (K-1) / 2$ most negative eigenvalues.
2. Compute $B = |n V V^\top|$ applying $|\cdot|$ entry-wise.
3. Construct graph $\hat{G}$ using $B$ as its similarity matrix.
4. Partition $\hat{G}$ into $K$ disconnected subgraphs.

## Simulation Study

Simulation setup:

1. $Z_1, ..., Z_n \stackrel{iid}{\sim} Categorical(1 / K, ..., 1 / K)$
2. $\lambda_{ik} \stackrel{iid}{\sim} Beta(a_{ik}, b_{ik})$  
$a_{ik} = \begin{cases} 2 & z_i = k \\ 1 & z_i \neq k \end{cases}$, 
$b_{ik} = \begin{cases} 1 & z_i = k \\ 2 & z_i \neq k \end{cases}$
3. $P_{ij} = \lambda_{i z_j} \lambda_{j z_i}$
4. $A \sim BernoulliGraph(P)$

```{r clust_err_k, fig.width = 10, fig.height = 3, out.width = '100%'}
setwd('..')
clustering.df <- readr::read_csv('clustering-k.csv')
ssc.df <- readr::read_csv('clustering-ssc-k.csv')
clustering.df %>%
  dplyr::group_by(n, K) %>%
  dplyr::summarise(
    med.err = median(error),
    first.q = quantile(error, .25),
    third.q = quantile(error, .75),
    med.err.ssc = median(error.ssc),
    first.q.ssc = quantile(error.ssc, .25),
    third.q.ssc = quantile(error.ssc, .75),
    med.err.ep = median(error.ep, na.rm = TRUE),
    first.q.ep = quantile(error.ep, .25, na.rm = TRUE),
    third.q.ep = quantile(error.ep, .75, na.rm = TRUE),
    med.err.louvain = median(error.louvain),
    first.q.louvain = quantile(error.louvain, .25),
    third.q.louvain = quantile(error.louvain, .75)
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::inner_join(
    ssc.df %>% 
      dplyr::group_by(n, K) %>% 
      dplyr::summarise(med.err.ssc2 = median(error.ssc2),
                       first.q.ssc2 = quantile(error.ssc2, .25),
                       third.q.ssc2 = quantile(error.ssc2, .75)) %>% 
      dplyr::ungroup()
  ) %>% 
  ggplot() +
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096)) +
  # scale_x_continuous(breaks = c(128, 256, 512, 1024, 2048, 4096)) + 
  scale_y_log10() +
  labs(y = 'community detection error rate', 
       colour = NULL) +
  geom_line(aes(x = n, y = med.err,
                colour = 'OSC')) +
  geom_errorbar(aes(x = n, ymin = first.q, ymax = third.q,
                    colour = 'OSC'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc,
                colour = 'SSC-ASE')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc, ymax = third.q.ssc,
                    colour = 'SSC-ASE'), width = .1) + 
  # geom_line(aes(x = n, y = med.err.ep,
  #               colour = 'MM-EP')) + 
  # geom_errorbar(aes(x = n, ymin = first.q.ep, ymax = third.q.ep,
  #                   colour = 'MM-EP'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc2,
                colour = 'SSC-A')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc2, ymax = third.q.ssc2,
                    colour = 'SSC-A'), width = .1) + 
  geom_line(aes(x = n, y = med.err.louvain,
                colour = 'MM-Louvain')) + 
  geom_errorbar(aes(x = n, ymin = first.q.louvain, ymax = third.q.louvain,
                    colour = 'MM-Louvain'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  facet_wrap(~ K, labeller = 'label_both')
```

## Conclusion

1. The PABM is a recently developed flexible block model that can be used to describe many graphs with community structure. 

2. Likelihood maximization is NP-complete for block models, so we need to take a different approach to community detection.

3. The GRDPG, which describes all block models, motivates a spectral approach to statistical inference on graphs.

4. Under the GRDPG framework, the PABM is induced by a latent configuration in $\mathbb{R}^{K^2}$ consisting of $K$ $K$-dimensional subspaces that are all orthogonal to each other.

5. The latent configuration of the PABM under the GRDPG framework leads to two methods for community detection, both of which have nice theoretical asymptotic properties. 

## Thank you

Code and draft of the paper available at  
https://github.com/johneverettkoo/pabm-grdpg

R package coming soon at https://github.com/johneverettkoo/pabm