sdss presentation outline (15 min)

1. block models
    * network models with bernoulli edges based on edge probability matrix
    * block model: P depends on hidden labels
    * example: SBM
    * example: PABM
3. (G)RDPG
    * also network models with bernoulli edges
    * not necessarily a block model
    * all block models are GRDPG
    * ASE
4. Connecting the PABM to the GRDPG
    * example: SBM
5. Community detection for the PABM
    * SSC
    * OSC

script

Hello. Thank you for attending my talk titled, "Connecting the popularity adjusted block model to the generalized random dot product graph." This is joint work with Minh Tang, assistant professor at NC State, and Michael Trosset, professor at IU, where I am currently a PhD student.

Here is the outline of my talk. Before we dive into our contributions, we first need to define the popularity adjusted block model and the generalized random dot product graph. Next, we will show how the PABM is a very special case of the GRDPG. Then finally, we will discuss two community detection techniques for the PABM based on this connection. 

Let's begin.

First, I want to introduce the block model.

Suppose we observe a network with binary and undirected edges, for example, a Facebook network in which the nodes are individuals and the edges represent whether pairs of individuals are friends. One analysis task might be to partition this network into groups in an unsupervised yet meaningful manner, called graph partitioning or graph clustering. There are infinitely approaches to this task, but we will take a statistical inference based approach by defining a probability model and trying to estimate parameters of that model, namely hidden and unobserved labels associated with each node.

Let's describe this network by adjacency matrix A such that each A_ij is 1 if there is an edge between nodes i and j, and 0 otherwise. We say this graph is a Bernoulli Graph if it is sampled from some other matrix P such that each A_ij is drawn independely as Bernoulli(P_ij). 

In addition, if the nodes have a hidden community labels 1, 2, up to K, and each P_ij depends in some way on the labels z_i and z_j, then this model is called a block model. 

For instance, this two community stochastic block model defines P such that P_ij is 1/2 if i and j are both in community 1, 1/4 if they are both in community 2, and 1/8 if they are in different communities. 

Another more flexible block model is the popularity adjusted block model. First, let's organize P by community such that the kl^th block represents the edge probabilities between communities k and l. Then for each pair of communities, there are popularity vectors lambda^kl in n_k-dimensional space and lambda^lk in n_l-dimensional space such that the kl^th block of P is the outer product of lambda^kl and lambda^lk.

Now that we have these block models, we want to go back to our original problem of graph partitioning via community detection. Unfortunately, likelihood maximization is NP-complete, so we need to take another approach. In order to motivate this, we will introduce another type of bernoulli graph model.

We will switch gears to another bernoulli graph model called the generalized random dot product graph. In this model, we start with latent vectors x_1, up to x_n in euclidean space, and we set the edge probability between each pair i and j by taking the indefinite inner product between latent vectors x_i and x_j. 

Under this model, an inference task might be to estimate the latent positions after observing one draw of the model. The adjacency spectral embedding does this by taking the spectral decomposition of A, and it has already been shown that the ASE asymoptotically approaches the true latent positions up to an indefinite orthogonal transformation. 

It is easy to show that all bernoulli graphs, including all block models, can be represented as generalized random dot product graphs, simply by taking the spectral decomposition of P. It's well known that the latent configuration that induces the stochastic block model under the GRDPG framework is a collection of point masses where each point mass corresponds to a community. Our contribution shows similarly that the latent configuration that induces the PABM consists of orthogonal subspaces where each subspace corresponds to a community. 

To illustrate this connection between the PABM and the GRDPG, we will take a look at the case where we have two communities. We can decompose P in this manner, and we see that the latent configuration is 4-dimensional and consists of a block diagonal matrix times an orthogonal matrix where each block is rank 2 and corresponds to a community. Thus the latent configuration for the PABM consists of 2-dimensional subspaces that are orthogonal to each other in 4-dimensional space. This easily generalizes to K > 2 in the same manner. 

Our theorem states that if we have a PABM, we can compute an X that is block diagonal and U that is orthogonal such that the model is GRDPG with latent configuration X U. Furthermore, this latent configuration is such that if two vectors x_i and x_j belong to two different communities, their dot product is zero. We call this the canonical latent configuration for the popularity adjusted block model under the generalized random dot product framework.

One thing to note is that this latent configuration is not unique. For each P, there are infinitely many latent configurations that can be used to construct P under the GRDPG, and these are all tied to indefinite orthogonal transformations. So while our canonical latent configuration for the PABM is not unique, all latent configurations that induce the PABM consist of subspaces with one community per subspace. We were also able to construct a type of adjacency spectral embedding that enforces orthogonality between pairs of subspaces, the details of which we omit in this presentation. 

So how does this lead to community detection?

Since the latent configuration for the PABM under the GRDPG framework consists of subspaces, the ASE consists of points lying near subspaces with some noise that goes to 0 with probability 1. There already is a family of algorithms for this type of data called subspace clustering. We will focus on sparse subspace clustering, which constructs a graph by solving n optimization problems. Input X obeys the subspace detection property if sparse subspace clustering results in a graph such that if x_i and x_j belong to different subspaces, there is no edge connecting the two. 

While they did not make the connection to the GRDPG and did not apply any embedding techniques, Noroozi, Rimal, and Pensky noticed something very similar and proposed using sparse subspace clustering for community detection on the PABM. Our work shows that a modified version of their SSC algorithm using the ASE of A as the input obeys the subspace detection property with probability 1. 

Next, we examined the eigenvectors of P for the PABM and saw that if we select two rows that correspond to different communities, their inner product will always be 0. This led to our own community detection method called orthogonal spectral clustering. Here, we show that if V are the eigenvectors of A, as n increases, the inner products of the rows of V that correspond to different communities goes to 0 with probability 1. 

Here is a simulation study of the various community detection algorithms for the PABM. In each simulation, community labels were sampled from a categorical distribution and the popularity parameters were sampled from beta distributions to construct P, and then A was sampled from P. Finally, we applied these four community detection algorithms to each A. 

Here, the red curve is modularity maximization using the louvian algorithm, blue is our orthogonal spectral clustering algorithm, green is sparse subspace clustering as implemented by noroozi et al., and purple is sparse subspace clustering on the adjacency spectral embedding. In these results, we can see that for K = 2, OSC outperforms all of the other algorithms for large n, and for K > 2, OSC and SSC have similar performance. Our theorems say that if n is large enough, then orthogonal spectral clustering will result in perfect community detection and the adjacency spectral embedding will obey the subspace detection property. 

In conclusion, we explored the popularity adjusted block model, which is a flexible block model that can describe many graphs with community structure. However, standard statistical inference techniques such as likelihood maximization is difficult for the popularity adjusted block model. Instead, we motivated a spectral method via the generalized random dot product graph, which can describe all block models. Under the generalized random dot product graph framework, the popularity adjusted block model is induced by latent configurations in K-squared dimensional space and consists of K K-dimensional subspaces that are all orthogonal to each other. The latent configuration of the popularity adjusted block model under the generalized random dot product graph framework leads to two methods for community detection, the first of which is a preexisting algorithm, sparse subspace clustering, and the second is a new method we call orthogonal spectral clustering, both of which have nice theoretical aysmptotic properties. 

Thank you for attending my talk. You can see the code and paper for this work on my github page, and I hope to have an R package for the two algorithms described in this talk on there as well. 

