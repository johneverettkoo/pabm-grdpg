Thank you for attending my talk, titled, "popularity adjusted block models are generalized random dot product graphs".

This is joint work with Dr. Minh Tang of NC State University and Dr. Michael Trosset of Indiana University, where i am currently a PhD student.


First, i'll motivate this work with an illustration. 
Suppose we observe a network with binary and undirected edges, for example, a Facebook network in which the nodes are individuals and edges represent whether pairs of individuals are friends. 

One analysis task for these data might be to group the vertices in an unsupervised yet meaningful manner. In the Facebook network example, we might want to identify cliques or friend groups. 

The statistical inference approach for this would be to assume that the data come from some probability model that includes community memberships. Then our goal is to develop a method for estimating those parameters for which we can derive asymptotic properties for these estimators, such as consistency. In this work, we consider graphs generated from a recent model called the popularity adjusted block model. 


Let's begin very generally with bernoulli graphs. 

Suppose we have a graph that is undirected and unweighted. We can describe this graph by an n by n adjacency matrix A, which contains entries 1 when there is an edge between its corresponding pair of vertices, and 0 otherwise. We say matrix A describes a Bernoulli graph if there is a corresponding edge probability matrix P such that each A_ij is drawn as Bernoulli P_ij. 


Let's build on this to bernoulli graphs with community structure. 
We begin by assigning each vertex of the graph community labels which can take on values from 1 to K. Then if each P_ij depends in some way on the labels of vertices i and j, we call this a block model. 

The simplest and most common example of this is the stochastic block model, which has a fixed edge probability for each pair of communities. In this particular example, we have two communities, so each z_i can take on values 1 or 2. Then we set each P_ij as p if both i and j are in community 1, q if they are both in community 2, and r if they are in different communities. 


The stochastic block model is the simplest type of block model and assumes a very rigid community structure, so various researchers came up with generalizations of this model. One such generalization is the popularity adjusted block model. In the pabm, each vertex has K popularity parameters, so vertex i has parameters lambda_i1, lambda_i2, up to lambda_iK. Each of these describes vertex i's affinity toward a particular community. For instance, lambda_il is vertex i's affinity toward community l. Then each P_ij is defined vertex i's affinity toward vertex j's community times vertex j's affinity toward vertex i's community. 

A research group at UCF came up with an another characterization of the PABM using popularity vectors. I don't have time to get into the details of this here, but in short, they describe the edge probability matrix P as a rank K^2 matrix that can be organized into blocks by communities. In their work, they use the low rank of P to justify the use of sparse subspace clustering for community detection. Our approach similarly takes advantage of the low rank of P, albeit more explicitly. 


But first, we need to take a detour to another family of bernoulli graph models. We say that A is sampled from a generalized random dot product graph if each vertex has a corresponding vector in some latent space and the edge probability between vertices i and j as the indefinite inner product between latent vectors x_i and x_j, where the indefinite inner product is characterized by Ipq, which is a diagonal matrix of p positive ones and q negative ones. Then our edge probability matrix is X Ipq X^top. 

Under this model, an inference task might be to estimate the latent positions X from one observation of the graph, A. The adjacency spectral embedding does this by taking the spectral decomposition of A, and it has already been shown that the ASE asymoptotically approaches the true latent positions up to an indefinite orthogonal transformation. 


So now we have two different models for graphs, one in which edges are drawn based on community structure and another where edges are drawn based on vectors in some latent space. 

Now, we'll show how these two families of models are connected, in particular for the PABM.

It can be easily shown that all bernoulli graphs are generalized random dot product graphs. P can be decomposed, and that decomposition describes vectors in latent space. To illustrate this more explicitly, let's return to the stochastic block model example from before. If we organize P by community, the upper left block is all p, the lower right is q, and the off diagonal blocks are r. Then it's easy to find a two dimensional set of latent vectors that multiply to P that consists of two unique rows, resulting in two point masses, one at (sqrt(p), 0) and another at (sqrt(r^2 / p), sqrt(q - r^2 / p)). 


This leads to a straightforward community detection method. We just take the ASE of A, and we know that as n increases, the ASE converges to the latent configuration, that is, two point masses which correspond to the two communities, and we can apply a clustering technique on the ASE that is based on clusters aggregating around point masses, such as K-means or gaussian mixture models. 


Similarly, the latent configuration for the pabm is organized by community, this time, as orthogonal subspaces. The rest of this talk is on establishing this connection and using it for community detection. 


First, the theorem that connects the two models. If A is the adjacency matrix of a pabm, then it is also an adjacency matrix of a grdpg with latent vectors described by X times U where we can compute X, U, p, and q exactly from the parameters of the PABM. The main takeaway from this connection is the structure of X, which is block-diagonal with each block consisting of K columns and corresponding to a community. What this means is in the latent space, each community of a PABM lies on a K-dimensional subspace, and the subspaces are orthogonal to each other. 


We can use this fact to come up with community detection algorithms. 

We start with a theorem. Let V Lambda V^T be the spectral decomposition of P where P is the edge probability matrix for the PABM. Then the rows of V that correspond to different communities are orthogonal. We can collect this information in this matrix B, so B_ij is 0 if i and j are in different communities. Then we can let B represent a similarity graph which consists of at least K disjoint subgraphs that correspond to the communities.

The most straightforward application of this theorem is to replace P with A. We call this algorithm orthogonal spectral clustering. And we have a theorem that states that the matrix B outputted by this algorithm will be such that B_ij goes to 0 for i and j in different communities, under some mild conditions. This guarantees that asymptotically, the error count of this algorithm goes to zero as n increases. This is an improvement over a previously proposed algorithm for the PABM, which only guarantees that the error proportion goes to zero as n increases. 


We also ran simulation studies to test our algorithm empirically. The results in red is the algorithm proposed by sengupta and chen, and we can see that while the error proportion goes to zero as n increases, the error count does not. The results in blue is our orthogonal spectral clustering algorithm. The results in green is sparse subspace clustering, which is an algorithm by elhamifar and vidal and applied to this problem by pensky et al., and the results in purple is our version of sparse subspace clustering using what we learned about the pabm's connection to the grdpg. 
