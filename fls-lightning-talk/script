Thank you all for attending my talk, titled, "popularity adjusted block models are generalized random dot product graphs".


This is joint work with Dr. Minh Tang of NC State University and Dr. Michael Trosset of Indiana University, where i am currently a PhD student.


First, i will try to motivate this work with an illustration. 
Suppose we observe a network with binary and undirected edges, for example, a Facebook network in which the nodes are individuals and edges represent whether pairs of individuals are friends. 

An analysis task for these data might be to partition this network into subgraphs in an unsupervised yet meaningful manner. In the Facebook network example, we might want to identify cliques or friend groups. 

One approach, based on statistical inference, would be to assume that the data come from some probability model that includes community memberships. Then our goal would be to develop a method for estimating those parameters and deriving asymptotic properties for these estimators, such as consistency. This is the approach we take. We consider graphs generated from a recent model called the popularity adjusted block model and develop estimators under this model. 


Let's begin very generally with bernoulli graphs. 

Suppose we have a graph that is undirected and unweighted. We can describe this graph by an n by n adjacency matrix A, which contains entries 1 when there is an edge between its corresponding pair of vertices, and 0 otherwise. We say matrix A describes a Bernoulli graph if there is a corresponding n by n edge probability matrix P such that each A_ij is drawn as Bernoulli P_ij. 


Now we want to turn our attention to bernoulli graphs with community structure. 
We begin by assigning each vertex of the graph community labels which can take on values from 1 to K. Then if each P_ij depends in some way on the labels of vertices i and j, we call this a block model. 

The simplest and most common example of this is the stochastic block model, which has a fixed edge probability for each pair of communities. In this particular example, we have two communities, so each z_i can take on values 1 or 2. Then we set each P_ij as p if both i and j are in community 1, q if they are both in community 2, and r if they are in different communities. 


Another type of block model, which the rest of this talk will focus on, is the popularity adjusted block model. In the pabm, each vertex has K popularity parameters, so vertex i has parameters lambda_i1, lambda_i2, up to lambda_iK. Then lambda_ik is vertex i's affinity toward community k. Each P_ij is defined as lambda_{i, z_j} lambda_{j z_i}, or vertex i's affinity toward vertex j's community times vertex j's affinity toward vertex i's community. 

Noroozi, Rimal, and Pensky came up with an another characterization of the PABM using popularity vectors. I won't get into the details of this here, but in short, they describe the edge probability matrix P as a rank K^2 matrix that can be organized by communities. In their work, they use the low rank of P to justify the use of sparse subspace clustering for community detection. Our approach similarly takes advantage of the low rank of P, albeit more explicitly. 


But first, we need to take a detour to another family of bernoulli graph models. We say that A is sampled from a generalized random dot product graph if each vertex has a corresponding vector in some latent space and the edge probability between vertices i and j as the indefinite inner product of latent vectors x_i and x_j, where the indefinite inner product is characterized by Ipq, which is a diagonal matrix of p positive ones and q negative ones. Then our edge probability matrix is X Ipq X^top. 

Under this model, an inference task might be to estimate the latent positions after observing one draw of the model. The adjacency spectral embedding does this by taking the spectral decomposition of A, and it has already been shown that the ASE asymoptotically approaches the true latent positions up to an indefinite orthogonal transformation. 


Now, we will show how these two families of models are connected, in particular for the PABM.

It can be easily shown that all bernoulli graphs are generalized random dot product graphs. P can be decomposed, and that decomposition can be used to construct an embedding. To illustrate this more explicitly, let's return to the stochastic block model example from before. In this example, we said that if two vectors are both in community 1, the probability of there being an edge between them is p, if they are both in community 2 then the probability of there being an edge between them is q, and if they are in different communities then the probability of there being an edge between them is r. If we organize P by community, the upper left block is all p, the lower right is q, and the off diagonal blocks are r. Then it's easy to find a two dimensional embedding for P that consists of two unique rows, resulting in two point masses, one at (sqrt(p), 0) and another at (sqrt(r^2 / p), sqrt(q - r^2 / p)). 


This leads to a straightforward community detection method. We just take the ASE of A, and we know that as n increases, the ASE converges to the latent configuration, that is, two point masses which correspond to the two communities, and we can apply a clustering technique on the ASE that is based on clusters aggregating around point masses, such as K-means or gaussian mixture models. 


Similarly, the latent configuration for the pabm is organized by community, this time, as orthogonal subspaces. The rest of this talk is on establishing this connection and using it for community detection. 


First, the theorem that connects the two models. If A is the adjacency matrix of a pabm, then it is also an adjacency matrix of a grdpg with latent vectors described by X times U where we can compute X, U, p, and q exactly from the parameters of the PABM. The actual forms of p, q, and U aren't all that important aside from the fact that p + q = K^2 and U is orthogonal. What's more interesting is the form of X, which is block diagonal with each block corresponding to a community. Thus, if we just look at X, then each community lives on a K-dimensional hyperplane, for instance, the first community lives on the hyperplane of coordinates x_1 to x_K, the second lives on the hyperplane described by coordinates x_{K+1} to x_{2K}, and so on. And since U is orthogonal, the latent configuration just rotates X, so we can't describe the resulting hyperplanes by coordinates, but they are still hyperplanes/subspaces, and they are still orthogonal. 


Now we go into specific community detection methods for the PABM using its connection to the GRDPG.

We start with a theorem. Let V Lambda V^T be the spectral decomposition of P. Then the rows of V that correspond to different communities are orthogonal. We can collect this information in this matrix B, so B_ij is 0 if i and j are in different communities. Then we can let B represent a similarity graph which consists of at least K disjoint subgraphs that correspond to the communities.

The most straightforward application of this theorem is to replace P with A. We call this algorithm orthogonal spectral clustering. And we have a theorem that states that the matrix B outputted by this algorithm will be such that B_ij goes to 0 for i and j in different communities, under some mild conditions. 


We also ran a few simulations to test our algorithms empirically. We compared our algorithm to modularity maximization, proposed by sengupta and chen, and sparse subspace clustering, proposed by Elhamifar and Vidal and used by Pensky et al. for this model. We also applied sparse subspace clustering a bit differently using the ASE. Here, we show the interquartile range of the community detection error rates of our simulations, with the x axis being the size of the graph.
