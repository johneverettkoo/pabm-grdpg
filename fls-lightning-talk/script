Thank you all for attending my talk, titled, "popularity adjusted block models are generalized random dot product graphs".


This is joint work with Dr. Minh Tang of NC State University and Dr. Michael Trosset of Indiana University, where i am currently a phd student.


First, i will try to motivate this work with an illustration. 
Suppose we observe a network with binary and undirected edges, for example, a Facebook network in which the nodes are individuals and edges represent whether pairs of individuals are friends. 

An analysis task for these data might be to partition this network into subgraphs in an unsupervised yet meaningful manner. In the Facebook network example, we might want to identify cliques or friend groups. 

One approach might be to define some metric or objective function, for example, minimizing the number of edges we need to cut to partition the graph into disconnected subgraphs. Then we just treat this as a standard ML optimization problem. examples of this include min-cut, ratio cut, normalized cut, etc. 

Another approach, based on statistical inference, would start by defining a probability model from which this network might be sampled or generated. The parameters of this model might include hidden or unobserved labels associated with each node, along with a few other parameters. Then our goal would be to develop a method for estimating those parameters and deriving asymptotic properties for these estimators, such as consistency. In our work, we take the statistical inference approach. 


Let's begin very generally with bernoulli graphs. 
Suppose we have a graph, which is a set of vertices V and a set of edges E, that is undirected and unweighted. We can describe this graph by an n by n adjacency matrix A, which contains entries 1 when there is an edge between its corresponding pair of vertices, and 0 otherwise. We say matrix A describes a Bernoulli graph if there is a corresponding n by n edge probability matrix P such that each A_ij is drawn as Bernoulli P_ij. The simpliest example of this is the erdos-renyi graph, which sets all entries of P to a constant probability. 


Now we want to turn our attention to bernoulli graphs with community structure. 
We begin by assigning each vertex v_1, v_2, up to v_n community labels z_1, z_2, up to z_n, which can take on values from 1 to K. Then if each P_ij depends in some way on the labels of vertices i and j, we call this a block model

The simplest and most common example of this is the stochastic block model. In this particular example, we have two communities, so each z_i can take on values 1 or 2. then we set each p_ij as p if both i and j are in community 1, q if they are both in community 2, and r if they are in different communities. 


Another type of block model, which the rest of this talk will focus on, is the popularity adjusted block model. In the pabm, each vertex has K popularity parameters, so vertex i has parameters lambda_i1, lambda_i2, up to lambda_iK. Then lambda_ik is vertex i's affinity toward community k. Each P_ij is defined as lambda_{i, z_j} lambda_{j z_i}, or vertex i's affinity toward vertex j's community times vertex j's affinity toward vertex i's community. 

Noroozi, Rimal, and Pensky came up with an another characterization of the pabm using popularity vectors. 

Let's designate P^kl as the n_k x n_l block of edge probabilities between communities k and l. We also organize the popularity parameters into popularity vectors, so lambda^kl contains the popularity parameters of community k toward community l. We have K^2 of these popularity vectors. Then we can decompose each block P^kl as the outer product of lambda^kl and lambda^lk. 

Now that we defined these models with community structure, the next task is to develop estimators for the community labels. Conventional statistical inference approaches such as likelihood maximization are not feasible, so we need to take another approach. In order to motiate this, we will introduce another type of bernoulli graph model.


Let's switch gears to another family of bernoulli graph models, the generalized random dot product graph. We say that A is sampled from a generalized random dot product graph by setting the edge probability between vertices i and j as the indefinite inner product of vectors x_i and x_j, where the indefinite inner product is characterized by Ipq, which is a diagonal matrix of p positive ones and q negative ones. 


Under this model, an inference task might be to estimate the latent positions after observing one draw of the model. The adjacency spectral embedding does this by taking the spectral decomposition of A, and it has already been shown that the ASE asymoptotically approaches the true latent positions up to an indefinite orthogonal transformation. 


Now, we will show how these two families of models are connected, in particular for the PABM.

It can be easily shown that all bernoulli graphs are generalized random dot product graphs. P can be decomposed, and that decomposition can be used to construct an embedding. To illustrate this more explicitly, let's return to the stochastic block model example from before. In this example, we said that if two vectors are both in community 1, the probability of there being an edge between them is p, if they are both in community 2 then the probability of there being an edge between them is q, and if they are in different communities then the probability of there being an edge between them is r. If we organize P by community, the upper left block is all p, the lower right is q, and the off diagonal blocks are r. Then it's easy to find a two dimensional embedding for P that consists of two unique rows, resulting in two point masses. 


This leads to a straightforward community detection method. We just take the ASE of A, and we know that as n increases, the ASE converges to the latent configuration, that is, two point masses which correspond to the two communities, and we can apply a clustering technique on the ASE that is based on clusters aggregating around point masses, such as K-means or gaussian mixture models. 


Similarly, the latent configuration for the pabm is organized by community, this time, as orthogonal subspaces. The rest of this talk is on establishing this connection and using it for community detection. 


First, the theorem that connects the two models. If A is the adjacency matrix of a pabm, then it is also an adjacency matrix of a grdpg with latent vectors described by X times U where we can compute X, U, p, and q exactly from the parameters of the PABM. The actual forms of p, q, and U aren't all that important aside from the fact that p + q = K^2 and U is orthogonal. What's more interesting is the form of X, which is block diagonal with each block corresponding to a community. Thus, if we just look at X, then each community lives on a K-dimensional hyperplane, for instance, the first community lives on the hyperplane of coordinates x_1 to x_K, the second lives on the hyperplane described by coordinates x_{K+1} to x_{2K}, and so on. So each Lambda takes on K columns. And since U is orthogonal, the latent configuration just rotates X, so we can't describe the resulting hyperplanes by coordinates, but they are still hyperplanes/subspaces, and they are still orthogonal. 


Now we go into specific community detection methods for the PABM using its connection to the GRDPG.

We start with a theorem. Let V Lambda V^T be the spectral decomposition of P. Then the rows of V that correspond to different communities are orthogonal. We can collect this information in this matrix B, so B_ij is 0 if i and j are in different communities. Then we can let B represent a similarity graph which consists of at least K disjoint subgraphs that correspond to the communities.

The most straightforward application of this theorem is to replace P with A. We call this algorithm orthogonal spectral clustering. And we have a theorem that states that the matrix B outputted by this algorithm will be such that B_ij goes to 0 for i and j in different communities, under some mild conditions. 


We also ran a few simulations to test our algorithms empirically. Here, we show the interquartile range of the community detection error rates of our simulations, with the x axis being the size of the graph. There is some weird behavior going on for ssc with K = 2, but otherwise, all of these methods result in zero error for large n, with orthogonal spectral clustering and ssc on the eigenvectors performing better than ssc on the adjacency matrix. 
