% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Referee Response for JCGS Submission JCGS-21-341},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{float}
\usepackage{mathtools}
\usepackage{natbib}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\setcitestyle{numbers,square,comma}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage{comment}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{Referee Response for JCGS Submission JCGS-21-341}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

We thank the referees for their time in reviewing this paper and
providing valuable comments and suggestions, which have all been taken
into account for this revision. Most of the changes are in the
simulation and application sections. In particular, we add a simulation
study of ``disassortative'' PABMs and clarify the choices made in the
data preprocessing.

The following sections are individual referee comments and our responses
(highlighted in \textcolor{blue}{blue}).

\hypertarget{referee-1-comments}{%
\section{Referee 1 Comments}\label{referee-1-comments}}

\hypertarget{main-comments}{%
\subsection{Main comments}\label{main-comments}}

The interest of this paper is to provide new perspectives on PABM
(theoretical and algorithmic). The theoretical results are sound.
However, the algorithmic results shown in the simulations are not really
impressive since they are done in settings that do not appear very
challenging. Indeed, they are done with fixed numbers of communities
which are quite low (no more than 4) and only with assortative structure
(larger probabilities of connections within a community than between two
communities). Therefore, I think the paper needs a more challenging
simulation study and the question of the choice of the number of
communities needs to be addressed at least with some heuristic that
works in simulation studies if theoretical results are not reachable.
Moreover, some points on the exposition could be made clearer. I have
specific comments below where I ask for some clarifications and detail
how the simulation study could be improved in my opinion.

\textcolor{blue}{
We thank the referee for the feedback. 
The concerns addressed here are answered in the following section. 
In particular, we make sure to address the concerns about the simulation studies. 
}

\hypertarget{specific-comments-and-questions}{%
\subsection{Specific comments and
questions}\label{specific-comments-and-questions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  p4. What is a hollow graph?\\
  \textcolor{blue}{
  We changed the terminology a bit here and throughout the paper 
  and specify that the analysis assumes $G$ is an unweighted and undirected graph with no self-loops (no edge directly from itself to itself), 
  so $A$ is binary and symmetric with zeros on the diagonal. 
  }
\item
  p4. \(G\) as a graph is non-oriented or undirected and \(A\) is
  symmetric.\\
  \textcolor{blue}{
  We have made minor edits clarifying the structure of $G$ and $A$. 
  }
\item
  p4. Please define the set \([n]\).\\
  \textcolor{blue}{
  This has been added. $[n] = \{1, 2, ..., n\}$. 
  }
\item
  p4. Definition 1. Do you consider a probability distribution on
  \(z_1, ..., z_n\)? Please give the range of variation of \((i, j)\)
  when you define \(P_{ij}\).\\
  \textcolor{blue}{
  We have included the range of $(i, j)$ in the definition. 
  In regard to the probability distribution on the labels $z_1, ..., z_n$, none of the theorems require this, although some do require that each $n_k$ be sufficiently large, so it did not factor into our definitions and analyses. 
  One thing that was discussed was a possibility of a Bayesian analysis of the PABM, and we have some results using Variational Inference and Mean Field Approximation, but we believe that this is a topic for another paper. 
  }
\item
  p4. line 50. Instead of ``increasing values of the community
  assignments'', you could write ``reorganized by community
  memberships''.\\
  \textcolor{blue}{
  This has been changed.
  }
\item
  p5. Definition 2. I do not understand the definition of the subset
  \(\mathcal{X}\). Is it \(x \in \mathcal{X}\) if for all
  \(y \in \mathbb{R}^d\) we have \(x^\top I_{p,q} y\) or is
  \(\mathcal{X}\) a subset on \((\mathbb{R}^d)^2\)? Please clearly state
  that \(d = p + q\) (not stated before Definition 3).\\
  \textcolor{blue}{
  We clarify the definition of the subset as $\mathcal{X} = \{x, y \in \mathbb{R}^d : x^\top I_{p, q} y \in [0, 1]\}$ and add that $d = p + q$. 
  }
\item
  p6. Remark 2. What are \(\hat{Z}\), \(V\), and \(D\)?\\
  \textcolor{blue}{
  We now include in the remark the $D$ is the diagonal matrix of the $p$ most positive and $q$ most negative eigenvalues of $A$ (or $P$), $V$ is the matrix of corresponding eigenvectors, and $\hat{Z}$ is the embedding $V |D|^{1/2}$.
  }
\item
  top of p7. \(\tilde{P}\) instead of \({P}\).\\
  \textcolor{blue}{
  We have corrected the typo. 
  }
\item
  p7 line 40-46. I don't understand the interest of this paragraph.\\
  \textcolor{blue}{
  The purpose of the additional material in the proof of Theorem 1 is to provide intuition for the arbitrary $K$ case. 
  In particular, for the $K = 2$ case, we show that $\tilde{P} = X \Pi X^\top$ where $\Pi$ is a permutation matrix with 2 fixed points and 1 cycle of order 2, and the rows of $X$ consist of vectors that lie on two 2-dimensional orthogonal subspaces. 
  The idea is that this generalizes to larger $K$ such that the rows of $X$ are vectors that lie on $K$ $K$-dimensional orthogonal subspaces, and $\Pi$ is a permutation matrix with $K$ fixed points and $K^2 - K = K (K - 1)$ cycles of order 2. 
  Then the eigenvalues of $\Pi$ are broken up by the following: $K$ $1$'s corresponding to the $K$ fixed points and $K (K - 1)$ pairs of $+1$ and $-1$ corresponding to the $K (K - 1)$ cycles of order 2, 
  resulting in $K (K + 1) / 2$ $+1$'s and $K (K - 1) / 2$ $-1$'s, i.e., the diagonal eigenvalue matrix of $\Pi$ is exactly $I_{K (K + 1) / 2, K (K - 1) / 2}$. 
  This gives us the embedding dimension ($\mathbb{R}^{K^2}$ broken up into $K (K + 1) / 2$ ``positive'' and $K (K - 1) / 2$ ``negative'' dimensions according to the eigenvalues of $\Pi$) as well as the structure ($K$ $K$-dimensional subspaces as described by the rows of $X$). 
  We have added additional details to the paper to address this. 
  }
\item
  p8, line 14-21. I don't see why the permutation given by \(\Pi\) has
  \(K\) fixed points.\\
  \textcolor{blue}{
  The purpose of Example 1 is to illustrate this explicitly. 
  Comparing matrices $X$ and $Y$, we can see that they share three columns: the first, fifth, and ninth. 
  The other columns are permuted. 
  From Example 1 (as well as the proof of Theorem 1 for $K = 2$), it should be clear how this property holds for larger $K$. 
  The proof of Theorem 1 for arbitrary $K$ should then confirm the intuition based on $K = 2$ and $3$. 
  }
\item
  Section 3.1. I found it confusing that community detection algorithms
  are detailed when \(P\) is assumed to be known and then methods for
  estimating \(P\) are recalled. A sentence in the beginning of this
  section to expose what is presented could help the reader.\\
  \textcolor{blue}{
  The goal of estimation is not to estimate $P$ but to identify the community labels and estimate the popularity parameters. 
  This is not immediately obvious even after observing $P$, although observing $P$ does make estimation much easier. 
  In the introduction to section 3 (before 3.1), we have added the following:
  }\\
  \textcolor{blue}{
  ``In our methods, the data that are observed for estimation is the adjacency matrix, $A \sim \mathrm{PABM}(\{\lambda^{(k \ell)}\}_K, \rho_n)$, along with an assumed number of communities, $K$. 
  To motivate some of the methods, we also consider community detection and parameter estimation in the case where we observe the edge probability matrix, $P$, noting that community memberships and popularity parameters are not immediately discernible from observing $P$.''
  }
\item
  p10, line 12. \(c^{(i)}\) is not defined.\\
  \textcolor{blue}{
  $c^{(i)}$ is the $i^{th}$ entry of column vector $c$. 
  This has been added to the paper.
  }
\item
  p10. I didn't understand the exposition of the SSC algorithm.\\
  \textcolor{blue}{
  We have added more detail to our description of the SSC algorithm. 
  In particular, we describe the intuition/motivation behind the algorithm. 
  This isn't added to the description, but $c$ and $c_i$ are column vectors of length $n$ and $c^{(i)}$ and $c_i^{(j)}$ are the $i^{th}$ and $j^{th}$ entries of vectors $c$ and $c_i$ respectively. 
  }\\
  \textcolor{blue}{The description of SSC now reads:
  }\\
  \textcolor{blue}{
  ``The SSC algorithm can be described as follows: 
  Given \(X \in \mathbb{R}^{n \times d}\) with vectors
  \(x_i^\top \in \mathbb{R}^d\) as rows of \(X\), the optimization problem
  \(c_i = \arg\min_{c} \|c\|_1\) subject to \(x_i = X^\top c\) and
  \(c^{(i)} = 0\), where $c^{(i)}$ is the $i^{th}$ entry of $c$, is solved for each \(i = [n]\). 
  The solutions are collected collected into matrix
  \(C = \bigl[ c_1 \mid \cdots \mid c_n \bigr]^\top\) to
  construct an affinity matrix \(B = |C| + |C^\top|\). If each \(x_i\) lies
  exactly on one of \(K\) subspaces, \(B\) describes an undirected graph
  consisting of {\em at least} \(K\) disjoint subgraphs, i.e., \(B_{ij} = 0\) if \(x_i, x_j\) lie on different subspaces. 
  The intuition here is that points that lie on the same subspace can be described as linear combinations of each other. 
  Thus, for each $c_i$, $c_i^{(j)}$ is zero if $x_i$ and $x_j$ belong to different subspaces and may be nonzero if they do. 
  If \(X\) instead represents points near \(K\) subspaces with some noise, 
  then this property may only hold approximately and thus 
  a final graph partitioning step may be required 
  (e.g., edge thresholding or spectral clustering).''
  }
\item
  p11, line 31. ``\ldots{} SSC on the spectral embedding of \(A\)''.
  Could you support this assertion by references?\\
  \textcolor{blue}{
  We have added, "... SSC on the rows of the spectral embedding of $A$, since they will still lie approximately on the $K$ subspaces \citep{rubindelanchy2017statistical}."
  }
\item
  p12, line 13. It could help recall what the \(z_i, z_j\) are.\\
  \textcolor{blue}{
  We have changed $z_i$ to ``community labels''.
  }
\item
  p14. Do you think that the OSC algorithm can retrieve other kinds of
  structure than assortative ones?\\
  \textcolor{blue}{
  In response to this, we added a third simulation study with $\lambda_{kk} \sim Beta(1,2)$ and $\lambda_{kl} \sim Beta(2,1)$, resulting in an average within-community edge probability of $1/3$ and between-community edge probability of $2/3$. 
  Results of this simulation study are very similar to that of the first two and consistent with the theoretical results.
  }\\
  \textcolor{blue}{
  On a related note, it is our view that the distinction between assortative and disassortative is not very important for the PABM, at least when it is viewed as a type of GRDPG. 
  In the case of the SBM and DCBM, it can be shown that the rank of $P$ (and therefore the number of embedding dimensions) is equal to $K$, but assortativity or disassortativity determines how many positive and negative eigenvalues $P$ has \citep{rubindelanchy2017statistical}. 
  In the case of the PABM, the number of positive eigenvalues is always $K (K + 1) / 2$ and the number of negative eigenvalues is always $K (K - 1) / 2$, (assuming each community spans its respective subspace). 
  }
\item
  Sections 4 and 5. Why do you use either the ARI criterion or the
  misclassification rate to assess the recovery of the nodes
  clustering?\\
  \textcolor{blue}{
  The first to datasets in section 5 refer to datasets analyzed by other PABM papers. 
  For the Leeds Butterfly dataset, while the paper with which we are comparing \citep{noroozi2019estimation} provides enough details to transform and preprocess the data to match their analysis, they do not provide code to run their clustering algorithm. 
  Therefore, we use their reported benchmark value, which is in the form of ARI. 
  In order to make an apples-to-apples comparison, we report the ARI for OSC and SSC-ASE here as well. 
  For all other analyses (simulation and real data), we use misclassification error (whether rate or count). 
  }
\item
  Do you consider sparsity in the simulated adjacency matrices?
\item
  Why do you limit your simulation to assortative structure? Is it
  possible to see the results with a larger value of \(K\)? To what
  extent does the value of \(K\) impact the computational burden for the
  algorithms?\\
  \textcolor{blue}{
  Regarding disassortativity, as mentioned in previous comments, we have added a third simulation study to address this. 
  Performance of the algorithms tested is similar to the other two simulation studies and consistent with the theory. 
  Regarding $K$, our algorithms require each $n_k$ to be large enough to span its subspace of dimension $K$, so we focused mostly on smaller values of $K$. 
  Cursory numerical experiments suggest that OSC and SSC-ASE behave similarly for larger $K$ provided that $n$ is large enough. 
  }\\
  \textcolor{blue}{
  Regarding computational complexity, because both OSC and SSC-ASE may have an arbitrary final processing step, we will focus on the complexity in constructing the affinity matrix $B$. 
  For OSC, this involves spectral decomposition followed by matrix multiplication of $n \times K^2$ and $K^2 \times n$ matrices, so the complexity is $O(n^3 + n^2 K^2)$. 
  For SSC-ASE, we again start with spectral decomposition, followed by $n$ LASSO problems with a design matrix of size $K^2 \times (n - 1)$, so the complexity is $O(n^3 + n^2 K^4 + n K^6)$ \cite{10.1214/009053604000000067}. 
  SSC-A involves solving $n$ LASSO regression problems each with design matrices of size $n \times (n - 1)$, so the complexity here is $O(n^4)$. 
  When $n \gg K^2$, we can see that OSC and SSC-ASE are both $O(n^3)$. 
  In practice, we have found that $K$ does not affect runtimes too much. 
  }
\item
  Again, in your application on the Leeds butterfly dataset, why did you
  limit your analysis to \(K = 4\)?\\
  \textcolor{blue}{
  For the Leeds butterfly dataset, we wanted to compare results of OSC and SSC-ASE to an analysis performed by another group studying the PABM \citep{noroozi2019estimation}. 
  In their results, they describe how they removed some of the nodes and discretized the edges, which we replicate in our analysis. 
  }
\item
  Please specify in the text that the proofs for Theorems 3, 4, and 5
  are given in the Appendix.\\
  \textcolor{blue}{
  We have added this to the end of the introduction section.
  }
\end{enumerate}

\hypertarget{typos}{%
\subsubsection{Typos}\label{typos}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  p 10, line 16 ``if each \(x_i\) lieS \ldots{}''\\
  \textcolor{blue}{
  We have corrected this typo. 
  }
\end{enumerate}

\hypertarget{referee-2-comments}{%
\section{Referee 2 Comments}\label{referee-2-comments}}

The first part of the paper shows that any PABM is a GRDPG and gives a
constructive proof of the result. In particular, the proof gives the
construction of the latent positions of the corresponding GRDPG.

The second part of the paper investigates the implications of this
relation to the GRDPG (and the identified latent positions) on
estimation algorithms for the PABM. New algorithms for community
detection and parameter estimation are proposed. In particular, a new
community detection method, named orthogonal spectral clustering (OSC),
is proposed that directly exploits the result of Theorem 1. Furthermore,
an improvement of the sparse subspace clustering (SSC) algorithm is
proposed consisting of applying SSC to the adjacency spectral embedding
of the network (ASE), referred to as SSC-ASE. For both algorithms it is
shown that the subspace detection property holds and that the
convergence rate of the detection error is faster than for classical
SSC. Likewise, it is shown that parameter estimation can be improved. A
numerical study illustrates the performance of the algorithms in
comparison with existing methods. It shows that OSC performs the best,
and also that SSC-ASE improves the results with respect to classical SSC
in some scenarios. In the application on real datasets, OSC and SSC-ASE
provide good results, but do not achieve the performance of modularity
maximization (MM).

The topic of the paper is interesting as it concerns a recent graph
model and the results are valuable. Contributions are two-fold:
theoretic and computational. The paper is well written, and the
presentation of the results and the proofs is very clear.

I only have some minor comments and a couple of questions:

\begin{itemize}
\item
  I have a general issue with latent position models like the GRDPG due
  to the nonidentifiability of the latent space. As stated in the paper,
  latent positions are not unique, and not even the dimension of the
  latent space is identifiable. Theorem 1 (or its proof) provide one of
  all possible latent positions. A general question is what is the
  consequence of this specific choice. Does the latent space (or its
  dimension) have an impact on the clustering results? Indeed, I lack
  intuition about what type of latent space would be optimal for
  community detection or estimation in the PABM. The space with the
  smallest possible dimension? Do the authors have an idea of the
  properties of the latent space that would give the best results for
  community detection? Related to this issue, is the latent space of the
  proof of Theorem 1 the smallest possible latent space to relate PABM
  to the GRDPG?\\
  \textcolor{blue}{
  We thank the referee for the questions. 
  While it is true that there are an infinite number of latent configurations that generate the PABM, based on Theorem 1 and GRDPG results by \citet{rubindelanchy2017statistical}, we can conclusively say the following: 
  \\1. The embedding dimension is $K^2$ with $K (K + 1) / 2$ positive and $K (K - 1) / 2$ negative eigenvalues, 
  \\2. The latent structure will always consist of $K$ $K$-dimensional subspaces---in the configuration outlined by Theorem 1, these subspaces are orthogonal, but the multiplication by arbitrary $Q \in \mathbb{O}(p, q)$ that results in the same $P$ may skew the subspaces making them no longer orthogonal (although they will still be $K$ $K$-dimensional subspaces), 
  \\3. $B = n V V^\top$, where $V$ is the $n \times K^2$ matrix of eigenvectors of $P$, will always be such that $B_{ij} = 0$ if and only if vertices $i$ and $j$ are in different communities. 
  \\4. The rows of $V$ are such that the inner product of the $i^{th}$ and $j^{th}$ rows is zero if and only if vertices $i$ and $j$ are in different communities. 
  }\\
  \textcolor{blue}{
  Therefore, the dimensionality of the latent configuration is not in question, only what kind of effect multiplication by unidentifiable $Q$ has on the configuration. 
  The OSC algorithm circumvents the nonidentifiability of the latent configuration by only considering the entries of $B = n V V^\top$. 
  SSC-ASE does rely on the latent configuration, but we show that the orthogonality property holds for the particular embedding we perform for this algorithm. 
  }\\
  \textcolor{blue}{
  That said, looking back at the latent configuration in Theorem 1, we can see that a $K$-dimensional (not $K^2$-dimensional) latent configuration is sufficient for clustering. 
  If we are able to obtain this exact embedding, this would greatly reduce the number of dimensions required for the embedding. 
  In practice, this isn't always possible due to the nonidentifiability issue (in particular, determining which of the $K$ out of $K^2$ eigenvectors should be used, although we have found via numerical experiments that community detection is often possible with the $2^{nd}$ to the $(K+1)^{st}$ embedding dimensions).
  }
\item
  This is clearly beyond the scope of the paper, but I wonder if there
  is a straightforward way to adapt the proposed methods to directed
  models or weighted graphs or any other types of graphs than binary
  undirected ones?\\
  \textcolor{blue}{
  For directed graphs, the ASE provides a consistent estimator and thus should also work here \citep{doi:10.1080/01621459.2012.699795}. 
  For weighted graphs, if $A_{ij}$ can be an estimator for $P_{ij}$ (e.g., if $A_{ij} \sim \mathrm{Poisson}(P_{ij})$), then all of the theory should transfer over to those models as well, 
  and we believe that the theory can be adapted to other models where $g(A_{ij})$ is an estimator for $P_{ij}$. 
  We hope to address these topics in future research. 
  }
\item
  In the simulation study, SSC-ASE works well for \(K \geq 3\), but not
  for \(K = 2\). The authors explain the bad performance by the use of
  gaussian mixtures in the last clustering step. However, I don't
  understand why this has an impact on the performance for \(K = 2\),
  but not for \(K \geq 3\).\\
  \textcolor{blue}{
  We are not entirely clear on why this happens for $K = 2$, but when comparing the normalized Laplacian eigenmaps of the affinity matrix $B$ for various $K$, we see that when $K > 2$, the points appear much closer to Gaussian mixtures, whereas with $K = 2$, there is some separation by community, but they do not appear as Gaussian mixtures, at least for the simulated data. 
  One possible reason for this may be because we failed to find the right hyperparameter for SSC-ASE. 
  }\\
  \textcolor{blue}{
  Under ideal circumstances, we would be able to circumvent this issue altogether via the subspace detection property, but SDP does not necessarily guarantee a partitioning of the graph into exactly $K$ disjoint subgraphs but rather {\em at least} $K$ disjoint subgraphs, and in our simulations, we found that setting $\vartheta$ to the appropriate value to obtain SDP often results in greater than $K$ subgraphs, which is why we ended up choosing an ``incorrect'' $\vartheta$ for SSC and then performing the final clustering step.
  }\\
  \textcolor{blue}{
  We also note that in our simulations, while SSC-ASE and SSC-A in the $K = 2$ case and MM-Louvain for all $K$ output labels such that the number of mislabeled vertices increases with $n$, the proportion of mislabeled vertices still decreases.
  }
\item
  In the application section, MM performs better than OSC and SSC-ASE,
  while in the simulation study the opposite is observed. What is the
  reason for the different behavior?\\
  \textcolor{blue}{
  In the application section, we analyze data that are not necessarily generated by the PABM (or any known model). 
  While all three algorithms are specific to the PABM, they might all behave differently to the different ways in which the model is misspecified. 
  There is also the question of hyperparameter tuning, especially in the case of SSC-ASE, and we spent a bit more time tweaking the hyperparameters, resulting in performance that is a bit closer to that of MM. 
  }
\item
  Typo on p.~10: \(X c\) should be \(X^\top c\). Likewise, \(X_{-i} c\)
  is to be replaced with \(X_{-i}^\top c\).\\
  \textcolor{blue}{
  We have corrected the typo.
  }
\end{itemize}

\newpage

  \bibliography{misc.bib}

\end{document}
