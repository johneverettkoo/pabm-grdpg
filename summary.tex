% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[12pt]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Connecting the Popularity Adjusted Block Model to the Generalized Random Dot Product Graph for Community Detection and Parameter Estimation},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
%\usepackage[margin=1in]{geometry}
\usepackage{graphicx,grffile}
\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{float}
\usepackage{mathtools}
% \usepackage{natbib}
\usepackage[backend=bibtex,maxbibnames = 10,style=authoryear,giveninits=true,natbib]{biblatex}
\DeclareNameAlias{sortname}{last-first}
\addbibresource{misc.bib}
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}
\usepackage{parskip}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% \setcitestyle{numbers,square,comma}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage{comment}
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\title{Popularity Adjusted Block Models are Generalized Random Dot Product Graphs}

\author{John Koo\\Department of Statistics, Indiana University\\
Minh Tang\\Department of Statistics, North Carolina State University\\
Michael Trosset\\Department of Statistics, Indiana University}

% \date{\vspace{-2.5em}}
\date{}

\begin{document}

\maketitle

\begin{abstract}
We connect two random graph models, the Popularity Adjusted Block Model
(PABM) and the Generalized Random Dot Product Graph (GRDPG),
by demonstrating that a PABM is a GRDPG in which communities correspond to 
mutually orthogonal subspaces of latent vectors. 
This insight allows us to construct new algorithms for community detection
and parameter estimation for the PABM, 
as well as improve an existing algorithm 
that relies on Sparse Subspace Clustering. 
Using established asymptotic properties of 
Adjacency Spectral Embedding for the GRDPG, 
we derive asymptotic properties of these algorithms. 
In particular, we demonstrate that the absolute number of 
community detection errors tends to zero as 
the number of graph vertices tends to infinity. 
Simulation experiments illustrate these properties. 
\end{abstract}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\blockdiag}{\mathrm{blockdiag}}
\newcommand{\indep}{\stackrel{\mathrm{ind}}{\sim}}
\newcommand{\iid}{\stackrel{\mathrm{iid}}{\sim}}
\newcommand{\Bernoulli}{\mathrm{Bernoulli}}
\newcommand{\Betadist}{\mathrm{Beta}}
\newcommand{\BG}{\mathrm{BernoulliGraph}}
\newcommand{\PABM}{\mathrm{PABM}}
\newcommand{\RDPG}{\mathrm{RDPG}}
\newcommand{\GRDPG}{\mathrm{GRDPG}}
\newcommand{\Multinomial}{\mathrm{Multinomial}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newpage
\spacingset{1.5} % DON'T change the spacing!

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Statistical inference on random graphs requires a suitable probability model. 
A general probability model for unweighted and undirected graphs is the Bernoulli Graph (also known as the inhomogeneous Erd\"{o}s-R\'{e}nyi model), which assumes that edges occur as independent Bernoulli trials.  A Bernoulli Graph is characterized by an edge probability matrix $P=[P_{ij}]$, where an edge between vertices $i$ and $j$ occurs with success probability $P_{ij}$.  A trivial example of a Bernoulli Graph is the (homogeneous) Erd\"{o}s-R\'{e}nyi model proposed by \citet{Gilbert:1959}, in which the vertices of the random graph are fixed and possible edges occur independently with fixed probability $P_{ij}=p$.  The requirement that $p$ is fixed is too strong for most applications, and various researchers have weakened that requirement in various ways.  The present work relates two lines of generalization.

Network analysis is often concerned with community detection.  One form of community detection assumes that each vertex belongs to an unobserved community, with the probability of an edge between vertices $i$ and $j$ depending on the communities to which $i$ and $j$ belong.  Formally, one assigns each vertex $v_i$ a community label $z_i$ and assumes a Bernoulli Graph in which $P_{ij}$ is a function of $z_i$ and $z_j$.  Such models, called Block Models, define the goal of community detection as a problem in statistical inference: identify the true community (up to permutation of labels) to which each vertex belongs.

The classical Stochastic Block Model (SBM) of \citet{doi:10.1080/0022250X.1971.9989788} specifies that each edge probability $P_{ij}$ depends only on the labels $z_i$ and $z_j$, i.e., $P_{ij} = \omega_{z_,z_j}$.  Subsequent researchers have weakened this assumption.   The Degree-Corrected Block Model (DCBM) of \citet{Karrer_2011} assigns an additional parameter $\theta_i$ to each vertex and sets $P_{ij} = \theta_i \theta_j \omega_{z_,z_j}$. The Popularity Adjusted Block Model (PABM) of \citet{307cbeb9b1be48299388437423d94bf1} generalizes the DCBM, allowing heterogeneity of edge probabilities within and between communities while still maintaining distinct community structure.

Another type of Bernoulli Graph was proposed by
\citet*{10.1007/978-3-540-77004-6_11}.  A Random Dot Product Graph
(RDPG) specifies that each vertex corresponds to a latent position
vector in Euclidean space and that the probability of an edge between
two vertices is the dot product of their latent position vectors.
Thus, if the latent positions are $x_1,\ldots,x_n \in \mathbb{R}^d$
and $X = [\begin{array}{ccc} x_1 & \cdots & x_n \end{array} ]^\top$,
then the edge probability matrix is $P = XX^\top$.  Clearly, any
Bernoulli Graph with positive semidefinite $P$ is an RDPG.  The
positive definite Euclidean inner product in the RDPG model was
replaced by an indefinite inner product in
\citet{rubindelanchy2017statistical}, resulting in the {\em
Generalized}\/ RDPG (GRDPG).

In contrast to Block Models, neither RDPGs nor GRDPGs inherently
specify distinct communities.  However, one can easily impose
community structure by assuming that the latent positions lie in
distinct clusters.  Hence, it is not surprising that Block Models can
be studied by reformulating them as RDPGs or GRDPGs.  For example, an
assortative SBM (an SBM for which $P$ is positive semidefinite) is
equivalent to an RDPG for which all vertices in the same community
correspond to the same latent position vector.
Likewise, the DCBM is equivalent to an RDPG for which all vertices in
the same community correspond to latent position vectors that lie on a
straight line. %(\cite{lyzinski2014, rubindelanchy2017statistical}).

Because the edge probability matrix of a PABM is not necessarily
positive semidefinite, a PABM is not necessarily an RDPG.  In Section
\ref{connecting-the-pabm-to-the-grdpg} we demonstrate that every PABM
is in fact a specific type of GRDPG for which the latent position
vectors lie in distinct orthogonal subspaces, each subspace
corresponding to a community.  This identification is our central
result.  In Section \ref{methods}, we use the geometry of the GRDPG to
derive more efficient algorithms for detecting the communities and
estimating the parameters in the PABM.  We report the results of
simulation studies in Section \ref{simulated-examples} and apply our
methods to three well-known data sets in Section
\ref{real-data-examples}. Section \ref{discussion} concludes.

\hypertarget{connecting-the-popularity-adjusted-block-model-to-the-generalized-random-dot-product-graph}{%
\section{PABMs are GRDPGs}\label{connecting-the-popularity-adjusted-block-model-to-the-generalized-random-dot-product-graph}}

In this section, we show that the PABM is a special case of the GRDPG. 
More specifically, a graph $G$ drawn from the PABM can be represented by 
a collection of latent vectors in Euclidean space. 
We further show that the latent configuration that
induces the PABM consists of orthogonal subspaces with each subspace
corresponding to a community.

\hypertarget{notation}{%
\subsection{Notation and Scope}\label{notation}}

Let $G = (V, E)$ be an unweighted, undirected, 
and hollow graph with vertex set $V$ ($|V| = n$) and edge set $E$. The matrix
$A \in \{0, 1\}^{n \times n}$ represents the adjacency matrix of $G$ 
such that $A_{ij} = 1$ if there exists an edge between vertices $i$ and $j$ 
and $0$ otherwise. Because $G$ is symmetric and hollow,
$A_{ij} = A_{ji}$ and $A_{ii} = 0$ for each $i, j \in [n]$. 
We further restrict our analyses to Bernoulli graphs. 
Let $P \in [0, 1]^{n \times n}$ be a symmetric matrix of edge probabilities. 
Graph $G$ is sampled from $P$ by drawing $A_{ij} \indep \Bernoulli(P_{ij})$ 
for each $1 \leq i < j \leq n$ (setting $A_{ji} = A_{ij}$ and $A_{ii} = 0$). 
We denote $A \sim \BG(P)$ as a graph with adjacency matrix $A$ 
sampled from edge probability matrix $P$ in this manner. 
If each vertex has a hidden label in $[K]$, 
they are denoted as $z_1, ..., z_n$. 
Finally, we denote 
$X = \bigl[x_1 \mid \cdots \mid x_n\bigr]^\top \in \mathbb{R}^{n \times d}$ 
as the matrix corresponding to a collection of $n$ latent vectors $x_1, ..., x_n \in \mathbb{R}^d$.

\hypertarget{the-popularity-adjusted-block-model-and-the-generalized-random-dot-product-graph}{%
\subsection{Two Probability Models for Graphs}\label{the-popularity-adjusted-block-model-and-the-generalized-random-dot-product-graph}}

\begin{definition}[Popularity Adjusted Block Model]
\label{pabm}
Let $P \in [0, 1]^{n \times n}$ be a symmetric edge probability matrix for a
graph $G = (V, E)$ with adjacency matrix $A$ 
such that $A \sim \mathrm{Bernoulli}(P)$.
Let each vertex have a community label between $1$ and $K$.
Then $G$ is drawn from a Popularity Adjusted Block Model if
each vertex has $K$ popularity parameters that describe its affinity
toward each of the $K$ communities, i.e., vertex $i$ has popularity parameters
$\lambda_{i1}, ..., \lambda_{iK}$, and  $P_{ij} = \lambda_{i z_j}
\lambda_{j z_i}$ for all $i \leq j$. 

Another characterization of the PABM is as follows.
Let the rows and columns of $P$ be arranged by community label
such that $n_k \times n_{\ell}$ block $P^{(k \ell)}$
describes the edge probabilities between vertices in communities
$k$ and $\ell$; note that $P^{(k \ell)} = (P^{(k \ell)})^\top$.
Now suppose that each block $P^{(k \ell)}$ can be written as the outer product of two vectors:
\begin{equation} \label{eq:pabm}
  P^{(k \ell)} = \lambda^{(k \ell)} (\lambda^{(\ell k)})^{\top}
\end{equation}
for a set of $K^2$ popularity vectors 
$\{\lambda^{(st)}\}_{1 \leq s \leq K, 1 \leq t \leq K}$ where each
$\lambda^{(st)}$ is a column vector of dimension $n_s$. Then a graph $G$
is drawn from a PABM with parameters
$\{\lambda^{(st)}\}$ if its adjacency matrix $A$
satisfies $A \sim \mathrm{BernoulliGraph}(P)$.
\end{definition}

We will use the notation \(A \sim \PABM(\{\lambda^{(k \ell)}\}_K)\) to denote
a random adjacency matrix \(A\) drawn from a PABM with parameters
\(\{\lambda^{(k \ell)}\}\) consisting of \(K\) underlying communities.

\begin{definition}[Generalized Random Dot Product Graph]
\label{grdpg}
Let graph $G = (V, E)$ be drawn as $A \sim \BG(P)$ for some symmetric $P \in
[0,1]^{n \times n}$. 
If there exists a data matrix $X \in \mathbb{R}^{n \times d}$ such that
\begin{equation} \label{eq:grdpg}
  P = X I_{p,q} X^\top
\end{equation}
for some $d, p, q \in \mathbb{N}$ and $p + q = d$, then
$G$ is drawn from the Generalized Random Dot Product Graph 
with latent positions $x_1, ..., x_n \in \mathbb{R}^d$ and signature $(p, q)$.
\end{definition}

We will use the notation \(A \sim \GRDPG_{p,q}(X)\) to denote a random
adjacency matrix \(A\) drawn from latent positions \(X\) and signature
\((p, q)\). If instead of fixed latent positions, 
they are drawn $X_1, ..., X_n \iid F$, 
we denote the GRDPG as $(A, X) \sim \GRDPG_{p,q}(F, n)$.

\begin{remark}
We can use Adjacency Spectral Embedding (ASE) 
\citep{doi:10.1080/01621459.2012.699795} to recover the latent vectors of a GRDPG. 
This procedure consists of taking the spectral decomposition of $A$ 
(or $P$ if available) and choosing the $p$ most positive and $q$ most negative
eigenvalues and their corresponding eigenvectors 
to construct the embedding $\hat{Z} = V |D|^{1/2}$. 
\end{remark}

\begin{definition}[Indefinite Orthogonal Group]
The indefinite orthogonal group with signature $(p, q)$ is
the set $\{Q \in \mathbb{R}^{d \times d} \colon Q I_{p, q} Q^{\top} = I_{p, q}\}$,
denoted as $\mathbb{O}(p, q)$. Here $d = p + q$. 
\end{definition}

\begin{remark}
The latent vectors that produce $X I_{p,q} X^\top = P$ are not unique
\citep{rubindelanchy2017statistical}.
More specifically, if $P_{ij} = x_i^\top I_{p, q} x_j$, 
then we also have for any $Q \in \mathbb{O}(p, q)$ that
$(Q x_i)^\top I_{p, q} (Q x_j) = x_i^\top (Q^\top I_{p, q} Q) x_j =
x_i^\top I_{p, q} x_j = P_{ij}$.
Unlike in the RDPG case, transforming the latent positions via multiplication
by $Q \in \mathbb{O}(p, q)$ does not necessarily maintain interpoint angles or
distances.
\end{remark}

\hypertarget{connecting-the-pabm-to-the-grdpg}{%
\subsection{The Geometry of PABMs}\label{connecting-the-pabm-to-the-grdpg}}

Now that we defined the PABM and GRDPG, 
we show the special geometry of the PABM when viewed as a GRDPG. 

\begin{theorem}[The latent configuration of the PABM]
\label{theorem2}
Let $A \sim \mathrm{PABM}(\{\lambda^{(k \ell)}\}_K)$ be an instance of a
PABM with $K \geq 1$ blocks and latent vectors $\{\lambda^{(k \ell)}
\colon 1 \leq k \leq K, 1 \leq \ell \leq K\}$. 
Then there exists a block diagonal matrix
$X \in \mathbb{R}^{n \times K^2}$ defined by $\{\lambda^{(k \ell)}\}$ and a 
$K^2 \times K^2$ {\em fixed} orthonormal matrix $U$ such 
that $A \sim \mathrm{GRDPG}_{K (K+1) / 2, K (K-1) / 2}(XU)$.
\end{theorem}

\begin{proof}
We will prove this theorem in two parts. First, for demonstration purposes, we focus on the case for $K = 2$. Then we generalize this to $K \geq 2$. 

For the $K = 2$ case, the proof is straightforward. Let 
$$X = \begin{bmatrix}
\lambda^{(11)} & \lambda^{(12)} & 0 & 0 \\
0 & 0 & \lambda^{(21)} & \lambda^{(22)}
\end{bmatrix} \quad \text{and} \quad
U = \begin{bmatrix} 1 & 0 & 0 & 0 \\
0 & 0 & 1 / \sqrt{2} & 1 / \sqrt{2} \\
0 & 0 & 1 / \sqrt{2} & - 1 / \sqrt{2} \\
0 & 1 & 0 & 0 \end{bmatrix}.$$
Then by straightforward matrix multiplication, we obtain 
\[X U I_{3, 1} U^\top X^\top =
\begin{bmatrix}
  \lambda^{(11)} (\lambda^{(11)})^\top & \lambda^{(12)} (\lambda^{(21)})^\top \\
  \lambda^{(21)} (\lambda^{(12)})^\top & \lambda^{(22)} (\lambda^{(22)})^\top
\end{bmatrix}, \]
which is a GRDPG with latent vectors described by $X U$.

It is nevertheless instructive to look at a few intermediate steps. 
More specifically, the product $U I_{3, 1} U^\top$ 
yields a permutation matrix $\Pi$ with fixed points at positions $1$ and $4$ 
and a cycle of order 2 swapping positions $2$ and $3$, i.e., 
$$\Pi = U I_{3, 1} U^\top = \begin{bmatrix} 1 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 1
\end{bmatrix}.$$
Furthermore, since $U$ is orthonormal and $I_{3, 1}$ is diagonal, $U I_{3, 1} U^\top$
is also an eigendecomposition of $\Pi$ where the fixed
points of $\Pi$ are mapped to the eigenvectors $e_1$ and $e_4$
while the cycles of order two are mapped to the eigenvectors  
$\tfrac{1}{\sqrt{2}}(e_{2} + e_3)$ and $\tfrac{1}{\sqrt{2}}(e_{2} -
e_3)$; here $e_i$ denote the $i^\mathrm{th}$ basis vector in $\mathbb{R}^{4}$.

For the general case, we first define the following matrices
\begin{gather}
\label{eq:xy}
\Lambda^{(k)} = \begin{bmatrix} \lambda^{(k1)} \mid \cdots \mid \lambda^{(kK)} \end{bmatrix}
\in \mathbb{R}^{n_k \times K}, \quad
X = \text{blockdiag}(\Lambda^{(1)}, \dots, \Lambda^{(K)}) \in
\mathbb{R}^{n \times K^2}, \\
L^{(k)} = \text{blockdiag}(\lambda^{(1k)}, \dots, \lambda^{(Kk)}) \in
\mathbb{R}^{n \times K}, \quad
Y = \begin{bmatrix} L^{(1)} \mid \cdots \mid L^{(K)} \end{bmatrix} \in
\mathbb{R}^{n \times K^2}.
\end{gather}
It is then straighforward to verify that
\begin{gather*}
  XY^{\top} = \text{blockdiag}(\Lambda^{(1)}, \dots,
\Lambda^{(K)}) \begin{bmatrix} L_1^{\top} \\ \vdots \\
  L_{K}^{\top} \end{bmatrix} = \begin{bmatrix} \Lambda^{(1)}
  (L^{(1)})^{\top} \\ \vdots \\
  \Lambda^{(K)} (L^{(K)})^{\top} \end{bmatrix}, \\
\Lambda^{(k)} (L^{(k)})^{\top} = \begin{bmatrix} \lambda^{(k1)}
  (\lambda^{(1k)})^{\top} \mid \dots \mid \lambda^{(kK)}
  (\lambda^{(Kk)})^{\top} \end{bmatrix} = \begin{bmatrix} P^{(k1)}
  \mid P^{(k2)} \mid \dots \mid P^{(kK)} \end{bmatrix}.
\end{gather*}
We therefore have $P = X Y^\top$. 
Similar to the $K = 2$ case, we also have $Y = X \Pi$ for some permutation matrix
$\Pi$ and hence $P = X \Pi X^\top$.
The permutation described by $\Pi$ has $K$ fixed points, which correspond to
$K$ eigenvalues equal to $1$ with corresponding eigenvectors $e_k$ where
$k = r (K + 1) + 1$ for $r = 0, ..., K - 1$. It also has
$\binom{K}{2} = K (K - 1) / 2$ cycles of order $2$. Each cycle corresponds to
a pair of eigenvalues $\{-1,+1\}$ and a pair of eigenvectors
$\{(e_s + e_t)/\sqrt{2},(e_s - e_t)/ \sqrt{2}\}$.

Let $p = K (K + 1) / 2$ and $q = K (K - 1) / 2$. 
We therefore have
\begin{equation} \label{eq:permutation}
\Pi = U I_{p,q} U^\top
\end{equation}
where $U$ is a $K^2 \times K^2$ orthogonal matrix and hence
\begin{equation} \label{eq:pabm-grdpg}
P = X U I_{p, q} (X U)^\top.
\end{equation}
In summary we can describe the PABM with $K$ communities as a GRDPG with latent
positions $X U$ and signature $(p,q) = \bigl( \tfrac{1}{2} K (K + 1) ,
\tfrac{1}{2} K (K - 1)\bigr)$.
\end{proof}

\begin{example} Let $A$ be a $3$ blocks PABM with latent vectors
  $\{\lambda^{(k \ell)} \colon 1 \leq k \leq 3, 1 \leq \ell \leq 3\}$. Using the same notation as in Theorem
  \ref{theorem2}, we can define
\begin{gather*}
X = \begin{bmatrix}
\lambda^{(11)} & \lambda^{(12)} & \lambda^{(13)} & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & \lambda^{(21)} & \lambda^{(22)} & \lambda^{(23)} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & \lambda^{(31)} & \lambda^{(32)} & \lambda^{(33)}
\end{bmatrix}, \\
Y = \begin{bmatrix}
\lambda^{(11)} & 0 & 0 & \lambda^{(12)} & 0 & 0 & \lambda^{(13)} & 0 & 0 \\
0 & \lambda^{(21)} & 0 & 0 & \lambda^{(22)} & 0 & 0 & \lambda^{(23)} & 0 \\
0 & 0 & \lambda^{(31)} & 0 & 0 & \lambda^{(32)} & 0 & 0 & \lambda^{(33)}
\end{bmatrix}.
\end{gather*}
Then $Y = X \Pi$ and $P = X Y^{\top}$ where $\Pi$ is a $9 \times 9$ 
permutation matrix of the form
$$\Pi = 
\Bigl[e_1 \mid e_4 \mid e_7 \mid e_2 \mid e_5 \mid e_8 \mid e_3
\mid e_6 \mid e_9 \Bigr].$$
where $e_i$ denotes the $i^{th}$ basis vector in $\mathbb{R}^{9}$. 
The matrix $\Pi$ corresponds to a permutation of $\{1,2,\dots,9\}$
with the following decomposition.
\begin{enumerate}
\item Positions 1, 5, 9 are fixed.
\item There are three cycles of length 2, namely $(2, 4)$, $(3, 7)$, and $(6, 8)$.
\end{enumerate}
We can therefore write $\Pi$ as $\Pi = U I_{6, 3} U^\top$ where the first three
columns of $U$ consist of $e_1$, $e_5$, and $e_9$ corresponding to the
{\em fixed} points, the next three columns consist of eigenvectors
$(e_k + e_{\ell}) / \sqrt{2}$, and the last three columns consist of eigenvectors
$(e_k - e_{\ell}) / \sqrt{2}$ for $(k, \ell) \in
\{(2,4),(3,7),(6,8)\}$.

The matrix $P$ is then the edge probabilities matrix for a 
Generalized Random Dot Product Graph whose latent positions 
are the rows of the matrix
$$XU = \begin{bmatrix}
  \lambda^{(11)} & 0 & 0 &
  \frac{\lambda^{(12)}}{\sqrt{2}} & \frac{\lambda^{(13)}}{\sqrt{2}} & 0 &
  \frac{\lambda^{(12)}}{\sqrt{2}} & \frac{\lambda^{(13)}}{\sqrt{2}} & 0 \\
  0 & \lambda^{(22)} & 0 &
  \frac{\lambda^{(21)}}{\sqrt{2}} & 0 & \frac{\lambda^{(23)}}{\sqrt{2}} &
  -\frac{\lambda^{(21)}}{\sqrt{2}} & 0 & \frac{\lambda^{(23)}}{\sqrt{2}} \\
  0 & 0 & \lambda^{(33)} &
  0 & \frac{\lambda^{(31)}}{\sqrt{2}} & \frac{\lambda^{(32)}}{\sqrt{2}} &
  0 & -\frac{\lambda^{(31)}}{\sqrt{2}} & -\frac{\lambda^{(32)}}{\sqrt{2}}
\end{bmatrix}.$$
\end{example}

\hypertarget{methods}{%
\section{Algorithms}\label{methods}}

Two inference objectives arise from the PABM:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Community membership identification (up to permutation).
\item
  Parameter estimation (estimating \(\lambda^{(k \ell)}\)'s).
\end{enumerate}

In our methods, we assume that \(K\), the number of communities, is
known beforehand and does not require estimation.

\hypertarget{related-work}{%
\subsection{Previous Work}\label{related-work}}

\citet{307cbeb9b1be48299388437423d94bf1} 
used Modularity Maximization (MM) and the Extreme Points (EP)
algorithm (\cite{le2016}) for community detection and parameter
estimation. They were able to show that as the sample size increases,
the {\em proportion} of misclassified community labels (up to permutation)
goes to 0.

\citet{noroozi2019estimation} used Sparse Subspace Clustering (SSC) 
(\cite{5206547}) for community detection in the PABM. 
The SSC algorithm can be described as follows: 
Given \(X \in \mathbb{R}^{n \times d}\) with vectors
\(x_i^\top \in \mathbb{R}^d\) as rows of \(X\), the optimization problem
\(c_i = \argmin_{c} \|c\|_1\) subject to \(x_i = X c\) and
\(c^{(i)} = 0\) is solved for each \(i = 1, ..., n\). The solutions are
collected collected into matrix
\(C = \bigl[ c_1 \mid c_2 \mid \cdots \mid c_n \bigr]^\top\) to
construct an affinity matrix \(B = |C| + |C^\top|\). If each \(x_i\) lie
exactly on one of \(K\) subspaces, \(B\) describes an undirected graph
consisting of \(K\) disjoint subgraphs, i.e., \(B_{ij} = 0\) if
\(x_i, x_j\) lie on different subspaces. If \(X\) instead represents
points near \(K\) subspaces with some noise, a final graph partitioning
step may be required (e.g., edge thresholding or spectral clustering).

In practice, due to presence of noise, SSC is often done by solving the LASSO problems
\begin{equation} \label{eq:ssc}
c_i = \argmin_c \frac{1}{2} \|x_i - X_{-i} c\|^2_2 + \lambda \|c\|_1
\end{equation}
for some sparsity parameter \(\lambda > 0\). The \(c_i\) vectors are
then collected into \(C\) and \(B\) as before.

\begin{definition}[Subspace Detection Property]
  \label{def:subspace_detection}
Let $X = \bigl[ x_1 \mid x_2 \mid \cdots \mid x_n \bigr]^\top$ be noisy
points sampled from $K$ subspaces, i.e., $x_i = y_i + z_i$ where the
$y_i$ belongs to the union of $K$ subspaces and the $z_i$ are noise
vectors. Let $\lambda \geq 0$ be given and
let $C$ and $B$ be constructed from the
solutions of LASSO problems as described in Eq.~\eqref{eq:ssc} with
this given choice of $\lambda$. Then $X$
is said to satisfy the subspace detection property 
with sparsity parameter $\lambda$ if each column of
$C$ has nonzero $\ell_2$ norm and $B_{ij} = 0$ whenever $y_i$ and $y_j$ are
from different subspaces.
\end{definition}

\begin{remark}
In practice, a noisy sample $X$ often does not obey the subspace detection
property. In such cases, $B$ is treated as an affinity matrix for a graph, 
which requires a partitioning step to obtain the final clustering. On the other
hand, if $X$ does obey the subspace detection property, $B$ describes a graph
with at least $K$ disconnected subgraphs. Ideally, when the subspace detection
property holds, there are exactly $K$ subgraphs which map onto each subspace,
but it could be the case that some of the subspaces are represented by
multiple disconnected subgraphs. The subspace detection property is contingent
on choosing a sufficiently large sparsity parameter $\lambda$.
\end{remark}

Theorem \ref{theorem2} suggests that SSC is appropriate for community
detection for the PABM. More precisely, Theorem \ref{theorem2} says that
each community consists of a \(K\)-dimensional subspace, and together
the subspaces lie in \(\mathbb{R}^{K^2}\). The natural approach then is
to perform SSC on the ASE of \(P\) or \(A\).
\citet{noroozi2019estimation} instead applied SSC to \(P\) and
\(A\), foregoing embedding altogether.

Using results from \citet{soltanolkotabi2012}, it can be easily shown
that the subspace detection property holds for \(XU\), which is an ASE
of \(P\). More specifically, if points lie exactly on mutually
orthogonal subspaces, then the subspace detection property will hold
with probability 1, and this is exactly the case for the PABM (Theorem
\ref{theorem2}). Much of our work is then built on
\citet{rubindelanchy2017statistical}, who describe the convergence
behavior of the ASE of \(A\) to the ASE of \(P\), and
\citet{jmlr-v28-wang13}, who show the necessary conditions for the
subspace detection property to hold in noisy cases where the points lie
near subspaces.

\hypertarget{community-detection}{%
\subsection{Algorithms for Community Detection}\label{community-detection}}

\begin{algorithm}[t]
  \label{alg:osc}
  \DontPrintSemicolon
  \SetAlgoLined
  \KwData{Adjacency matrix $A$, number of communities $K$}
  \KwResult{Community assignments $1, ..., K$}
    Compute the eigenvectors of $A$ that correspond to the $K (K+1) / 2$ most
    positive eigenvalues and $K (K-1) / 2$ most negative eigenvalues. Construct
    $V$ using these eigenvectors as its columns.\;
    Compute $B = |n V V^\top|$, applying $|\cdot|$ entry-wise.\;
    Construct graph $G$ using $B$ as its similarity matrix.\;
    Partition $G$ into $K$ disconnected subgraphs
    (e.g., using edge thresholding or spectral clustering).\;
    Map each partition to the community labels $1, ..., K$.\;
  \caption{Orthogonal Spectral Clustering.}
\end{algorithm}

We previously stated one possible set of latent positions that result in
the edge probability matrix of a PABM, \(P = (XU) I_{p, q} (XU)^\top\). If
we have (or can estimate) \(XU\) directly, then both the community
detection and parameter identification problem are trivial since \(U\)
is orthonormal and fixed for each value of \(K\). However, direct
identification or estimation of \(XU\) is not possible
(\cite{rubindelanchy2017statistical}).

More specifically, if we decompose \(P = Z I_{p, q} Z^\top\), then
there exists a \(Q \in \mathbb{O}(p, q)\) such that \(XU = Z Q\). Therefore, even if we
start with the true but unknown edge probability matrix $P$ we
will not be able to recover the ``original'' latent positions \(XU\).
Note that unlike in the case of the RDPG, \(Q\) is not necessarily an
orthogonal matrix and hence if \(z_i\)'s
are the rows of \(XU\) then
\(\|z_i - z_j\|^2 \neq \|Q z_i - Q z_j\|^2\), and
\(\langle z_i, z_j \rangle \neq \langle Q z_i, Q z_j \rangle\). This
prevents us from using the orthogonality property of \(XU\) directly. 

The explicit form of \(XU\) represents points in \(\mathbb{R}^{K^2}\)
such that points within each community lie on \(K\)-dimensional
orthogonal subspaces. Multiplication by \(Q \in \mathbb{O}(p, q)\)
removes the orthogonality property but retains the property that each
community is represented by a \(K\)-dimensional subspace. Therefore, the
ASE of \(P\) still consists of subspaces 
that correspond to communities of the PABM, even if they are not orthogonal, 
suggesting the use of SSC. Before exploring SSC, we will first consider
a different approach.

\begin{theorem}
\label{theorem3}
Let $P = V D V^\top$ be the spectral decomposition of the edge probability
matrix. Let $B = n V V^\top$. Then $B_{ij} = 0$ if vertices $i$ and $j$ are
from different communities.
\end{theorem}
\begin{proof}
% \begin{lemma}
% \label{lemma1}
% Let $P = V D V^\top$ be the spectral decomposition of the edge probability 
% matrix for a PABM.
We first show that $V V^\top = X (X^\top X)^{-1} X^\top$ where $X$ is 
defined as in Eq.~\eqref{eq:xy}. Indeed, by Theorem 2, 
\(P = X U I_{p, q} U^\top X^\top\) for $p = K(K+1)/2$ and $q = K(K-1)/2$. 

The eigendecomposition \(P = V D V^\top\) also yields $P = V
|D|^{1/2} I_{p, q} |D|^{1/2} V^\top$ where \(|\cdot|^{1/2}\) is
applied entry-wise. Now let $Y = XU$ and $Z = V|D|^{1/2}$; note that
$Y$ and $Z$ both have full column ranks. 
Because $P = Y I_{p,q} Y^{\top} = Z I_{p,q} Z^{\top}$, we have
$$Y = Z I_{p,q} Z^{\top} Y (Y^{\top} Y)^{-1} I_{p,q}.$$
Let $Q = I_{p,q} Z^{\top} Y (Y^{\top} Y)^{-1} I_{p,q}$ and note that
$Y = Z Q$. We then have
\begin{equation*}
  \begin{split}
  Q^{\top} I_{p,q} Q &= I_{p,q} (Y^{\top} Y)^{-1} Y^{\top} Z I_{p,q}
I_{p,q} I_{p,q} Z^{\top} Y (Y^{\top} Y)^{-1} I_{p,q} \\ &= I_{p,q} (Y^{\top} Y)^{-1} Y^{\top} Y I_{p,q}
Y^{\top} Y (Y^{\top} Y)^{-1} I_{p,q} =  I_{p,q}
\end{split}
\end{equation*}
and hence $Q$ is an indefinite orthogonal matrix. 

Let $R = U Q |D|^{-1/2}$ and note that $V = XR$. Because $R$ is
invertible, we can write

$$X (X^{\top} X)^{-1} X^{\top} = X R (R^{\top} X^{\top} X R)^{-1}
R^{\top} X^{\top}.$$ 

Furthermore, as $V$ has orthonormal columns, $R^{\top} X^{\top} X R =
V^{\top} V = I$. We thus conclude
$$X (X^{\top} X)^{-1} X^{\top} = V (V^{\top} V)^{-1} V^{\top} = V V^{\top}$$
as desired.

To complete the proof of Theorem~\ref{theorem3}, recall that \(X\) 
is block diagonal with each block corresponding to one community, 
and hence \(X (X^\top X)^{-1} X^\top\) is also a
block diagonal matrix with each block corresponding to a community. 
As $B = n VV^{\top} = n X (X^\top X)^{-1} X^\top$, 
we conclude that $B_{ij} = 0$ 
whenever vertices $i$ and $j$ belong to different communities.  
\end{proof}

Under the condition that $\|\Lambda\|_F > 0$, where 
$\Lambda$ is the $n \times K$ matrix of popularity parameters, 
Theorem \ref{theorem3} provides perfect community detection from \(P\).
Letting \(|B|\) be the affinity matrix for graph \(G'\), 
\(G'\) is partitioned into \(K\) disjoint subgraphs, as each of the
\(K\) communities have no edges between them. 
Then mapping each subgraph to a community provides a clustering with zero error 
(up to permutation).

In practice, we observe $A$ instead of $P$. 
Using \(A\) instead of \(P\) to estimate $B$ as in Theorem \ref{theorem3} introduces error, 
which converges to \(0\) almost surely. 
This is shown via properties of the GRDPG and ASE. 
We use this fact to motivate Orthogonal Spectral Clustering (Alg. 1). 
In order to show that OSC is consistent, 
we introduce the sparsity factor $\rho_n \in (0, 1]$, 
which can be characterized as the average expected degree of the vertices 
divided by the number of vertices. 
The treatment is the same as in \citet{307cbeb9b1be48299388437423d94bf1} (PABM) 
and \citet{rubindelanchy2017statistical} (GRDPG) in which the popularity vectors 
are scaled as $\rho_n^{1/2} \lambda^{(k \ell)}$. 
This results in the edge probability matrix being scaled as 
$P_{ij} = \rho_n \lambda_{i z_j} \lambda_{j z_i}$. 
Since $\rho_n$ is unidentifiable, we use the convention that the popularity parameters are drawn such that $\|\Lambda\|_F = O(\sqrt{n})$ and require that the $L^2$ norm of each row is bounded away from 0 and 1. 

\begin{theorem}
\label{theorem4}
Let $\hat{B}$ with entries $\hat{B}_{ij}$ be the affinity matrix from OSC
(Alg. 1). Then for all pairs $(i, j)$ belonging to different communities
and sparsity factor satisfying $n \rho_n = \omega(\log^{4}{n})$, we have
\begin{equation} \label{eq:thm4}
\max_{i, j} |n \hat{v}_i^\top \hat{v}_j| =
O\Big( \frac{\log n}{\sqrt{n \rho_n}} \Big)
\end{equation}
with high probability. 

Thus for all pairs $(i, j)$ in different communities,
we have uniform convergence $\hat{B}_{ij} \stackrel{a.s.}{\to} 0$. 
\end{theorem}

Theorem \ref{theorem4} provides an algorithm for community detection 
that results in zero community detection error as $n \to \infty$. 
For example, if we use edge thresholding to partition $\hat{B}$, 
Theorem \ref{theorem4} implies that for any $\epsilon > 0$, 
the number of edges of $\hat{B}$ between vertices of different communities 
that are larger than $\epsilon$ converges to zero with probability 1. 

Theorems \ref{theorem2}, \ref{theorem3}, and \ref{theorem4} also provide
a natural path toward using SSC for community detection. 
We established in Theorem \ref{theorem2} that an ASE of the edge
probability matrix \(P\) can be constructed from a latent vector configuration 
consisting of orthogonal subspaces. Theorem \ref{theorem3} shows how 
this property can be recovered from the eigenvectors of \(P\). 
Then Theorem \ref{theorem4} shows that replacing $P$ with $A$ 
approximates these properties with error that goes to zero as \(n \to \infty\). 
This allows us to obtain the subspace detection property 
in Theorem \ref{theorem5}. 

\begin{algorithm}[t]
  \label{alg:ssc}
  \DontPrintSemicolon
  \SetAlgoLined
  \caption{Sparse Subspace Clustering using LASSO.}
  \KwData{Adjacency matrix $A$, number of communities $K$,
  hyperparameter $\lambda$}
  \KwResult{Community assignments $1, ..., K$}
    Find $V$, the matrix of eigenvectors of $A$
    corresponding to the $K (K + 1) / 2$ most positive
    and the $K (K - 1) / 2$ most negative eigenvalues.\;
    Normalize $V \leftarrow \sqrt{n} V$.\;
    \For {$i = 1, ..., n$} {
      Assign $v_i^\top$ as the $i^{th}$ row of $V$.
      Assign $V_{-i} = \bigl[
      v_1 \mid \cdots \mid v_{i-1} \mid v_{i+1} \mid \cdots \mid v_n \bigr]^\top$.\;
      Solve the LASSO problem
      $c_i = \arg\min_{\beta}
      \frac{1}{2} \|v_i - V_{-i} \beta\|_2^2 + \lambda \|\beta\|_1$.\;
      Assign $\tilde{c}_i = (c_i^{(1)}, \dots, c_i^{(i-1)}, 0, c_i^{(i)}, \dots, c_i^{(n-1)})^\top$ such that the superscript is the index of
      $\tilde{c}_i$.\;
    }
    Assign
    $C = \bigr[ \tilde{c}_1 \mid \cdots \mid \tilde{c}_n \bigr]$.\;
    Compute the affinity matrix $B = |C| + |C^\top|$.\;
    Construct graph $G$ using $B$ as its similarity matrix.\;
    Partition $G$ into $K$ disconnected subgraphs (e.g., using edge
    thresholding or spectral clustering).\;
    Map each partition to the community labels $1, ..., K$.
\end{algorithm}

\begin{theorem}
\label{theorem5}
Let $P$ describe the edge probability matrix of the PABM with
$n$ vertices, and let $A \sim \Bernoulli(P)$.  Let $\hat{V}$ be the
matrix of eigenvectors of $A$ corresponding to the $K^2$ largest
eigenvalues in modulus. Then for any $\epsilon > 0$ 
there exists a choice of $\lambda > 0$ and $N \in \mathbb{N}$ such
that for all $n \geq N$, $\sqrt{n} \hat{V}$ obeys the subspace detection property with
probability at least $1 - \epsilon$.  
\end{theorem}

\hypertarget{parameter-estimation}{%
\subsection{Algorithms for Parameter Estimation}\label{parameter-estimation}}

For any edge probability matrix \(P\) for the PABM such that the rows
and columns are organized by community, the \(kl\)\textsuperscript{th}
block is an outer product of two vectors, i.e.,
\(P^{(k \ell)} = \lambda^{(k \ell)} (\lambda^{(\ell k)})^\top\). Therefore, given
\(P^{(k \ell)}\), \(\lambda^{(k \ell)}\) and \(\lambda^{(\ell k)}\) are solvable
up to multiplicative constant using singular value
decomposition. More specifically, 
let \(P^{(k \ell)} = (\sigma^{(k \ell)})^2 u^{(k \ell)} (v^{(k \ell)})^\top\)
be the singular value decomposition of \(P^{(k \ell)}\) where
\(u^{(k \ell)} \in \mathbb{R}^{n_k}\) and 
\(v^{(k \ell)} \in \mathbb{R}^{n_\ell}\) are vectors
and \(\sigma^{(k \ell)}\) is a scalar. 
Then \(\rho_n^{1/2} \lambda^{(k \ell)} = s_1 u^{(k \ell)}\)
and \(\rho_n^{1/2} \lambda^{(\ell k)} = s_2 v^{(k \ell)}\) 
for unidentifiable $s_1 s_2 = (\sigma^{(k \ell)})^2$.
Since each $\lambda^{(k \ell)}$ is not strictly identifiable,
we instead estimate each 
$\tilde{\lambda}^{(k \ell)} = \sigma^{(k \ell)} u^{(k \ell)}$. 
Given the adjacency matrix \(A\)
instead of edge probability matrix \(P\), we can simply use plug-in
estimators by taking the SVD of each $A^{(k \ell)}$ to obtain 
$\hat{\lambda}^{(k \ell)} = \hat{\sigma}^{(k \ell)} \hat{u}^{(k \ell)}$ 
using the largest singular value of $A$ and its corresponding singular vectors. 

\begin{algorithm}[t]
  \label{alg:param_est}
  \DontPrintSemicolon
  \SetAlgoLined
  \caption{PABM parameter estimation.}
  \KwData{Adjacency matrix $A$, community assignments $1, ..., K$}
  \KwResult{PABM parameter estimates $\{\hat{\lambda}^{(k \ell)}\}_K$.}
  Arrange the rows and columns of $A$ by community such that each 
  $A^{(k \ell)}$ block consists of estimated edge probabilities between 
  communities $k$ and $l$.\;
  \For {$k, \ell = 1, ..., K$, $k \leq \ell$} {
    Compute $A^{(k \ell)} = U \Sigma V^\top$, the SVD of the $k\ell$-th 
    block.\;
    Assign $u^{(k \ell)}$ and $v^{(k \ell)}$ as the first columns of $U$ and $V$. 
    Assign $(\sigma^{(k \ell)})^2 \leftarrow \Sigma_{11}$.\;
    Assign $\hat{\lambda}^{(k \ell)} \leftarrow \pm \sigma^{(k \ell)} u^{(k \ell)}$ and 
    $\hat{\lambda}^{(\ell k)} \leftarrow \pm \sigma^{(k \ell)} v^{(k \ell)}$.
  }
\end{algorithm}

\begin{theorem}
\label{theorem6}
Let each $\tilde{\lambda}^{(k \ell)}$ be the popularity vector derived from its corresponding $P^{(k \ell)}$, and let $\hat{\lambda}^{(k \ell)}$ be its estimate from $A^{(k \ell)}$ (Alg. 3). Then if $n \rho_n = \omega( \log^{4}{n})$,
\begin{equation} \label{eq:thm6}
\max_{k, l \in \{1, ..., K\}} 
\|\hat{\lambda}^{(k \ell)} - \tilde{\lambda}^{(k \ell)}\|_{\infty} = 
O\bigg(\frac{\log n_k}{\sqrt{n_k}} \bigg)
\end{equation}
with high probability.
\end{theorem}

\hypertarget{simulated-examples}{%
\section{Simulation Study}\label{simulated-examples}}

For each simulation, community labels are drawn from a multinomial
distribution, the popularity vectors \(\{\lambda^{(k \ell)}\}_K\) are drawn
from two types of joint distributions depending on whether \(k = \ell\),
the edge probability matrix \(P\) is constructed using the popularity
vectors, and finally the adjacency matrix \(A\)
is drawn from \(P\). OSC (Algorithm~\ref{alg:osc}) is then used for community detection, and this
method is compared against (1) SSC using the spectral embedding $\hat{V}$,
c.f. Algorithm~\ref{alg:ssc}, (2) SSC using the rows of the
observed adjacency matrix $A$ as is done in \citet{noroozi2019estimation}
and (3) modularity maximization (MM) as is done in
\citet{307cbeb9b1be48299388437423d94bf1}. 
We also estimate the latent popularity vectors $\{\lambda^{(k \ell)}\}$
by assuming that the true community labels are known and then apply
Algorithm~\ref{alg:param_est}, and we compare this estimation method against an
MLE-based estimator as described in \citet{noroozi2019estimation} and
\citet{307cbeb9b1be48299388437423d94bf1}.

Modularity Maximization is NP-hard, so
\citet{307cbeb9b1be48299388437423d94bf1} used the Extreme Points
(EP) algorithm (\cite{le2016}) as a greedy
relaxation of the optimization problem; the EP algorithm has a running
time of $O(n^{K-1})$ where $n$ is the number of vertices in the graph
and $K$ is the number of communities.
For these simulations we instead replace the EP algorithm with the
Louvain algorithm for modularity maximization,
as the implementation of the EP algorithm in
\citet{307cbeb9b1be48299388437423d94bf1} is too computationally expensive for \(K > 2\). For \(K = 2\), it
was verified that the Louvain algorithm produces comparable results
to EP-MM.

Two implementations of SSC are shown here. The first method, denoted as
SSC-A, treats the columns of the adjacency matrix \(A\) as points in
\(\mathbb{R}^n\), as described in \citet{noroozi2019estimation}.
The second method, denoted as SSC-ASE, first embeds \(A\) and then
performs SSC on the embedding, as described in Algorithm~\ref{alg:ssc}. The 
parameter \(\lambda\) that controls the sparsity of the SSC was chosen via a preliminary cross-validation
experiment. For the final clustering step, a Gaussian Mixture Model was
fit on the normalized Laplacian eigenmap of the affinity matrix \(B\).

For comparing methods, we define the community detection error as:
\[L_c(\hat{\sigma}, \sigma; V) =
\min_\pi \sum_i I(\pi \circ \hat{\sigma}(v_i) = \sigma(v_i))\]
where \(\sigma(v_i)\) is the true community label of vertex \(v_i\),
\(\hat{\sigma}(v_i)\) is the predicted label of \(v_i\), and \(\pi\) is
a permutation operator. This is effectively the ``misclustering count''
of clustering function \(\hat{\sigma}\).

For parameter estimation, because the popularity parameters $\{\lambda_{ik}\}$ are unidentifiable, we instead estimate the edge probabilities $P_{ij} = \lambda_{i z_j} \lambda_{j, z_i}$. The parameter estimation error is the root mean square error between the true and estimated edge probabilities, which is equivalent to the Frobenius norm of $P$ divided by the number of vertices:

$$\mathrm{RMSE}(\hat{P}, P) = \frac{1}{n} \|\hat{P} - P\|_F.$$

We also note that unlike the MLE-based method \citep{307cbeb9b1be48299388437423d94bf1}, the ASE method of Algorithm 3 does not require community labels if all we are interested in is the edge probability matrix, $P$. The estimate $\hat{P}$ we obtain by combining the popularity vectors outputted by Algorithm 3 is equivalent to using the full ASE of $A$ to reconstruct the edge probability matrix. 

\hypertarget{balanced-communities}{%
\subsection{Balanced Communities}\label{balanced-communities}}

In each simulation, community labels \(z_1, ..., z_n\) were drawn from a
multinomial distribution with mixture parameters
\(\{\alpha_1, ..., \alpha_K\}\), then \(\{\lambda^{(k \ell)}\}_K\) according
to the drawn community labels, \(P\) was constructed using the drawn
\(\{\lambda^{(k \ell)}\}_K\), and \(A\) was drawn from \(P\).

For these examples, we set the following parameters:

\begin{itemize}
\tightlist
\item
  Number of vertices \(n = 128, 256, 512, 1024, 2048, 4096\)
\item
  Number of underlying communities \(K = 2, 3, 4\)
\item
  Mixture parameters \(\alpha_k = 1 / K\) for \(k = 1, ..., K\), (i.e.,
  each community label has an equal probability of being drawn)
\item
  Community labels
  \(z_k \stackrel{\text{iid}}{\sim} \Multinomial(\alpha_1, ..., \alpha_K)\)
\item
  Within-group popularities
  \(\lambda^{(kk)} \stackrel{\text{iid}}{\sim} \Betadist(2, 1)\)
\item
  Between-group popularities
  \(\lambda^{(k \ell)} \stackrel{\text{iid}}{\sim} \Betadist(1, 2)\) for
  \(k \neq \ell\)
\end{itemize}
Fifty simulations were performed for each combination of $n$ and $K$.

Fig.~\ref{fig:clust_err_ct_sim} shows the number of mislabeled vertices
going to 0 for large \(n\). SSC on both the embedding and on the
adjacency matrix produces similar trends for \(K > 2\). Weaker
performance of SSC for \(K = 2\) can be attributed to the final spectral
clustering step of the affinity matrix. 
While the subspace detection property is guaranteed for large \(n\), in our
simulations, setting the sparsity parameter to the required value resulted
in more than \(K\) disconnected subgraphs. 
We instead chose a smaller sparsity parameter, 
necessitating a final clustering step. 
A GMM was fit to the normalized Laplacian eigenmap of $B$,
but visual inspection suggests that the communities are not
distributed as a mixture of Gaussians in the eigenmap. 
A different choice of mixture distribution may result in better performance. 

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/clust_err_ct_sim-1}

}

\caption{Median and IQR of community detection error. Communities are approximately balanced. Simulations were repeated 50 times for each sample size.}\label{fig:clust_err_ct_sim}
\end{figure}

Given ground truth community labels, Algorithm~\ref{alg:param_est} and the MLE-based
plug-in estimators perform similarly, with root mean square
error decaying at rate approximately \(n^{-1/2}\) (Fig. \ref{fig:p_block_est}). 

\begin{figure}[H]
{\centering \includegraphics{summary_files/figure-latex/lambda_est_p-1}}
\caption{Median and IQR RMSE for edge probability matrices reconstructed from the outputs of Algorithm 3 (red) compared against outputs of an MLE-based method (blue). Simulations were repeated 50 times for each sample size. Communities were drawn to be approximately balanced.}
\label{fig:p_block_est}
\end{figure}

\hypertarget{imbalanced-communities}{%
\subsection{Imbalanced Communities}\label{imbalanced-communities}}

Simulations performed in this section are the same as those in the
previous section with the exception of the mixture parameters
\(\{\alpha_1, ..., \alpha_K\}\) used to draw community labels from the
multinomial distribution. For these examples, we set the following
parameters:

\begin{itemize}
\tightlist
\item
  Number of vertices \(n = 128, 256, 512, 1024, 2048, 4096\)
\item
  Number of underlying communities \(K = 2, 3, 4\)
\item
  Mixture parameters \(\alpha_k = \frac{k^{-1}}{\sum_{\ell=1}^K \ell^{-1}}\)
  for \(k = 1, ..., K\)
\item
  Community labels
  \(z_k \stackrel{\text{iid}}{\sim} \Multinomial(\alpha_1, ..., \alpha_K)\)
\item
  Within-group popularities
  \(\lambda^{(kk)} \stackrel{\text{iid}}{\sim} \Betadist(2, 1)\)
\item
  Between-group popularities
  \(\lambda^{(k \ell)} \stackrel{\text{iid}}{\sim} \Betadist(1, 2)\) for
  \(k \neq \ell\)
\end{itemize}

\(50\) simulations were performed for each \((n, K)\) pair.

We again see community detection error trending to 0 for OSC, as well as
for SSC when \(K > 2\) (Fig.~\ref{fig:clust_err_ct_sim_imba}). 
The parameter estimation error decays at rate $n^{-1/2}$ as in the balanced communities case
(Fig. \ref{fig:lambda_est_p_imba}). 

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/clust_err_ct_sim_imba-1}
}

\caption{Median and IQR of community detection error. Communities are imbalanced. Simulations were repeated 50 times for each sample size.}\label{fig:clust_err_ct_sim_imba}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/lambda_est_p_imba-1}

}

\caption{Median and IQR RMSE of edge probabilities derived from the outputs of Algorithm 3 (red) compared against an MLE-based method (blue). Simulations were repeated 50 times for each sample size. Communities were drawn to be imbalanced.}
\label{fig:lambda_est_p_imba}
\end{figure}

\hypertarget{real-data-examples}{%
\section{Applications}\label{real-data-examples}}

In the first example, we applied OSC (Algorithm~\ref{alg:osc}) to the Leeds Butterfly
dataset \citep{Wang_2018} consisting of visual similarity measurements
among 832 butterflies across 10 species. The graph was modified to match
the example from \citet{noroozi2019estimation}, i.e., only the $K=4$ most
frequent species were considered, and the similarities were discretized
to \(\{0, 1\}\) via thresholding. Fig.~\ref{fig:butterfly} shows a
sorted adjacency matrix sorted by the resultant clustering.

Comparing against the ground truth species labels, OSC achieves an adjusted Rand index of 92\%, while SSC on the ASE achieves an adjusted Rand index of 96\%. In comparison, SSC on the adjacency matrix yields an adjusted Rand index of 73\% \citep{noroozi2019estimation}.

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/butterfly-1}

}

\caption{Adjacency matrix of the Leeds Butterfly dataset 
after sorting by the clustering outputted by OSC.}\label{fig:butterfly}
\end{figure}

In the second example, we applied OSC to the British MPs Twitter network
\citep{greene2013producing}, the Political Blogs network
\citep{10.1145/1134271.1134277}, and the DBLP network
\citep{NIPS2009_3855, 10.1007/978-3-642-15880-3_42}. For this data
analysis, we subsetted the data as described in
\citet{307cbeb9b1be48299388437423d94bf1} for their analysis of the
same networks. Our methods slightly underperformed compared to modularity
maximization, although performance is comparable. The run time of OSC
is however much smaller than that of modularity maximization.

\begin{figure}[H]
{\centering \includegraphics{summary_files/figure-latex/mp-1}
}
\caption{Adjacency matrices of (from left to right) the British MPs, Political Blogs, and DBLP networks after sorting by the clustering outputted by OSC.}\label{fig:mp}
\end{figure}

\begin{table}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
Network & MM & SSC-ASE & OSC\\
\hline
British MPs & 0.003 & 0.018 & 0.009\\
\hline
Political Blogs & 0.050 & 0.196 & 0.062\\
\hline
DBLP & 0.028 & 0.087 & 0.059\\
\hline
\end{tabular}
\caption{\label{tab:unnamed-chunk-6}Community detection error rates on the British MPs Twitter, Political Blogs, and DBLP networks using modularity maximization, sparse subspace clustering, and OSC.}
\end{table}

In the third example (Fig.~\ref{fig:households-figure} and 
Table~\ref{tab:households-table}), 
we analyzed the Karantaka villages data studied by
\citet{DVN/U3BIHX_2013}. We chose the \texttt{visitgo}
networks from villages 12, 31, and 46 at the household level. 
In these networks, each node is a household and each edge is 
an interaction between members of pairs of households. 
The label of interest is the religious affiliation. 
The networks were truncated to religions ``1'' and ``2'', 
and vertices of degree 0 were removed. 
The villages were chosen based on there being an adequate number of nodes 
between households within each religion. 

\begin{figure}[tp]
{\centering \includegraphics{summary_files/figure-latex/unnamed-chunk-7-1}
}
\caption{Adjacency matrix of the Karnataka villages data, arranged by the clustering produced by OSC (left). The villages studied here are, from left to right, 12, 31, and 46.}\label{fig:households-figure}
\end{figure}

\begin{table}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
Network & MM & SSC-ASE & OSC\\
\hline
Village 12 & 0.270 & 0.291 & 0.227\\
\hline
Village 31 & 0.125 & 0.066 & 0.110\\
\hline
Village 46 & 0.052 & 0.463 & 0.078\\
\hline
\end{tabular}
\caption{\label{tab:households-table}Community detection error rates for identifying household religion.}
\end{table}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

Our central result states that the Popularity Adjusted Block Model 
is a special case of the Generalized Random Dot Product Graph. 
In particular, the PABM with $K$ communities is a GRDPG 
for which the communities are represented by 
mutually orthogonal $K$-dimensional subspaces 
of the $K^2$-dimensional latent space. 
This result extends previous results that connected 
the Stochastic Block Model and the Degree Corrected Block Model 
to Random Dot Product Graphs. 
Replacing RDPGs with GRDPGs is a critical step in this line of research, 
as a PABM is not necessarily a RDPG. 

Because all Bernoulli Graphs are GRDPGs, it should be possible to
invent and study new families of Bernoulli Graphs by characterizing
them as special cases of GRDPGs and exploiting the latent structures
that define them.  The present work illustrates the power of this
approach.  We recover the latent structure of the PABM by Adjacency
Spectral Embedding, then exploit that structure to improve statistical
inference.  Exploiting the fact that PABM communities correspond to
orthogonal subspaces, we propose Orthogonal Spectral Clustering for
community detection and demonstrate that the number of misclassified
vertices approaches zero with high probability as the size of the
graph increases.  This is a stronger result than previously proposed
algorithms \citep{307cbeb9b1be48299388437423d94bf1}, which only
guarantee that the error rate (and not count) approaches zero
asymptotically.  Parameter estimation can be performed in a similar
fashion using the ASE, for which we also prove that the per-parameter
error approaches zero asymptotically.

A secondary benefit of the GRDPG approach is that the latent structure
may be used to improve existing algorithms.  For example, one
algorithm for PABM community detection \citep{noroozi2019estimation}
relies on Sparse Subspace Clustering.  The latent structure of the
PABM provides a natural justification for SSC for the PABM and leads
to an improvement over the previous implementation.  The improved
algorithm applies SSC to the ASE, and we prove that the ASE of the
PABM obeys the Subspace Detection Property with high probability if
the graph is large.

Finally, one might well inquire what one gains and what one sacrifices
by assuming that a Bernoulli Graph is a PABM.  The GRDPG model offers
a plausible way to pursue this inquiry.  Absent a known latent
structure that can be exploited by specialized methods, the GRDPG-ASE
approach transforms the problem of network community detection to the
much-studied problem of clustering vectors in Euclidean space.
Communities of vertices are defined as clusters of latent vectors.
After ASE, a standard clustering algorithm, e.g., single linkage, is
used ot infer the communities.  In future research, we intend to use
such general algorithms as baselines and measure the efficiency of the
PABM algorithms (and other specialized algorithms) by studying how
much they improve on general algorithms when the specified latent
structure obtains.

\appendix
\renewcommand\refname{References}
% \bibliographystyle{plain}
% \bibliography{misc.bib}
\printbibliography

\hypertarget{proofs}{%
\section{Proofs of Theorem~\ref{theorem4}, Theorem~\ref{theorem5}, and Theorem~\ref{theorem6}}}
Let \(V_n\) and \(\hat{V}_n\)
be the $n \times K^2$ matrices whose columns are the eigenvectors of \(P\) and \(A\) corresponding to the
$K^2$ largest eigenvalues (in modulus), respectively. 
We first state an important technical lemma for bounding the maximum
$\ell_2$ norm difference between the rows of $\hat{V}_n$ and
$V_n$. See \citet{cape_biometrika} and 
\citet[Lemma~5]{rubindelanchy2017statistical} for a proof. 
\begin{lemma}
  \label{lem:technical}
Let $A \sim \mathrm{PABM}(\{\lambda^{(k \ell \ell)}\}_{K})$ be a $K$-blocks
PABM graph on $n$ vertices and let \(V_n\) and \(\hat{V}_n\)
be the $n \times K^2$ matrices whose columns are the eigenvectors of \(P\) and \(A\) corresponding to the
$K^2$ largest eigenvalues in modulus, respectively.
Let \((v_n^{(i)})^\top\) and \((\hat{v}_n^{(i)})^\top\) denote the $i$th 
row of \(V_n\) and \(\hat{V}_n\), respectively. 
Then there exists a constant $c > 1$ and an orthogonal matrix $W$ such
that with high probability,
$$\max_{i} \|W \hat{v}_n^{(i)} - v_n^{(i)}\|  = O\Big(\frac{\log^{c}n}{n \sqrt{\rho_n}} \Big).$$
In particular we can take $c = 1 + \epsilon$ for any $\epsilon > 0$. 
\end{lemma}


%
%then the \(ij\)\textsuperscript{th} element of
%\(n X (X^\top X)^{-1} X^\top = n V V^\top = B\) is 0.

\begin{proof}[Proof of Theorem~\ref{theorem4}]
  Recall the notations in Lemma~\ref{lem:technical} and note that,
  under our assumption that the latent vectors $\lambda^{(k \ell)}$
  are all homogeneous, we have $\max_{i} \|v_n^{(i)}\| =
  O(n^{-1/2})$. 
%   Let \(V_n\) and \(\hat{V}_n\)
% be the $n \times K^2$ matrices whose columns are the eigenvectors of \(P\) and \(A\) corresponding to the
% \(K (K + 1) / 2\) most positive and \(K (K - 1) / 2\) most negative
% eigenvalues, respectively. From Lemma~5 in
% \citet{rubindelanchy2017statistical} we have, for some
% \(W \in \mathbb{O}(K^2)\) and \(c > 0\), that
% $$\|\hat{V} W - V\|_{2 \to \infty} = O_P \big(\frac{\log^{c}n}{n \sqrt{\rho_n}} \big).$$
% We furthermore have \(\|V\|_{2 \to \infty} = O_P(n^{-1/2})\). 
% Let
% \((v_n^{(i)})^\top\) and \((\hat{v}_n^{(i)})^\top\) denote the $i$th 
% row of \(V_n\) and \(\hat{V}_n\), respectively. 
Next recall Theorem~\ref{theorem3}; in particular $B_{ij} = n(v_n^{(i)})^{\top}
v_n^{(j)}$. % Now suppose that vertices $i$ and $j$ belongs to different
% communities. Then $B_{ij} = 0$ and hence
% Then for \(i\) and
% \(j\) belonging to different
% communities, we ha using the fact that \((v_n^{(i)})^\top v_n^{(j)} = 0\),
We therefore have
\[\begin{split}
 \max_{ij} |\hat{B}_{ij} - B_{ij}| &= \max_{ij} n |(\hat{v}_n^{(i)})^\top \hat{v}_n^{(j)} -
(v_n^{(i)})^\top v_n^{(j)}| \\
& \leq n \max_{ij} |(\hat{v}_n^{(i)})^\top W W^\top \hat{v}_n^{(j)} -
(v_n^{(i)})^\top v_n^{(j)}| \\
& \leq n \max_{i,j} \Bigl(\|W^{\top} \hat{v}_n^{(i)} - v_n^{(i)}\| \times \|\hat{v}_n^{(j)}\|
+ \|W^{\top} \hat{v}_n^{(j)} - v_n^{(j)}\| \times \|v_n^{(i)}\|\Bigr) \\
% & \leq 
% \hat{V}_n W^\top W \hat{V}_n - V_n V_n^\top||_{2 \to \infty} \\
% & = ||2 \hat{V}_n W V_n^\top - 2 V_n V_n^\top + \hat{V}_n W W^\top V_n^\top - 
% 2 \hat{V}_n W V_n^\top + V_n V_n^\top||_{2 \to \infty} \\
% & = ||2 (\hat{V}_n W - V_n) V_n^\top + 
% (\hat{V}_n W - V_n) (\hat{V}_n W - V_n)^\top ||_{2 \to \infty} \\
% & \leq 2 
% &\leq  n \Bigl(\max_{ij}  \|W \hat{v}_n^{(i)}  - v_n^{(i)} \| \times \|\hat{v}_n^{(j)}\| +
% n \|W \hat{v}_n^{(j)}  - v_n^{(j)} \| \times \|v_n^{(i)}\|\Bigr)
% \\
& \leq n \Bigl(\max_{ij}  \|W \hat{v}_n^{(i)}  - v_n^{(i)} \|^2 +  \|W
\hat{v}_n^{(i)}  - v_n^{(i)} \| \times \|v_n^{(j)}\| +  \|W \hat{v}_n^{(j)}  - v_n^{(j)} \| \times \|v_n^{(i)}\|\Bigr) 
\\ &
\leq n \max_{i} \|W \hat{v}_n^{(i)}  - v_n^{(i)} \|^2 + 2n \max_{i}
\|W \hat{v}_n^{(i)}  - v_n^{(i)} \| \times \max_{j} \|v_n^{(j)}\|
\\
& = O \Big( \frac{\log^{c}{n}}{n^{1/2} \rho_n^{1/2}} \Big)
\end{split}\]
with high probability.
Theorem~\ref{theorem4} follows from the above bound together with the
conclusion in Theorem~\ref{theorem3} that $B_{ij} = 0$ whenever vertices $i$ and $j$
belongs to different communities. 
\end{proof}
% Scaling all terms in the above inequality by \(n\), we obtain
% $$|n (\hat{v}_n^{(i)})^\top \hat{v}_n^{(j)}| = O_P \Big( \frac{(\log n)^c}{\sqrt{n \rho_n}} \Big)$$
% as desired.

We now provide a proof of Theorem~\ref{theorem5}. Our proof is based
on verifying the sufficient conditions given in Theorem~6 of
\citet{jmlr-v28-wang13}
under which sparse subspace clustering based on solving the
optimization problem in Eq.~\eqref{eq:ssc} yields an affinity matrix
$B = |C| + |C^{\top}|$ satisfying the subspace detection property of
Definition~\ref{def:subspace_detection}. We first recall a few
definitions used in \citet{soltanolkotabi2012} and \citet{jmlr-v28-wang13}; for ease of exposition,
these definitions are stated using the notations of the current
paper and we will drop the explicit dependency on $n$ from our
eigenvectors $\hat{V}_n$ of $A$ and $V_n$ of $P$.
 % We first recall the
% notion of the inradius of a set of points. 
\begin{definition}[Inradius]
  \label{inradius}
The inradius of a convex body $\mathcal{P}$, denoted by $r(\mathcal{P})$, is
defined as the radius of the largest Euclidean ball inscribed in $\mathcal{P}$.
Let $X$ be a $n \times d$ matrix with rows $x_1, x_2, \dots,
x_n$. We then define, with a slight abuse of notation, $r(X)$ as the
inradius of the convex hull formed by $\{\pm x_1, \pm x_2, \dots, \pm x_n\}$. 
\end{definition}

\begin{definition}[Subspace incoherence]
  \label{def:subspace_incoherence}
Let $\hat{V}$ be the eigenvectors of $A$
corresponding to the $K^2$ largest eigenvalues in modulus. Let
$\hat{V}^{(k)}$ denote the matrix formed by keeping only the rows of
$\hat{V}$ corresponding to the $k^{th}$
community and let $\hat{V}^{(-k)}$ denote the matrix formed by
omitting the rows of $\hat{V}$
corresponding to the $k^{th}$ community. Let $(\hat{v}_i^{(k)})^\top$ denote
the $i$th row of $\hat{V}^{(k)}$ and $\hat{V}_{-i}^{(k)}$ be $\hat{V}^{(k)}$ with
the $i^{th}$ row omitted. Let $V$, $V^{(k)}$, $V^{(-k)}$, and
$v_i^{(k)}$ be defined similarly using the eigenvectors $V$ of
$P$. Finally let $\mathcal{S}^{(k)}$ be the vector space spanned by the
rows of $V^{(k)}$. 

Now define $\nu_{i}^{(k)}$ for $k = 1,2,\dots,K$ and $i =
1,2,\dots,n_{k}$ as the solution of the following optimization problem
$$\nu_{i}^{(k)} = \max_\eta (\hat{v}_i^{(k)})^\top \eta - \frac{1}{2
  \lambda} \eta^\top \eta, \quad \text{subject to $\|V_{-i}^{(k)}
  \eta\|_\infty \leq 1$.}$$
Given $\nu_i^{(k)}$, let $\mathbb{P}_{\mathcal{S}^{(k)}}(\nu_i^{(k)})$
be the vector in $\mathbb{R}^{K^2}$ corresponding to the orthogonal projection of $\nu_i^{(k)}$ onto the vector space
$\mathcal{S}^{(k)}$ and define the projected dual direction $w_{i}^{(k)}$
as
$$w_i^{(k)} =
\frac{\mathbb{P}_{\mathcal{S}^{(k)}}(\nu_i^{(k)})}{\|\mathbb{P}_{\mathcal{S}^{(k)}}(\nu_i^{(k)})\|}.$$
Now let $W^{(k)} = \bigl[ w_1^{(k)} \mid \cdots \mid w_{n_k}^{(k)} \bigr]^\top$
and define the subspace incoherence for $\hat{V}^{(k)}$ by
$$\mu^{(k)} = \mu(\hat{V}^{(k)}) = \max\limits_{v \in V^{(-k)}} \|W^{(k)} v\|_\infty.$$
\end{definition}

With the above definitions in place, we are now ready to state our
proof of Theorem~\ref{theorem5}.

\begin{proof}[Proof of Theorem~\ref{theorem5}]
For a given $k = 1,2\dots,K$, let $r^{(k)} = \min_{i}r(V_{-i}^{(k)})$ be inradius of the convex hull formed by
the rows of $V_{-i}^{(k)}$ and let $r_* = \min_{k} r^{(k)}$. Then Theorem~6 in
\citet{jmlr-v28-wang13} states that there exists a $\lambda > 0$
such that $\sqrt{n} \hat{V}$ satisfies
the subspace detection property in
Definition~\ref{def:subspace_detection} whenever
the following two conditions are satisfied
\begin{gather}
  \label{eq:cond1}
  \mu^{(k)} < r^{(k)} \quad \text{for all $k = 1,2,\dots,K$}, \\
  \label{eq:cond2}
  \max_{i} \|W \hat{v}_{i} - v_{i}\| \leq \min_{k} \frac{r_*(r^{(k)} -
    \mu^{(k)})}{2 + 7 r^{(k)}}.
\end{gather}
We now verify that for sufficiently large n, Eq.~\eqref{eq:cond1} and Eq.~\eqref{eq:cond2}
holds with high probability.

{\bf Verifying Eq.~\eqref{eq:cond1}}. If $n$ is sufficiently large then
there are enough vertices in each community $k$ so that
$\mathrm{span}(V_{-i}^{(k)}) = \mathcal{S}^{(k)}$ for all $i$ and hence
\(r^{(k)} = \min_{i} r(V_{-i}^{(k)}) > 0\) for
all $k = 1,2,\dots,K$. 

Next, by Theorem \ref{theorem3} we have that the subspaces
  $\{\mathcal{S}^{(1)}, \dots, \mathcal{S}^{(K)}\}$
  are mutually orthogonal, i.e., $v^{\top} w = 0$ for all $v \in
  \mathcal{S}^{(k)}$ and $w \in \mathcal{S}^{(\ell)}$ with $k \not =
  \ell$. Now let $z \in \mathbb{R}^{K^2}$ be arbitrary and let
  $\tilde{z} = \mathbb{P}_{\mathcal{S}^{(k)}} z$ be the projection of
    $z$ onto $\mathcal{S}^{(k)}$. We then have $v^{\top} \tilde{z} =
    0$ for all $v \in V^{(-k)}$. Since $z$ is arbitrary, this implies $\|W^{(k)} v\|_{\infty} = 0$ for all $v
    \in V^{(-k)}$ and hence $\mu^{(k)} = 0$ for all $k
    =1,2,\dots,K$. Therefore $\mu^{(k)} < r^{(k)}$ for all $k =
    1=2,\dots,K$ as desired.

{\bf Verifying Eq.~\eqref{eq:cond2}}.
Let $\delta = \max_{i} \sqrt{n} \|W
\hat{v}_{i} - v_{i}\|$. Then from Lemma~\ref{lem:technical}, we have
\(\delta \stackrel{a.s.}{\to} 0\) and hence
$$\delta < \min_{k} \frac{r_* (r^{(k)} - \mu^{(k)})}{2 + 7 r^{(k)}}$$
asymptotically almost surely. 

In summary $\sqrt{n} \hat{V}$ satisfies the subspace detection property
with probability converging to $1$ as \(n \to \infty\).
\end{proof}

\begin{remark}
Theorem 6 of \citet{jmlr-v28-wang13} assumes that each row $v_i$
of $V$ has unit norm, i.e., $\|v_{i}\| = 1$  for all $i$. This
assumption has the effect of scaling the $r^{(k)}$ so that $r^{(k)}
\leq 1$ for all $k = 1,2,\dots,K$. We emphasize that this assumption
has no effect on the proof of Theorem \ref{theorem5}. Indeed,
since $\mu^{(k)} = 0$ for all $k$, as long as the rows of $V^{(k)}$
spans the subspace $\mathcal{S}^{(k)}$, then $a r^{(k)} > \mu^{(k)}$ 
for any scalar $a > 0$. 
\end{remark}

\begin{proof}[Proof of Theorem \ref{theorem6}] Let \(P\) be
organized by community such that \(P^{(k \ell)}\) denote the $n_k \times
n_{\ell}$ matrix obtained by keeping only the rows of $P$
corresponding to vertices in community $k$ and the columns of $P$
corresponding to vertices in community $\ell$. We define $A^{(k
  \ell)}$ analogously. Recall that $P^{(k \ell)} = \lambda^{(k \ell)} (\lambda^{(\ell k)})^{\top}$ for all $k, \ell$. We now consider estimation of $P^{(k \ell)}$
for the cases when $k = \ell$ versus when $k \not = \ell$.

\emph{Case \(k = l\)}. Let $P^{(kk)} =
\sigma_{kk}^2 u^{(kk)} (u^{(kk)})^\top$ be the singular value
decomposition of $P^{(kk)}$. We can then define
$\tilde{\lambda}^{(kk)} = \sigma_{kk} u^{(kk)}$. 
Now let $\hat{U}^{(kk)} \hat{\Sigma}^{(kk)} (\hat{U}^{(kk)})^\top$ be the
singular value decomposition of \(A^{(kk)}\), and let
$\hat{\sigma}_{kk}^2 \hat{u}^{(kk)} (\hat{u}^{(kk)})^\top$ be the
best rank-one approximation of $A^{(kk)}$. Define
\(\hat{\lambda}^{(kk)} = \hat{\sigma}_{kk} \hat{u}^{(kk)}\). Then
\(\hat{\lambda}^{(kk)}\) is the adjacency spectral embedding approximation of \(\lambda^{(kk)}\)
and by Theorem 5 of \citet{rubindelanchy2017statistical}, we have
%adjacency spectral embedding \(\hat{\lambda}^{(kk)}\) approximates
$$\|\hat{\lambda}^{(kk)} - \lambda^{(kk)}\|_{\infty} = O\Bigl(\frac{\log n_k}{\sqrt{n_k}}\Bigr)$$
with high probability. 
Here $\|\cdot\|_{\infty}$ denote the $\ell_{\infty}$ norm of a vector.

\emph{Case \(k \neq l\)}. % \(P^{(kl)}\) and \(A^{(kl)}\) represent edge
% probabilities and edges between communities \(k\) and \(l\). Note that
% \(P^{(kl)} = (P^{(lk)})^\top\).\\
% By definition, \(P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top\). 
Let \(P^{(k \ell)} = \sigma_{k \ell}^2 u^{(k \ell)} (v^{(k \ell)})^\top\) and 
\(P^{(\ell k)} = \sigma_{kl}^2 u^{(\ell k)} (v^{(\ell k)})^\top\)  be the singular
value decompositions and note that \(\sigma_{k \ell} = \sigma_{\ell
  k}\), \(u^{(k \ell)} = v^{(\ell k)}\), and
\(v^{(k \ell)} = u^{(\ell k)}\). 
Now define \(\lambda^{(k \ell)} = \sigma_{k \ell} u^{(k \ell)}\) and
\(\lambda^{(\ell k)} = \sigma_{k \ell}
v^{(k \ell)}\).

% (We can also note that
% the SVD of \(P^{(lk)} = \sigma_{kl}^2 v^{(kl)} (u^{(kl)})^\top\), i.e.,
% \(\sigma_{kl} = \sigma_{lk}\), \(u^{(kl)} = v^{(lk)}\), and
% \(v^{(kl)} = u^{(lk)}\).)\\
Next consider the Hermitian dilation

\[M^{(k \ell)} = 2 \begin{bmatrix} 0 & P^{(k \ell)} \\ P^{(\ell k)} & 0 \end{bmatrix}\]

which is a symmetric \((n_k + n_\ell) \times (n_k + n_\ell)\) matrix. The
eigendecomposition of \(M^{(k \ell)}\) is then

\[M^{(k \ell)} = 
\begin{bmatrix} u^{(k \ell)} & -u^{(k \ell)} \\ v^{(k \ell)} & v^{(k \ell)} \end{bmatrix} \times 
\begin{bmatrix} \sigma^2_{kl} & 0 \\ 0 & -\sigma^2_{kl} \end{bmatrix} \times
\begin{bmatrix} u^{(k \ell)} & -u^{(k \ell)} \\ v^{(k \ell)} & v^{(k \ell)} \end{bmatrix}^\top\]
Thus treating \(M^{(k \ell)}\) as the edge probability matrix of a GRDPG, we
have latent positions in \(\mathbb{R}^2\) given by the $(n_k + n_{\ell}) \times 2$ matrix
\[\Lambda^{(k \ell)} = \begin{bmatrix} 
  \sigma_{k \ell} u^{(k \ell)} & \sigma_{k \ell} u^{(k \ell)} \\ 
  \sigma_{k \ell} v^{(k \ell)} & -\sigma_{k \ell} v^{(k \ell)} 
\end{bmatrix} = 
\begin{bmatrix} 
  \lambda^{(k \ell)} & \lambda^{(k \ell)} \\ 
  \lambda^{( \ell k)} & -\lambda^{( \ell k)} 
\end{bmatrix}.\]
Now consider
\[\hat{M}^{(k \ell)} = \begin{bmatrix} 0 & A^{(k \ell)} \\ A^{(\ell k)} & 0 \end{bmatrix}\]
% Then \(\hat{M}^{(kl)} = M^{(kl)} + E'\) where

% \[E' = \begin{bmatrix} 0 & E \\ E^\top & 0 \end{bmatrix}\]

% and \(E\) is the \(n_k \times n_\ell\) matrix of independent noise (to
% generate the Bernoulli entries in \(A^{(kl)}\).\\
We can then view \(\hat{M}^{(k \ell)}\) as an adjacency matrix drawn from
the edge probabilities matrix \(M^{(k \ell)}\). Now suppose that the adjacency spectral
embedding of $\hat{M}^{(k \ell)}$ is represented as the $(n_k +
n_{\ell}) \times 2$ matrix
\[\hat{\Lambda}^{(k \ell)} = \begin{bmatrix} 
  \hat{\lambda}^{(k \ell)} & \hat{\lambda}^{(k \ell)} \\ 
  \hat{\lambda}^{(\ell k)} & -\hat{\lambda}^{(\ell k)} 
\end{bmatrix}\]
where each \(\hat{\lambda}^{(k \ell)}\) is defined as in Algorithm 3. Then
by Theorem 5 of \citet{rubindelanchy2017statistical}, there
exists an indefinite orthogonal transformation $W^{*}$ such that,  with
high probability,
$$\max_{i} |W^{*} \hat{\Lambda}_{i}^{(k \ell)} - \Lambda_{i}^{(k \ell)} \| =
O\Bigl(\frac{\log (n_k + n_{\ell})}{\sqrt{n_k + n_{\ell}}}\Bigr)$$
with high probability. Here $\Lambda_{i}^{(k \ell)}$ and
$\hat{\Lambda}_i^{(k \ell)}$ denote the $i$th rows of $\Lambda^{(k
  \ell)}$ and $\hat{\Lambda}^{(k \ell)}$, respectively. 
% approximates the latent positions of \(M^{(kl)}\) up to indefinite
% orthogonal transformation by the rate given in Theorem 5 of
% \citet{rubindelanchy2017statistical}.\\
Furthermore, by looking at the proof of Theorem~5 in
\citep{rubindelanchy2017statistical}, we see that $W^{*}$ is also
blocks diagonal with $2$ blocks where the positive eigenvalues of $M^{(k \ell)}$
forming a block and the negative eigenvalues of $M^{(k \ell)}$ forming
the remaining block. % \(U^\top \hat{U}\). 
Since $M^{(k \ell)}$ has one positive eigenvalue and one negative
eigenvalue, we see that $W^{*}$ is necessarily of the form $W^{*}
= \Bigl[\begin{smallmatrix} 1 & 0 \\ 0 & - 1\end{smallmatrix}\Bigr]$
% \((1, 1)\), and \(U^\top \hat{U}\) is block
% diagonal, resulting in \(W_* \stackrel{a.s.}{\to} I\). 
Using this form for $W^{*}$, we obtain
$$\max\{\|\hat{\lambda}^{(k \ell)} - \lambda^{(k \ell)}\|_{\infty},
\|\hat{\lambda}^{(\ell k)} - \lambda^{(\ell k)}\|_{\infty}\}  =
O\Bigl(\frac{\log(n_k + n_{\ell})}{\sqrt{n_k + n_{\ell}}}\Bigr)$$
with high probability. Combining this bound with the bound for
$\|\hat{\lambda}^{(kk)} - \lambda^{(kk)}\|_{\infty}$ given above
yields Eq.~\eqref{eq:thm6} in Theorem~\ref{theorem6}. 
\end{proof}
\end{document}
