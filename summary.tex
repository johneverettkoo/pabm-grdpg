% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  12pt,
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Connecting the Popularity Adjusted Block Model to the Generalized Random Dot Product Graph for Community Detection and Parameter Estimation},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{float}
\usepackage{mathtools}
\usepackage{natbib}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\setcitestyle{numbers,square,comma}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage{comment}
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{Connecting the Popularity Adjusted Block Model to the Generalized Random
Dot Product Graph for Community Detection and Parameter Estimation}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle
\begin{abstract}
We connect two random graph models, the Popularity Adjusted Block Model
(PABM) and the Generalized Random Dot Product Graph (GRDPG),
demonstrating that a PABM is a GRDPG in which communities correspond to
certain mutually orthogonal subspaces of latent positions. This insight
leads to the construction of improved algorithms for community detection
and parameter estimation with PABM. Using established asymptotic
properties of Adjacency Spectral Embedding (ASE) for GRDPG, we derive
asymptotic properties of these algorithms, including algorithms that
rely on Sparse Subspace Clustering (SSC). We illustrate these properties
via simulation.
\end{abstract}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\newcommand{\diag}{\text{diag}}
\newcommand{\tr}{\text{Tr}}
\newcommand{\blockdiag}{\text{blockdiag}}
\newcommand{\indep}{\stackrel{\text{indep}}{\sim}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\Bernoulli}{\text{Bernoulli}}
\newcommand{\Betadist}{\text{Beta}}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{example}
\newtheorem*{example}{Example}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Statistical analysis on graphs or networks often involves the
partitioning of a graph into disconnected subgraphs or clusters. This is
often motivated by the assumption that there exist underlying and
unobserved communities to which each vertex of the graph belongs, and
edges between pairs of vertices are determined by drawing from a
probability distribution based on the community relationships between
each pair. The goal of the analysis then is population community
detection, or the recovery of the true underlying community labels for
each vertex, up to permutation (with some additional parameter
estimation being of possible interest), assuming some underlying
probability model. One such model is the Stochastic Block Model (SBM),
first proposed by \citet{doi:10.1080/0022250X.1971.9989788}, which
assumes that the edge probability from one vertex to another follows a
Beronulli distribution with fixed probabilities for each pair of
community labels. Other random graph models have been proposed and
studied, such as the Degree-Corrected Block Model (DCBM), introduced by
\citet{Karrer_2011}, which is a generalization of the SBM. The
Popularity Adjusted Block Model (PABM) was then introduced by
\citet*{307cbeb9b1be48299388437423d94bf1} as a generalization of the
DCBM to address the heterogeneity of edge probabilities within and
between communities while still maintaining distinct community
structure.

The underlying similarity among the SBM, PABM, and other such models is
that they involve a symmetric edge probability matrix
\(P \in [0, 1]^{n \times n}\) where \(n\) is the number of vertices in
the graph. An undirected and unweighted graph is then drawn from this
edge probability matrix such that the existence of an edge between each
pair of vertices \(i\) and \(j\) is given by
\(\text{Bernoulli}(P_{ij})\). For example, for the SBM with two
communities for which the within-community edge probability is \(\xi\)
and the between-community edge probability is \(\eta\), the entries of
\(P\) consist of \(P_{ij} = \xi\) if \(i\) and \(j\) are in the same
community and \(i \neq j\), \(P_{ij} = \eta\) if \(i\) and \(j\) belong
to separate communities, and \(P_{ii} = 0\).

The Random Dot Product Graph (RDPG) model proposed by
\citet*{10.1007/978-3-540-77004-6_11} is another graph model with
Bernoulli edge probabilities. Under this model, each vertex of the graph
can be represented by a point in some latent space such that the edge
probability between any pair of vertices is given by their corresponding
dot product in the latent space, i.e., given a latent positions
\(x_1, ..., x_n \in \mathbb{R}^d\), the edge probability matrix is
\(P = X X^\top\) where
\(X = \begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix}^\top\). The SBM
is equivalent to a special case of the RDPG model in which all vertices
of a given community share the same position in the latent space
\cite{lyzinski2014}. It has also been shown that similar random graph
models, including the DCBM, can be represented in this way
\cite{lyzinski2014} \cite{rubindelanchy2017consistency}. An analogous
property exists for the PABM but not for the RDPG model but under the
\emph{Generalized} Random Dot Product Graph (GRDPG) model. This
relationship will be explored in this paper and exploited to construct
algorithms for community detection and parameter estimation for the
PABM.

In this paper, we will only consider undirected graphs, that is the edge
weight from vertex \(i\) to vertex \(j\) is equal to the edge weight in
the opposite direction, from vertex \(j\) to vertex \(i\). Furthermore,
we will only consider unweighted graphs with binary (\(0\) or \(1\))
edge weights We will also assume that graphs are hollow, i.e., there are
no edges from a vertex to itself. All such graphs can be represented by
a symmetric adjacency matrix \(A \in \{0, 1\}^{n \times n}\) for which
\(A_{ij} = 1\) if there exists an edge between vertices \(i\) and \(j\)
and \(0\) otherwise, and \(A\) is an element-wise independent Bernoulli
draw from a symmetric edge probability matrix
\(P \in [0, 1]^{n \times n}\).

\hypertarget{connecting-the-popularity-adjusted-block-model-to-the-generalized-random-dot-product-graph}{%
\section{Connecting the Popularity Adjusted Block Model to the
Generalized Random Dot Product
Graph}\label{connecting-the-popularity-adjusted-block-model-to-the-generalized-random-dot-product-graph}}

\hypertarget{the-popularity-adjusted-block-model-and-the-generalized-random-dot-product-graph}{%
\subsection{The popularity adjusted block model and the generalized
random dot product
graph}\label{the-popularity-adjusted-block-model-and-the-generalized-random-dot-product-graph}}

\begin{definition}[Popularity Adjusted Block Model]
\label{pabm}
Let $P \in [0, 1]^{n \times n}$ be a symmetric edge probability matrix for a 
set of $n$ 
vertices, $V$. Each vertex has a community label $1, ..., K$, and the rows and 
columns of $P$ are arranged by community label such that $n_k \times n_l$ block 
$P^{(kl)}$ describes the edge probabilities between vertices in communities 
$k$ and $l$ ($P^{(lk)} = (P^{(kl)})^\top$). 
Let graph $G = (V, E)$ be an undirected, unweighted graph such 
that its corresponding adjacency matrix $A \in \{0, 1\}^{n \times n}$ is a 
realization of $\text{Bernoulli}(P)$, i.e., 
$A_{ij} \stackrel{\text{indep}}{\sim}\text{Bernoulli}(P_{ij})$ for $i > j$ 
($A_{ij} = A_{ji}$ and $A_{ii} = 0$). 

If each block $P^{(kl)}$ can be written as the outer product of two vectors:

\begin{equation} \label{eq:pabm}
  P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top
\end{equation}

for a set of $K^2$ fixed vectors $\{\lambda^{(st)}\}_{s, t = 1}^K$ where each 
$\lambda^{(st)}$ is a column vector 
of dimension $n_s$, then graph $G$ and its corresponding adjacency matrix $A$ 
is a realization of a popularity adjusted block model with parameters 
$\{\lambda^{(st)}\}_{s, t = 1}^K$. 
\end{definition}

We will use the notation \(A \sim PABM(\{\lambda^{(kl)}\}_K)\) to denote
a random adjacency matrix \(A\) drawn from a PABM with parameters
\(\lambda^{(kl)}\) consisting of \(K\) underlying communities.

\begin{definition}[Generalized Random Dot Product Graph] 
\label{grdpg} 
Let $P \in [0, 1]^{n \times n}$ be a symmetric edge probability matrix for a 
set of $n$ vertices, $V$. If $\exists X \in \mathbb{R}^{n \times d}$ such that 

\begin{equation} \label{eq:grdpg}
  P = X I_{pq} X^\top
\end{equation}

for some $d, p, q \in \mathbb{N}$ and $p + q = d$, then 
graph $G = (V, E)$ with adjacency matrix $A$ such that 
$A_{ij} \stackrel{\text{indep}}{\sim}\text{Bernoulli}(P_{ij})$ for $i > j$ ($A_{ij} = A_{ji}$ and 
$A_{ii} = 0$) is a draw from the generalized random dot product graph model 
with latent positions $X$ and signature $(p, q)$. More precisely, if vertices 
$i$ and $j$ have latent positions $x_i$ and $x_j$ respectively, then the edge 
probability between the two is $P_{ij} = x_i^\top I_{pq} x_j$, and $X$ contains 
the latent positions as rows $x_i^\top$. 
\end{definition}

We will use the notation \(A \sim GRDPG_{p,q}(X)\) to denote a random
adjacency matrix \(A\) drawn from latent positions \(X\) and signature
\((p, q)\).

\begin{definition}[Indefinite Orthogonal Group] 
The indefinite orthogonal group with signature $(p, q)$ is 
the set $\{Q \in \mathbb{R}^{d \times d} : Q I_{pq} Q^{\top} = I_{pq}\}$, 
denoted as $\mathbb{O}(p, q)$.
\end{definition}

\begin{remark}
Like the RDPG, the latent positions of a GRDPG are not unique 
\cite{rubindelanchy2017statistical}. 
More specifically, if $P_{ij} = x_i^\top I_{pq} x_j$, then we also have for any 
$Q \in \mathbb{O}(p, q)$, 
$(Q x_i)^\top I_{pq} (Q x_j) = x_i^\top (Q^\top I_{pq} Q) x_j = 
x_i^\top I_{pq} x_j = P_{ij}$. 
Unlike in the RDPG case, transforming the latent positions via multiplication 
by $Q \in \mathbb{O}(p, q)$ does not necessarily maintain interpoint angles or 
distances. 
\end{remark}

\hypertarget{connecting-the-pabm-to-the-grdpg}{%
\subsection{Connecting the PABM to the
GRDPG}\label{connecting-the-pabm-to-the-grdpg}}

\begin{theorem}[Connecting the PABM to the GRDPG for $K = 2$]
\label{theorem1}  
Let 

$$X = \begin{bmatrix} 
\lambda^{(11)} & \lambda^{(12)} & 0 & 0 \\ 
0 & 0 & \lambda^{(21)} & \lambda^{(22)} 
\end{bmatrix}$$

$$U = \begin{bmatrix} 1 & 0 & 0 & 0 \\
0 & 0 & 1 / \sqrt{2} & 1 / \sqrt{2} \\
0 & 0 & 1 / \sqrt{2} & - 1 / \sqrt{2} \\
0 & 1 & 0 & 0 \end{bmatrix}$$

where each $\lambda^{(kl)}$ is a vector as in Definition 1. 
Then $A \sim GRDPG_{3, 1}(X U)$ and $A \sim PABM(\{(\lambda^{(kl)}\}_2)$ are 
equivalent.
\end{theorem}

\begin{theorem}[Generalization to $K > 2$] 
\label{theorem2}
There exists a block diagonal matrix 
$X \in \mathbb{R}^{n \times K^2}$ defined by PABM parameters 
$\{\lambda^{(kl)}\}_K$ and orthonormal matrix 
$U \in \mathbb{R}^{K^2 \times K^2}$ that is fixed 
for each $K$ such that $A \sim GRDPG_{K (K+1) / 2, K (K-1) / 2}(XU)$ and 
$A \sim PABM(\{(\lambda^{(kl)}\})_K)$ are equivalent.
\end{theorem}

\begin{proof}
Define the following matrices from $\{\lambda^{(kl)}\}_K$: 

$$\Lambda^{(k)} = 
\begin{bmatrix} \lambda^{(k,1)} & \cdots & \lambda^{(k, K)} \end{bmatrix}
\in \mathbb{R}^{n_k \times K}$$

\begin{equation} \label{eq:xy}
X = \text{blockdiag}(\Lambda^{(1)}, ..., \Lambda^{(K)}) \in \mathbb{R}^{n \times K^2}
\end{equation}

$$L^{(k)} = \text{blockdiag}(\lambda^{(1k)}, ..., \lambda^{(Kk)}) \in 
\mathbb{R}^{n \times K}$$

$$Y = \begin{bmatrix} L^{(1)} & \cdots & L^{(K)} \end{bmatrix} \in 
\mathbb{R}^{n \times K^2}$$

Then $P = X Y^\top$.

Similar to the $K = 2$ case, we have $Y = X \Pi$ for a permutation matrix
$\Pi$, resulting in $P = X \Pi X^\top$.  
The permutation described by $\Pi$ has $K$ fixed points, which correspond to 
$K$ eigenvalues equal to $1$ with corresponding eigenvectors $e_k$ where 
$k = r (K + 1) + 1$ for $r = 0, ..., K - 1$. It also has 
$\binom{K}{2} = K (K - 1) / 2$ cycles of order $2$. Each cycle corresponds to 
a pair of eigenvalues $+1$ and $-1$ and a pair of eigenvectors 
$(e_s + e_t) / \sqrt{2}$ and $(e_s - e_t) / \sqrt{2}$.

Then $\Pi$ has $K (K + 1) / 2$ eigenvalues equal to $1$ and $K (K - 1) / 2$ 
eigenvalues equal to $-1$. $\Pi$ has the decomposed form 

\begin{equation} \label{eq:permutation}
\Pi = U I_{K (K + 1) / 2, K (K - 1) / 2} U^\top
\end{equation}

The edge probability matrix then can be written as:

\begin{equation} \label{eq:pabm-grdpg}
P = X U I_{p, q} (X U)^\top
\end{equation}

\begin{equation} \label{eq:p}
p = K (K + 1) / 2
\end{equation}

\begin{equation} \label{eq:q}
q = K (K - 1) / 2
\end{equation}

and we can describe the PABM with $K$ communities as a GRDPG with latent 
positions $X U$ with signature $\big( K (K + 1) / 2, K (K - 1) / 2 \big)$.
\end{proof}

\begin{example}[$K = 3$] Using the same notation as in Theorem \ref{theorem2}:

$$X = \begin{bmatrix} 
\lambda^{(11)} & \lambda^{(12)} & \lambda^{(13)} & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & \lambda^{(21)} & \lambda^{(22)} & \lambda^{(23)} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & \lambda^{(31)} & \lambda^{(32)} & \lambda^{(33)}
\end{bmatrix}$$

$$Y = \begin{bmatrix} 
\lambda^{(11)} & 0 & 0 & \lambda^{(12)} & 0 & 0 & \lambda^{(13)} & 0 & 0 \\
0 & \lambda^{(21)} & 0 & 0 & \lambda^{(22)} & 0 & 0 & \lambda^{(23)} & 0 \\
0 & 0 & \lambda^{(31)} & 0 & 0 & \lambda^{(32)} & 0 & 0 & \lambda^{(33)}
\end{bmatrix}$$

Then $P = X Y^\top$ and $Y = X \Pi$ where $\Pi$ is a permutation matrix 
consisting of $3$ fixed points and $3$ cycles of order 2:

$$\Pi = \begin{bmatrix} 
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1
\end{bmatrix}$$

* Positions 1, 5, 9 are fixed.

* The cycles of order 2 are $(2, 4)$, $(3, 7)$, and $(6, 8)$.
    
Therefore, we can decompose $\Pi = U I_{6, 3} U^\top$ where the first three 
columns of $U$ consist of $e_1$, $e_5$, and $e_9$ corresponding to the fixed 
positions $1$, $5$, and $9$, the next three columns consist of eigenvectors 
$(e_k + e_l) / \sqrt{2}$, and the last three columns consist of eigenvectors 
$(e_k - e_l) / \sqrt{2}$, where pairs $(k, l)$ correspond to the cycles of 
order 2 described above.

The latent positions are the rows of  
$$XU = \begin{bmatrix}
  \lambda^{(11)} & 0 & 0 & 
  \lambda^{(12)} / \sqrt{2} & \lambda^{(13)} / \sqrt{2} & 0 & 
  \lambda^{(12)} / \sqrt{2} & \lambda^{(13)} / \sqrt{2} & 0 \\
  0 & \lambda^{(22)} & 0 & 
  \lambda^{(21)} / \sqrt{2} & 0 & \lambda^{(23)} / \sqrt{2} & 
  -\lambda^{(21)} / \sqrt{2} & 0 & \lambda^{(23)} / \sqrt{2} \\
  0 & 0 & \lambda^{(33)} & 
  0 & \lambda^{(31)} / \sqrt{2} & \lambda^{(32)} / \sqrt{2} & 
  0 & -\lambda^{(31)} / \sqrt{2} & -\lambda^{(32)} / \sqrt{2}
\end{bmatrix}$$
\end{example}

\hypertarget{methods}{%
\section{Methods}\label{methods}}

Two inference objectives arise from the PABM:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Community membership identification (up to permutation).
\item
  Parameter estimation (estimating \(\lambda^{(kl)}\)'s).
\end{enumerate}

In our methods, we assume that \(K\), the number of communities, is
known beforehand and does not require estimation.

\hypertarget{related-work}{%
\subsection{Related work}\label{related-work}}

\citeauthor{307cbeb9b1be48299388437423d94bf1}, who first proposed the
PABM, used Modularity Maximization (MM) and the Extreme Points (EP)
algorithm \cite{le2016} for community detection and parameter
estimation. They were able to show that as the sample size increases,
the proportion of misclassified community labels (up to permutation)
goes to 0.

\citet*{noroozi2019estimation} used Sparse Subspace Clustering (SSC) for
community detection in the PABM. SSC is performed by solving an
optimization problem for each observed point. Given
\(X \in \mathbb{R}^{n \times d}\) with vectors
\(x_i^\top \in \mathbb{R}^d\) as rows of \(X\), the optimization problem
\(c_i = \arg\min_{c} ||c||_1\) subject to \(x_i = X c\) and
\(c^{(i)} = 0\) is solved for each \(i = 1, ..., n\). The solutions are
collected collected into matrix
\(C = \begin{bmatrix} c_1 & \cdots & c_n \end{bmatrix}^\top\) to
construct an affinity matrix \(B = |C| + |C^\top|\). If each \(x_i\) lie
perfectly on one of \(K\) subspaces, \(B\) describes an undirected graph
consisting of \(K\) disjoint subgraphs, i.e., \(B_{ij} = 0\) if
\(x_i, x_j\) are in different subspaces. If \(X\) instead represents
points near \(K\) subspaces with some noise, a final graph partitioning
step may be performed (e.g., edge thresholding or spectral clustering).

In practice, SSC is often performed by solving the LASSO problems

\begin{equation} \label{eq:ssc}
c_i = \arg\min_c \frac{1}{2} ||x_i - X_{-i} c||^2_2 + \lambda ||c||_1
\end{equation}

for some sparsity parameter \(\lambda > 0\). The \(c_i\) vectors are
then collected into \(C\) and \(B\) as before.

\begin{definition}[Subspace Detection Property] 
Let $X = \begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix}^\top$ be noisy 
points sampled from $K$ subspaces. Let $C$ and $B$ be constructed from the 
solutions of LASSO problems as described in (\ref{eq:ssc}). If each column of 
$C$ has nonzero norm and $B_{ij} = 0$ $\forall$ $x_i$ and $x_j$ sampled from 
different subspaces, then $X$ obeys the subspace detection property. 
\end{definition}

\begin{remark} 
In practice, a noisy sample $X$ often does not obey the subspace detection
property. In such cases, $B$ is treated as an affinity matrix for a graph which 
is then partitioned into $K$ subgraphs to obtain the clustering. On the other 
hand, if $X$ does obey the subspace detection property, $B$ describes a graph 
with at least $K$ disconnected subgraphs. Ideally, when the subspace detection
property holds, there are exactly $K$ subgraphs which map to each subspace, 
but it could be the case that some of the subspaces are represented by 
multiple disconnected subgraphs. The subspace detection property is contingent 
on choosing a sufficiently large sparsity parameter $\lambda$. 
\end{remark}

Theorem \ref{theorem2} suggests that SSC is appropriate for community
detection for the PABM. More precisely, Theorem \ref{theorem2} says that
each community consists of a \(K\)-dimensional subspace, and together
the subspaces lie in \(\mathbb{R}^{K^2}\). The natural approach then is
to perform SSC on the ASE of \(P\) or \(A\).
\citeauthor{noroozi2019estimation} instead applied SSC to \(P\) and
\(A\), foregoing embedding altogether.

Using results from \citet{soltanolkotabi2012}, it can be easily shown
that the subspace detection property holds for \(XU\), which is an ASE
of \(P\). More specifically, if points lie exactly on mutually
orthogonal subspaces, then the subspace detection property will hold
with probability 1, and this is exactly the case for the PABM (Theorem
\ref{theorem2}). Much of our work is then built on
\citeauthor{rubindelanchy2017statistical}, who describe the convergence
behavior of the ASE of \(A\) to the ASE of \(P\), and
\citet{jmlr-v28-wang13}, who show the necessary conditions for the
subspace detection property to hold in noisy cases where the points lie
near subspaces.

\hypertarget{community-detection}{%
\subsection{Community detection}\label{community-detection}}

\begin{algorithm}[t]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwData{Adjacency matrix $A$, number of communities $K$}
  \KwResult{Community assignments $1, ..., K$}
    Compute the eigenvectors of $A$ that correspond to the $K (K+1) / 2$ most 
    positive eigenvalues and $K (K-1) / 2$ most negative eigenvalues. Construct 
    $V$ using these eigenvectors as its columns.\;
    Compute $B = |n V V^\top|$, applying $|\cdot|$ entry-wise.\;
    Construct graph $G$ using $B$ as its similarity matrix.\;
    Partition $G$ into $K$ disconnected subgraphs  
    (e.g., using edge thresholding or spectral clustering).\;
    Map each partition to the community labels $1, ..., K$.\;
  \caption{Orthogonal Spectral Clustering.}
\end{algorithm}

We previously stated one possible set of latent positions that result in
the edge probability matrix of a PABM, \(P = (XU) I_{pq} (XU)^\top\). If
we have (or can estimate) \(XU\) directly, then both the community
detection and parameter identification problem are trivial since \(U\)
is orthonormal and fixed for each value of \(K\). However, direct
identification or estimation of \(XU\) is not possible
\cite{rubindelanchy2017statistical}.

If we decompose \(P = Z I_{pq} Z^\top\), then
\(\exists Q \in \mathbb{O}(p, q)\) such that \(XU = Z Q\). Even if we
start with the exact edge probability matrix, we cannot recover the
``original'' latent positions \(XU\). Note that unlike in the case of
the RDPG, \(Q\) is not necessarily an orthogonal matrix. If \(z_i\)'s
are the rows of \(XU\), then
\(||z_i - z_j||^2 \neq ||Q z_i - Q z_j||^2\), and
\(\langle z_i, z_j \rangle \neq \langle Q z_i, Q z_j \rangle\). This
prevents us from using the properties of \(XU\) directly. In particular,
if \(Q \in \mathbb{O}(n)\), then we could use the fact that
\(\langle z_i, z_j \rangle = \langle Q z_i, Q z_j \rangle = 0\) if
vertices \(i\) and \(j\) are in different communities.

The explicit form of \(XU\) represents points in \(\mathbb{R}^{K^2}\)
such that points within each community lie on \(K\)-dimensional
orthogonal subspaces. Multiplication by \(Q \in \mathbb{O}(p, q)\)
removes the orthogonality property but retains the property that each
community is represented by a \(K\)-dimensional subspace. Therefore, the
ASE of \(P\) results in subspaces that correspond to each community,
suggesting the use of SSC. Before exploring SSC, we will first consider
a different approach.

\begin{theorem}
\label{theorem3}
Let $P = V D V^\top$ be the spectral decomposition of the edge probability
matrix. Let $B = n V V^\top$. Then $B_{ij} = 0$ if vertices $i$ and $j$ are 
from different communities.
\end{theorem}

Theorem \ref{theorem3} provides perfect community detection given \(P\).
Letting \(|B|\) be the affinity matrix for graph \(G\), \(G\) is
partitioned into at least \(K\) disjoint subgraphs since each of the
\(K\) communities have no edges between them. Similar to the subspace
detection property, it could be the case that some of the communities
are represented by multiple disjoint subgraphs in \(G\), in which case
additional reconstruction is required to identify the communities
exactly.

Using \(A\) instead of \(P\) introduces error, which converges to \(0\)
almost surely:

\begin{theorem}
\label{theorem4} 
Let $\hat{B}_n$ with entries $\hat{B}_n^{(ij)}$ be the affinity matrix from OSC 
(Alg. 1). Then $\forall$ pairs $(i, j)$ belonging to different communities 
and sparsity factor satisfying $n \rho_n = \omega\{(\log n)^{4c}\}$, 

\begin{equation} \label{eq:thm4}
\max_{i, j} |n (\hat{v}_n^{(i)})^\top \hat{v}_n^{(j)}| = 
O_P \Big( \frac{(\log n)^c}{\sqrt{n \rho_n}} \Big)
\end{equation}

This provides the result that for $i, j$ in different communities, 
$\hat{B}_n^{(ij)} \stackrel{a.s.}{\to} 0$.
\end{theorem}

Theorems \ref{theorem2}, \ref{theorem3}, and \ref{theorem4} also provide
a very natural path toward using SSC for community detection for the
PABM. We established in Theorem \ref{theorem2} that an ASE of the edge
probability matrix \(P\) can be constructed such that the communities
lie on mutually orthogonal subspaces, and this property can be recovered
from the eigenvectors of \(P\). Then Theorems \ref{theorem3} and
\ref{theorem4} show that this property holds for the unscaled ASE of
\(A\) drawn from \(P\) as \(n \to \infty\).

\begin{algorithm}[t]
  \DontPrintSemicolon
  \SetAlgoLined
  \caption{Sparse Subspace Clustering using LASSO \cite{jmlr-v28-wang13}.}
  \KwData{Adjacency matrix $A$, number of communities $K$, 
  hyperparameter $\lambda$}
  \KwResult{Community assignments $1, ..., K$}
    Find $V$, the matrix of eigenvectors of $A$ 
    corresponding to the $K (K + 1) / 2$ most positive 
    and the $K (K - 1) / 2$ most negative eigenvalues.\;
    Normalize $V \leftarrow \sqrt{n} V$.\;
    \For {$i = 1, ..., n$} {
      Assign $v_i^\top$ as the $i^{th}$ row of $V$. 
      Assign $V_{-i} = \begin{bmatrix} 
      v_1 & \cdots & v_{i-1} & v_{i+1} & \cdots & v_n \end{bmatrix}^\top$.\;
      Solve the LASSO problem 
      $c_i = \arg\min_{\beta} 
      \frac{1}{2} ||v_i - V_{-i} \beta||_2^2 + \lambda ||\beta||_1$.\;
      Assign $\tilde{c}_i = \begin{bmatrix} 
      c_i^{(1)} & \cdots & c_i^{(i-1)} & 0 & c_i^{(i)} & \cdots & c_i^{(n-1)}
      \end{bmatrix}^\top$ such that the superscript is the index of 
      $\tilde{c}_i$.\;
    }
    Assign 
    $C = \begin{bmatrix} \tilde{c}_1 & \cdots & \tilde{c}_n \end{bmatrix}$.\;
    Compute the affinity matrix $B = |C| + |C^\top|$.\;
    Construct graph $G$ using $B$ as its similarity matrix.\;
    Partition $G$ into $K$ disconnected subgraphs (e.g., using edge 
    thresholding or spectral clustering).\;
    Map each partition to the community labels $1, ..., K$. 
\end{algorithm}

\begin{theorem}
\label{theorem5}
Let $P_n$ describe the edge probability matrix of the PABM with 
$n$ vertices, and let $A_n \sim \text{Bernoulli}(P_n)$.  Let $\hat{V}_n$ be the 
matrix of eigenvectors of $A_n$ corresponding to the $K (K + 1) / 2$ most
positive and $K (K - 1) / 2$ most negative eigenvalues. Then 
$\exists \lambda > 0$ and $N \in \mathbb{N}$ such that when $n > N$, 
$\sqrt{n} \hat{V}_n$ obeys the subspace detection property with probability 1.
\end{theorem}

\begin{remark}
The proof of Theorem \ref{theorem5} is a direct consequence of Theorem 6 
from  \citeauthor{jmlr-v28-wang13} and the fact that the unscaled ASE of 
$P_n$ consists of orthogonal subspaces. \citeauthor{jmlr-v28-wang13} assume 
that the points in the embedding are all of unit length, and while we apply 
this normalization in the simulated examples, it is not strictly necessary 
for Theorem \ref{theorem5} due to orthogonality. 
\end{remark}

\hypertarget{parameter-estimation}{%
\subsection{Parameter estimation}\label{parameter-estimation}}

For any edge probability matrix \(P\) for the PABM such that the rows
and columns are organized by community, the \(kl\)\textsuperscript{th}
block is an outer product of two vectors, i.e.,
\(P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top\). Therefore, given
\(P^{(kl)}\), \(\lambda^{(kl)}\) and \(\lambda^{(lk)}\) are solvable
exactly (up to multiplication by \(-1\)) using singular value
decomposition. More specifically, let \(P^{(kl)} = \sigma^2 u v^\top\)
be the singular value decomposition of \(P^{(kl)}\).
\(u \in \mathbb{R}^{n_k}\) and \(v \in \mathbb{R}^{n_l}\) are vectors
and \(\sigma^2 > 0\) is a scalar. Then \(\lambda^{(kl)} = \pm \sigma u\)
and \(\lambda^{(lk)} = \pm \sigma v\). Given the adjacency matrix \(A\)
instead of edge probability matrix \(P\), we can simply use plug-in
estimators (algorithm 3), which converge to the true parameters.

\begin{algorithm}[t]
  \DontPrintSemicolon
  \SetAlgoLined
  \caption{PABM parameter estimation.}
  \KwData{Adjacency matrix $A$, community assignments $1, ..., K$}
  \KwResult{PABM parameter estimates $\{\hat{\lambda}^{(kl)}\}_K$.}
  Arrange the rows and columns of $A$ by community such that each 
  $A^{(kl)}$ block consists of estimated edge probabilities between 
  communities $k$ and $l$.\;
  \For {$k, l = 1, ..., K$, $k \leq l$} {
    Compute $A^{(kl)} = U \Sigma V^\top$, the SVD of the $kl^{th}$ 
    block.\;
    Assign $u^{(kl)}$ and $v^{(kl)}$ as the first columns of $U$ and $V$. 
    Assign $(\sigma^{(kl)})^2 \leftarrow \Sigma_{11}$.\;
    Assign $\hat{\lambda}^{(kl)} \leftarrow \pm \sigma^{(kl)} u^{(kl)}$ and 
    $\hat{\lambda}^{(lk)} \leftarrow \pm \sigma^{(kl)} v^{(kl)}$.
  }
\end{algorithm}

\begin{theorem}
\label{theorem6}
Under regularity and sparsity assumptions, given fixed $K$, 

\begin{equation} \label{eq:thm6}
\max_{k, l \in \{1, ..., K\}} 
||\hat{\lambda}^{(kl)} - \lambda^{(kl)}|| = 
O_P \bigg(\frac{(\log n_k)^c}{\sqrt{n_k}} \bigg)
\end{equation}
\end{theorem}

\hypertarget{simulated-examples}{%
\section{Simulated Examples}\label{simulated-examples}}

For each simulation, community labels are drawn from a multinomial
distribution, the popularity vectors \(\{\lambda^{(kl)}\}_K\) are drawn
from two types of joint distributions depending on whether \(k = l\),
the edge probability matrix \(P\) is constructed using the popularity
vectors, and finally an unweighted and undirected adjacency matrix \(A\)
is drawn from \(P\). OSC is then used for community detection, and this
method is compared against SSC \cite{noroozi2019estimation}
\cite{soltanolkotabi2014} and MM \cite{igraph}
\cite{307cbeb9b1be48299388437423d94bf1}. True community labels are used
with Algorithm 3 to estimate the popularity vectors
\(\{\lambda^{(kl)}\}_K\), and this method is then compared against an
MLE-based estimator described in \citeauthor{noroozi2019estimation} and
\citeauthor{307cbeb9b1be48299388437423d94bf1}.

Modularity Maximization is NP-hard, so
\citeauthor{307cbeb9b1be48299388437423d94bf1} used the Extreme Points
(EP) algorithm \cite{le2016}, which is \(O(n^{K - 1})\), as a greedy
relaxation of the optimization problem. For these simulations, the
Louvain algorithm was used, as the EP algorithm proved to be
prohibitively computationally expensive for \(K > 2\). For \(K = 2\), it
was verified that EP and Louvain produce comparable results.

Two implementations of SSC are shown here. The first method, denoted as
SSC-A, treats the columns of the adjacency matrix \(A\) as points in
\(\mathbb{R}^n\), as described in \citeauthor{noroozi2019estimation}.
The second method, denoted as SSC-ASE, first embeds \(A\) and then
performs SSC on the embedding, as described in algorithm 2. The sparsity
parameter \(\lambda\) was chosen via a preliminary cross-validation
experiment. For the final clustering step, a Gaussian Mixture Model was
fit on the normalized Laplacian eigenmap of the affinity matrix \(B\).

For comparing methods, we define the community detection error as:

\[L_c(\hat{\sigma}, \sigma; \{v_i\}) = 
\min_\pi \sum_i I(\pi \circ \hat{\sigma}(v_i) = \sigma(v_i))\]

where \(\sigma(v_i)\) is the true community label of vertex \(v_i\),
\(\hat{\sigma}(v_i)\) is the predicted label of \(v_i\), and \(\pi\) is
a permutation operator. This is effectively the ``misclustering count''
of clustering function \(\hat{\sigma}\).

We also define parameter estimation error as the RMSE up to sign flip:

\[RMSE(\{\hat{\lambda}^{(kl)}\}, \{\lambda^{(kl)}\}) = 
\sqrt{\frac{1}{n} \sum_{k < l} \min_{s \in \{-1, 1\}} 
\sum_{i = 1}^{n_k} 
(s \hat{\lambda}^{(kl)}_i - \lambda^{(kl)}_i)^2}\]

\hypertarget{balanced-communities}{%
\subsection{Balanced communities}\label{balanced-communities}}

In each simulation, community labels \(z_1, ..., z_n\) were drawn from a
multinomial distribution with mixture parameters
\(\{\alpha_1, ..., \alpha_K\}\), then \(\{\lambda^{(kl)}\}_K\) according
to the drawn community labels, \(P\) was constructed using the drawn
\(\{\lambda^{(kl)}\}_K\), and \(A\) was drawn from \(P\) by
\(A_{ij} \stackrel{indep}{\sim} Bernoulli(P_{ij})\). Each simulation has
a unique edge probability matrix \(P\).

For these examples, we set the following parameters:

\begin{itemize}
\tightlist
\item
  Number of vertices \(n = 128, 256, 512, 1024, 2048, 4096\)
\item
  Number of underlying communities \(K = 2, 3, 4\)
\item
  Mixture parameters \(\alpha_k = 1 / K\) for \(k = 1, ..., K\), (i.e.,
  each community label has an equal probability of being drawn)
\item
  Community labels
  \(z_k \stackrel{\text{iid}}{\sim}Multinomial(\alpha_1, ..., \alpha_K)\)
\item
  Within-group popularities
  \(\lambda^{(kk)} \stackrel{\text{iid}}{\sim}Beta(2, 1)\)
\item
  Between-group popularities
  \(\lambda^{(kl)} \stackrel{\text{iid}}{\sim}Beta(1, 2)\) for
  \(k \neq l\)
\end{itemize}

\(50\) simulations were performed for each \((n, K)\) pair.

\begin{comment}

Fig. \ref{fig:clust_err_k} show that for large $n$, OSC results in 
a misclustering rate of 0. 

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/clust_err_k-1} 

}

\caption{IQR of community detection error error using OSC (blue) compared against SSC on the ASE of A (purple), MM (red), and SSC on the adjacency matrix (green). Communities are approximately balanced. Simulations were repeated 50 times for each sample size.}\label{fig:clust_err_k}
\end{figure}

Theorem \ref{theorem4} implies that OSC will result in not just in the error
rate converging to 0 but the error *count* as well.
We explore this in Fig. \ref{fig:clust_err_ct_sim}.

\end{comment}

Fig \ref{fig:clust_err_ct_sim} shows OSC's community detection error
going to 0 for large \(n\). SSC on both the embedding and on the
adjacency matrix produces similar results for \(K > 2\). Weaker
performance of SSC for \(K = 2\) can be attributed to the final spectral
clustering step of the affinity matrix. A GMM was fit to the Laplacian
eigenmap, but visual inspection suggests that the communities are not
distributed as a mixture of Gaussians in the eigenmap. While the
subspace detection property is guaranteed for large \(n\), in our
simulations, setting a large enough sparsity parameter for SSC resulted
in more than \(K\) disconnected subgraphs.

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/clust_err_ct_sim-1} 

}

\caption{Median and IQR of community detection error. Communities are approximately balanced. Simulations were repeated 50 times for each sample size.}\label{fig:clust_err_ct_sim}
\end{figure}

Given ground truth community labels, Algorithm 3 and the MLE-based
plug-in estimators \cite{307cbeb9b1be48299388437423d94bf1}
\cite{noroozi2019estimation} perform similarly, with root mean square
error decaying at rate approximately \(n^{-1/2}\).

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/lambda_est_k-1} 

}

\caption{Median and IQR RMSE from Algorithm 3 (red) compared against an MLE-based method (blue). Simulations were repeated 50 times for each sample size. Communities were drawn to be approximately balanced.}\label{fig:lambda_est_k}
\end{figure}

\hypertarget{imbalanced-communities}{%
\subsection{Imbalanced communities}\label{imbalanced-communities}}

Simulations performed in this section are similar to those in the
previous section with the exception of the mixture parameters
\(\{\alpha_1, ..., \alpha_K\}\) used to draw community labels from the
multinomial distribution. For these examples, we set the following
parameters:

\begin{itemize}
\tightlist
\item
  Number of vertices \(n = 128, 256, 512, 1024, 2048, 4096\)
\item
  Number of underlying communities \(K = 2, 3, 4\)
\item
  Mixture parameters \(\alpha_k = \frac{k^{-1}}{\sum_{l=1}^K l^{-1}}\)
  for \(k = 1, ..., K\)
\item
  Community labels
  \(z_k \stackrel{\text{iid}}{\sim}Multinomial(\alpha_1, ..., \alpha_K)\)
\item
  Within-group popularities
  \(\lambda^{(kk)} \stackrel{\text{iid}}{\sim}Beta(2, 1)\)
\item
  Between-group popularities
  \(\lambda^{(kl)} \stackrel{\text{iid}}{\sim}Beta(1, 2)\) for
  \(k \neq l\)
\end{itemize}

\(50\) simulations were performed for each \((n, K)\) pair.

\begin{comment}

Fig. \ref{fig:clust_err_k_imba} and \ref{fig:clust_err_ct_sim_imba} show 
similar results as in the balanced communities case, with both OSC and 
SSC resulting in no misclustered vertices for a 
sufficiently large sample size. However, Fig. \ref{fig:lambda_est_k_imba} 
suggests that while Algorithm 3 retains $\sqrt{n}$ efficiency, the MLE-based 
plug-in estimator is less efficient for this setup. 

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/clust_err_k_imba-1} 

}

\caption{Median and IQR of community detection error using OSC (blue) compared against SSC on the ASE of A (purple), MM (red), and SSC on the adjacency matrix (green). Communities are imbalanced. Simulations were repeated 50 times for each sample size.}\label{fig:clust_err_k_imba}
\end{figure}

\end{comment}

We again see community detection error trending to 0 for OSC, as well as
for SSC when \(K > 2\) (Fig. \ref{fig:clust_err_ct_sim_imba}). Alg. 3
continues to see \(n^{-1/2}\) decay in parameter estimation error
(\ref{fig:lambda_est_k_imba}).

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/clust_err_ct_sim_imba-1} 

}

\caption{Median and IQR of community detection error. Communities are imbalanced. Simulations were repeated 50 times for each sample size.}\label{fig:clust_err_ct_sim_imba}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/lambda_est_k_imba-1} 

}

\caption{Median and IQR RMSE from Algorithm 3 (red) compared against an MLE-based method (blue). Simulations were repeated 50 times for each sample size. Communities were drawn to be imbalanced.}\label{fig:lambda_est_k_imba}
\end{figure}

\begin{comment}

## Additional experiments

Using the same set of parameters for generating $P$ and $A$ as in the 
balanced communities examples for $K = 2$, we generated one instance of $A$ for
each $n$ and constructed $B$ according to Algorithm 3 to verify that 
as $n \to \infty$, $(\hat{v}_i)^\top \hat{v_j} \stackrel{a.s.}{\to} 0$ for $i, j$ 
in different clusters. Furthermore, the distribution of these inner products 
should be approximately normal. 

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/unnamed-chunk-2-1} 

}

\caption{Between-cluster inner products of the eigenvectors of $A$ for varying sample sizes.}\label{fig:unnamed-chunk-2}
\end{figure}

\end{comment}

\hypertarget{real-data-examples}{%
\section{Real data examples}\label{real-data-examples}}

In the first real data example, we applied OSC to the Leeds Butterfly
dataset \cite{Wang_2018} consisting of visual similarity measurements
among 832 butterflies across 10 species. The graph was modified to match
the example from \citeauthor{noroozi2019estimation}: Only the 4 most
frequent species were considered, and the similarities were discretized
to \(\{0, 1\}\) via thresholding. Fig. \ref{fig:butterfly} shows a
sorted adjacency matrix sorted by the resultant clustering.

Comparing against the ground truth species labels, OSC achieves an
accuracy of 63\% and an adjusted Rand index of 73\%. In comparison,
\citeauthor{noroozi2019estimation} achieved an adjusted Rand index of
73\% using sparse subspace clustering on the same dataset.

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/butterfly-1} 

}

\caption{Adjacency matrix of the Leeds Butterfly dataset after sorting by the clustering outputted by OSC.}\label{fig:butterfly}
\end{figure}

In the second example, we applied OSC to the British MPs Twitter network
\cite{greene2013producing}, the Political Blogs network
\cite{10.1145/1134271.1134277}, and the DBLP network
\cite{NIPS2009_3855} \cite{10.1007/978-3-642-15880-3_42}. For this data
analysis, we subsetted the data as described by
\citeauthor{307cbeb9b1be48299388437423d94bf1} for their analysis of the
same networks. Our methods underperformed compared to modularity
maximization, although performance is comparable. In addition, OSC's
runtime is much lower than that of modularity maximization.

\begin{table}

\caption{\label{tab:unnamed-chunk-6}Community detection error rates for modularity maximization, sparse subspace clustering,  and OSC.}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
Network & MM & SSC-ASE & OSC\\
\hline
British MPs & 0.003 & 0.018 & 0.009\\
\hline
Political blogs & 0.050 & 0.196 & 0.062\\
\hline
DBLP & 0.028 & 0.087 & 0.059\\
\hline
\end{tabular}
\end{table}

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/mp-1} 

}

\caption{Adjacency matrices of (from left to right) the British MPs, Political Blogs, and DBLP networks after sorting by the clustering outputted by OSC.}\label{fig:mp}
\end{figure}

In the third example, we consider the Karantaka villages data studied by
\citet{DVN/U3BIHX_2013}. For this example, we chose the \texttt{visitgo}
networks from villages 12, 31, and 46 at the household level. The label
of interest is the religious affiliation. The networks were truncated to
religions ``1'' and ``2'', and vertices of degree 0 were removed.

\begin{figure}[H]

{\centering \includegraphics{summary_files/figure-latex/unnamed-chunk-7-1} 

}

\caption{Adjacency matrix of the Karnataka villages data, arranged by the clustering produced by OSC (left). The villages studied here are, from left to right, 12, 31, and 46.}\label{fig:unnamed-chunk-7}
\end{figure}

\begin{table}

\caption{\label{tab:unnamed-chunk-8}Community detection error rates for identifying household religion.}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
Network & MM & SSC-ASE & OSC\\
\hline
Village 12 & 0.270 & 0.291 & 0.227\\
\hline
Village 31 & 0.125 & 0.066 & 0.110\\
\hline
Village 46 & 0.052 & 0.463 & 0.078\\
\hline
\end{tabular}
\end{table}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

This paper shows the connection between the PABM and the GRDPG, namely
that a PABM graph can be represented as a union of orthogonal subspaces
in an embedding under the GRDPG framework. We then exploited this
relationship to develop community detection and parameter estimation
methods. In fact, we can represent any graph with Bernoulli edges as a
GRDPG, and in the PABM case, it turns out that this relationship leads
to a very straightforward applications of previous work from
\citeauthor{rubindelanchy2017statistical},
\citeauthor{soltanolkotabi2012}, and \citeauthor{jmlr-v28-wang13}, which
lead to asymptotically correct solutions with high probability. Similar
methods can be applied for other models, such as the Nested Block Model
\cite{noroozi2021hierarchy}.

\hypertarget{proofs}{%
\section{Proofs}\label{proofs}}

\textbf{Proof of Theorem \ref{theorem1}}. This is given by
straightforward matrix multiplication. It suffices to show that

\[X U I_{3, 1} U^\top X^\top = 
\begin{bmatrix} 
  \lambda^{(11)} (\lambda^{(11)})^\top & \lambda^{(12)} (\lambda^{(21)})^\top \\
  \lambda^{(21)} (\lambda^{(12)})^\top & \lambda^{(22)} (\lambda^{(22)})^\top
\end{bmatrix}\]

\begin{remark} 
While we can just perform the matrix multiplication to show the 
equivalence, it is more illustrative to look at a few intermediate steps. Note 
that the product of the three inner matrices results in a permutation matrix 
with fixed points at positions $1$ and $4$ and a cycle of order 2 swapping 
positions $2$ and $3$: 

$$U I_{3, 1} U^\top = \begin{bmatrix} 
  1 & 0 & 0 & 0 \\ 
  0 & 0 & 1 & 0 \\ 
  0 & 1 & 0 & 0 \\ 
  0 & 0 & 0 & 1 
\end{bmatrix} = \Pi$$

Since $U$ is orthonormal and $I_{3, 1}$ is diagonal, $\Pi = U I_{3, 1} U^\top$ 
is a spectral decomposition of this permutation matrix. Note that the two fixed 
points result in eigenvalues of $+1$ with corresponding eigenvectors $e_i$ 
where $i = 1, 4$ corresponding to the locations of the fixed points, and the 
cycle of order two results in two eigenvalues $\pm 1$ with corresponding 
eigenvectors $(e_i \pm e_j) / \sqrt{2}$ where $i = 2, j = 3$, pair that is 
swapped.
\end{remark}

\begin{lemma}
\label{lemma1}
Let $P = V D V^\top$ be the spectral decomposition of the edge probability 
matrix for a PABM. Then $V V^\top = X (X^\top X)^{-1} X^\top$ where $X$ is 
defined as in (\ref{eq:xy}).
\end{lemma}

\emph{Proof}. By Theorem 2, \(P = X U I_{p, q} U^\top X^\top\), where
\(X\) is defined as in (\ref{eq:xy}) and \(p\) and \(q\) are defined as
in equations (\ref{eq:p}) and (\ref{eq:q}). Alternatively, the spectral
decomposition can be written as
\(P = V D V^\top = V |D|^{1/2} I_{p, q} |D|^{1/2} V^\top\) for the same
\((p, q)\) and \(|\cdot|^{1/2}\) is applied entry-wise. Thus for some
\(Q \in \mathbb{O}(p, q)\),

\[X U Q = V |D|^{1/2}\]

Therefore, using the fact that \(U U^\top = I\) and \(V^\top V = I\),

\[(V |D|^{1/2}) ((V |D|^{1/2})^\top (V |D|^{1/2}))^{-1} (V |D|^{1/2})^\top = 
(X U Q) ((X U Q)^\top (X U Q))^{-1} (X U Q)^\top\]

The right-hand side becomes

\[\begin{split}
  (X U Q) ((X U Q)^\top (X U Q))^{-1} (X U Q)^\top & 
    = X U Q Q^{-1} U^\top (X^\top X)^{-1} 
    U (Q^\top)^{-1} Q^\top U^\top X^\top \\
  & = X U U^\top (X^\top X)^{-1} U U^\top X^\top \\
  & = X (X^\top X)^{-1} X^\top
\end{split}\]

The left-hand side becomes:

\[\begin{split}
  (V |D|^{1/2}) ((V |D|^{1/2})^\top (V |D|^{1/2}))^{-1} (V |D|^{1/2})^\top & =
    V |D|^{1/2} |D|^{-1/2} (V^\top V)^{-1} |D|^{-1/2} |D|^{1/2} V^\top \\
  & = V V^\top
\end{split}\]

\textbf{Proof of Theorem \ref{theorem3}}. By Lemma \ref{lemma1},
\(V V^\top = X (X^\top X)^{-1} X^\top\) where \(X\) is defined as in
(\ref{eq:xy}). Since \(X\) is block diagonal with each block
corresponding to one community, \(X (X^\top X)^{-1} X^\top\) is also a
block diagonal matrix with each block corresponding to a community and
zeros elsewhere. Therefore, if vertices \(i\) and \(j\) belong to
different communities, then the \(ij\)\textsuperscript{th} element of
\(n X (X^\top X)^{-1} X^\top = n V V^\top = B\) is 0.

\textbf{Proof of Theorem \ref{theorem4}}. Let \(V_n\) and \(\hat{V}_n\)
be the eigenvectors of \(P\) and \(A\) corresponding to the
\(K (K + 1) / 2\) most positive and \(K (K - 1) / 2\) most negative
eigenvalues. By \citeauthor{rubindelanchy2017statistical}, for some
\(W \in \mathbb{O}(K^2)\), and \(c > 0\),\\
\(||\hat{V} W - V||_{2 \to \infty} = O_P \big(\frac{(\log n)^c}{n \sqrt{\rho_n}} \big)\).
We furthermore have \(||V||_{2 \to \infty} = O_P(n^{-1/2})\). Then if
\((v_n^{(i)})^\top\) and \((\hat{v}_n^{(i)})^\top\) correspond to the
rows of \(V_n\) and \(\hat{V}_n\), for \(i\) and \(j\) in different
communities, using the fact that \((v_n^{(i)})^\top v_n^{(j)} = 0\),

\[\begin{split}
\max_{i, j} |(\hat{v}_n^{(i)})^\top \hat{v}_n^{(j)}| &
= \max_{i, j} |(\hat{v}_n^{(i)})^\top \hat{v}_n^{(j)} - 
(v_n^{(i)})^\top v_n^{(j)}| \\
& = \max_{i, j} |(\hat{v}_n^{(i)})^\top W W^\top \hat{v}_n^{(j)} -
(v_n^{(i)})^\top v_n^{(j)}| \\
& = ||\hat{V}_n W^\top W \hat{V}_n - V_n V_n^\top||_{2 \to \infty} \\
& = ||2 \hat{V}_n W V_n^\top - 2 V_n V_n^\top + \hat{V}_n W W^\top V_n^\top - 
2 \hat{V}_n W V_n^\top + V_n V_n^\top||_{2 \to \infty} \\
& = ||2 (\hat{V}_n W - V_n) V_n^\top + 
(\hat{V}_n W - V_n) (\hat{V}_n W - V_n)^\top ||_{2 \to \infty} \\
& \leq 2 ||\hat{V}_n W - V_n||_{2 \to \infty} ||V_n||_{2 \to \infty} +
||\hat{V}_n W - V_n||_{2 \to \infty}^2 \\
& = O_P \Big( \frac{(\log n)^c}{n^{3/2} \rho_n^{1/2}} \Big)
\end{split}\]

Then scaling by \(n\), we get
\(|n (\hat{v}_n^{(i)})^\top \hat{v}_n^{(j)}| = O_P \big( \frac{(\log n)^c}{\sqrt{n \rho_n}} \big)\).

\begin{definition}[Inradius \cite{soltanolkotabi2012, jmlr-v28-wang13}]
The inradius of a convex body $\mathcal{P}$, denoted by $r(\mathcal{P})$, is 
defined as the radius of the largest Euclidean ball inscribed in $\mathcal{P}$.
In addition, $r(X)$ for data matrix $X$ with rows $x_i^\top$ represents 
the inradius of the symmetric convex hull of $X$. 
\end{definition}

\begin{definition}[Subspace incoherence property \cite{jmlr-v28-wang13}]
\end{definition}

\begin{lemma}
\label{lemma2}
Let $\hat{V}$ be the eigenvectors of $A_n$ corresponding to the $K (K + 1) / 2$ most positive and $K (K - 1) / 2$ most negative eigenvalues such that the rows of $\hat{V}$ are ordered by community, and let $\hat{V}^{(k)}$ be the rows of the $k^{th}$ community in $\hat{V}$ and $\hat{V}^{(-k)}$ be the rows of $\hat{V}$ with the $k^{th}$ community omitted. Denote $(\hat{v}_i^{(k)})^\top$ as the rows of $\hat{V}$, $\hat{V}_{-i}^{(k)}$ as $\hat{V}^{(k)}$ with the $i^{th}$ row omitted, and $\mathcal{S}^{(k)}$ as the subspace spanned by $V^{(k)}$. Let $V$, $V^{(k)}$, $V^{(-k)}$, and $v_i^{(k)}$ be the corresponding values for $P_n$. 

Let $\nu_{i}^{(k)} = \max\limits_\eta (\hat{v}_i^{(k)})^\top \eta - \frac{1}{2 \lambda} \eta^\top \eta$ subject to $||V_{-i}^{(k)} \eta||_\infty \leq 1$, and define the projected dual direction $w_{i}^{(k)}$ as $\mathbb{P}_{\mathcal{S}^{(k)}}(\nu_i^{(k)})$ normalized to length 1. Collect the projected dual directions into $W = \begin{bmatrix} w_1^{(k)} & \cdots & w_{n_k}^{(k)} \end{bmatrix}^\top$.

Define the subspace incoherence:

$$\mu_n^{(k)} = \mu(\hat{V}^{(k)}) = \max\limits_{v \in V^{(-k)}} ||W^{(k)} v||_\infty$$

Then $\forall k, n$, 

\begin{equation} \label{eq:mu-conv}
\mu_n^{(k)} = 0
\end{equation}
\end{lemma}

\emph{Proof}. Since by Theorem \ref{theorem2} each \(\mathcal{S}^{(k)}\)
are mutually orthogonal, any vector projected to \(\mathcal{S}^{(k)}\)
must be orthogonal to each row of \(V^{(-k)}\). Therefore,
\(W^{(k)} v = 0\) \(\forall v \in \mathcal{S}^{(-k)}\).

\begin{lemma}
\label{lemma3}
Let $(v_n^{(i)})^\top$ and $(\hat{v}_n^{(i)})^\top$ be the rows of $V_n$ and
$\hat{V}_n$ respectively. By \citeauthor{rubindelanchy2017statistical}, 

\begin{equation} \label{eq:delta-conv}
\delta_n = \max_i ||\hat{v}_n^{(i)} - v_n^{(i)}|| \stackrel{a.s.}{\to} 0
\end{equation}
\end{lemma}

\textbf{Proof of Theorem \ref{theorem5}}. The basis of this proof is
Theorem 6 from \citeauthor{jmlr-v28-wang13}, which states that the
subspace detection property holds if the noise is small enough and the
subspace inradius is greater than the subspace incoherence for each
community \(k\).

Let \(V_{n, -i}^{(k)}\) be \(V_n^{(k)}\) with the
\(i\)\textsuperscript{th} entry removed. Suppose that for each community
\(k\), there are enough vertices such that for each \(i\),
\(V_{n, -i}^{(k)}\) spans its corresponding subspace (Theorem
\ref{theorem2}). Then
\(r_n^{(k)} = \min\limits_i r(V_{n, -i}^{(k)}) > 0\). Thus by
(\ref{eq:mu-conv}), for each \(k\), \(r_n^{(k)} > \mu_n^{(k)} = 0\)
where \(\mu_n^{(k)} = \mu(\hat{V}_n^{(k)})\) and \(n\) is large enough
such that \(\min\limits_{k, i} \text{rank}(V_{n, -i}^{(k)}) = K\).

Let \(r_n = \min\limits_k r_n^{(k)}\). By (\ref{eq:delta-conv}),
\(\delta_n \stackrel{a.s.}{\to} 0\). Then as \(n\to \infty\),
\(\delta_n < \min\limits_k \frac{r_n (r_n^{(k)} - \mu_n^{(k)})}{2 + 7 r_n^{(k)}}\)
\(= \min\limits_k \frac{r_n r_n^{(k)}}{2 + 7 r_n^{(k)}}\) with
probability 1.

Thus the conditions for the subspace detection property from Theorem 6
from \citeauthor{jmlr-v28-wang13} are satisfied with probability 1 as
\(n \to \infty\).

\begin{remark}
Theorem 6 of \citeauthor{jmlr-v28-wang13} assume that each $||v_n^{(i)}|| = 1$, 
which scales each $r_n^{(k)} \leq 1$. This is not strictly necessary for 
the proof of Theorem \ref{theorem5} since each $\mu_n^{(k)} = 0$, so as long 
as the $k^{th}$ community spans its subspace, $a r_n^{(k)} > 0 = \mu_n^{(k)}$ 
$\forall a > 0$.
\end{remark}

\textbf{Proof of Theorem \ref{theorem6}}. Let \(P\) and \(A\) be
organized by community such that the elements of blocks \(P^{(kl)}\) and
\(A^{(kl)}\) correspond to the edges between communities \(k\) and
\(l\).

\emph{Case \(k = l\)}. \(P^{(kk)}\) and \(A^{(kk)}\) represent
within-community edge probabilities and edges for community \(k\).\\
By definition, \(P^{(kk)} = \lambda^{(kk)} (\lambda^{(kk)})^\top\). This
implies that the singular value decomposition
\(P^{(kk)} = \sigma_{kk}^2 u^{(kk)} (u^{(kk)})^\top\) has one singular
value and one pair of singular vectors (\(P^{(kk)}\) is symmetric, so
the left and right singular vectors are identical). Then
\(\lambda^{(kk)} = \sigma_{kk} u^{(kk)}\).\\
Let \(\hat{U}^{(kk)} \hat{\Sigma}^{(kk)} (\hat{U}^{(kk)})^\top\) be the
singular value decomposition of \(A^{(kk)}\), and let
\(\hat{\sigma}_{kk}^2 \hat{u}^{(kk)} (\hat{u}^{(kk)})^\top\) be its
one-dimensional approximation. Define
\(\hat{\lambda}^{(kk)} = \hat{\sigma}_{kk} \hat{u}^{(kk)}\). Then
\(\hat{\lambda}^{(kk)}\) is the adjacency spectral embedding
approximation of \(\lambda^{(kk)}\).\\
Then by Theorem 5 of \citeauthor{rubindelanchy2017statistical}, the
adjacency spectral embedding \(\hat{\lambda}^{(kk)}\) approximates
\(\lambda^{(kk)}\) at rate \(\frac{(\log n_k)^c}{\sqrt{n_k}}\).

\emph{Case \(k \neq l\)}. \(P^{(kl)}\) and \(A^{(kl)}\) represent edge
probabilities and edges between communities \(k\) and \(l\). Note that
\(P^{(kl)} = (P^{(lk)})^\top\).\\
By definition, \(P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top\). As
in the \(k = l\) case, we note that the singular value decomposition
\(P^{(kl)} = \sigma_{kl}^2 u^{(kl)} (v^{(kl)})^\top\) is one-dimensional
and \(\lambda^{(kl)} = \sigma_{kl} u^{(kl)}\). (We can also note that
the SVD of \(P^{(lk)} = \sigma_{kl}^2 v^{(kl)} (u^{(kl)})^\top\), i.e.,
\(\sigma_{kl} = \sigma_{lk}\), \(u^{(kl)} = v^{(lk)}\), and
\(v^{(kl)} = u^{(lk)}\).)\\
Now consider the Hermitian dilation

\[M^{(kl)} = 2 \begin{bmatrix} 0 & P^{(kl)} \\ P^{(lk)} & 0 \end{bmatrix}\]

which is a symmetric \((n_k + n_l) \times (n_k + n_l)\) matrix. It can
be shown that the spectral decomposition of \(M^{(kl)}\) is

\[M^{(kl)} = 
\begin{bmatrix} u^{(kl)} & -u^{(kl)} \\ v^{(kl)} & v^{(kl)} \end{bmatrix} \times 
\begin{bmatrix} \sigma^2_{kl} & 0 \\ 0 & -\sigma^2_{kl} \end{bmatrix} \times
\begin{bmatrix} u^{(kl)} & -u^{(kl)} \\ v^{(kl)} & v^{(kl)} \end{bmatrix}^\top\]

Thus treating \(M^{(kl)}\) as the edge probability matrix of a GRDPG, we
have latent positions in \(\mathbb{R}^2\) given by

\[\begin{bmatrix} 
  \sigma_{kl} u^{(kl)} & \sigma_{kl} u^{(kl)} \\ 
  \sigma_{kl} v^{(kl)} & -\sigma_{kl} v^{(kl)} 
\end{bmatrix} = 
\begin{bmatrix} 
  \lambda^{(kl)} & \lambda^{(kl)} \\ 
  \lambda^{(lk)} & -\lambda^{(lk)} 
\end{bmatrix}\]

Now consider

\[\hat{M}^{(kl)} = \begin{bmatrix} 0 & A^{(kl)} \\ A^{(lk)} & 0 \end{bmatrix}\]
Then \(\hat{M}^{(kl)} = M^{(kl)} + E'\) where

\[E' = \begin{bmatrix} 0 & E \\ E^\top & 0 \end{bmatrix}\]

and \(E\) is the \(n_k \times n_l\) matrix of independent noise (to
generate the Bernoulli entries in \(A^{(kl)}\).\\
Then \(\hat{M}^{(kl)}\) is an adjacency matrix drawn from \(M^{(kl)}\),
so its adjacency spectral embedding, given by

\[\begin{bmatrix} 
  \hat{\lambda}^{(kl)} & \hat{\lambda}^{(kl)} \\ 
  \hat{\lambda}^{(lk)} & -\hat{\lambda}^{(lk)} 
\end{bmatrix}\]

where each \(\hat{\lambda}^{(kl)}\) is defined as in Algorithm 3,
approximates the latent positions of \(M^{(kl)}\) up to indefinite
orthogonal transformation by the rate given in Theorem 5 of
\citeauthor{rubindelanchy2017statistical}.\\
In this case, the indefinite orthogonal transformation \(W_*\) in the
GRDPG result \cite{rubindelanchy2017statistical} is of the form
\(U^\top \hat{U}\). The eigenvalues of \(M\) are distinct since the
signature for this GRDPG is \((1, 1)\), and \(U^\top \hat{U}\) is block
diagonal, resulting in \(W_* \stackrel{a.s.}{\to} I\). Therefore, the
adjacency spectral embedding of \(\hat{M}^{(kl)}\) is a direct
estimation of the specific latent positions outlined for \(M^{(kl)}\),
up to sign flip.

\renewcommand\refname{References}
  \bibliography{misc.bib}

\end{document}
