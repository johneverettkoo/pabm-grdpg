---
title: "Clustering and Parameter Estimation for the Popularity Adjusted Block Model"
output:
  pdf_document:
    citation_package: natbib
    number_sections: yes
# output: html_document
# geometry: "left=1cm,right=1cm,top=1cm,bottom=1.5cm"
urlcolor: blue
header-includes:
- \usepackage{float}
- \usepackage{mathtools}
- \usepackage{natbib}
- \usepackage[linesnumbered,vlined,ruled]{algorithm2e} 
- \setcitestyle{numbers,square}
- \usepackage{verbatim}
bibliography: misc.bib
abstract: |
  In this paper, we connect two probabilistic models for graphs, the Popularity 
  Adjusted Block Model (PABM) and the Generalized Random Dot Product Graph 
  (GRDPG) and use properties established in this connection to aid in 
  clustering and parameter estimation. In particular, we note that the PABM 
  can be represented as latent positions such that points within the same 
  cluster lie on a subspace, and the subspaces that represent each 
  cluster are orthogonal to one another. Using this property as well as the 
  asymptotic properties of Adjacency Spectral Embedding (ASE) of the GRDPG, we 
  are able to establish theoretical asymptotic results of our clustering and 
  parameter estimation methods for the PABM. 
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 3, 
                      fig.width = 5, 
                      fig.dpi = 300)

options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')
```

```{r}
import::from(magrittr, `%>%`)
library(plot.matrix)
library(ggplot2)
```

\newcommand{\diag}{\text{diag}}

# Introduction

One popular probabilistic model for graphs with underlying communities is the 
Stochastic Block Model (SBM), which assumes that the edge probability from 
one cluster to another follows a Beronulli distribution with a fixed 
probability for each pair of communities. 
The Popularity Adjusted Block Model (PABM) was introduced by 
\citet*{307cbeb9b1be48299388437423d94bf1} as a generalization of the SBM to 
address the heterogeneity of edge probabilities within and between communities 
or clusters while still maintaining the community structure. 

The Random Dot Product Graph (RDPG) \cite{athreya2017statistical} is another 
probabilistic model with Beronulli edge probabilities. Under this model, each 
vertex of the graph can be represented by a point in latent space such that 
the edge probability between any pair of vertices is given by their 
corresponding dot product in the latent space. The SBM is equivalent to a 
special case of the RDPG model in which all vertices in a given community share 
the same position in the latent space. It has also been shown that similar 
probabilistic graph models, (such as the Mixed Membership Stochastic Block Model
\cite{rubindelanchy2017statistical}). An analogous property exists for the 
PABM not under the RDPG model but under the *Generalized* Random Dot Product 
Graph model. 

## Previous work

\citet*{noroozi2019estimation} proposed using sparse subspace clustering (SSC) 
to identify the cluster memberships given either an edge probability matrix $P$ 
or an adjacency matrix $A$. In the case that $P$ is known, the cluster 
memberships can be identified exactly (up to permutation). A similar procedure 
can be applied if $P$ is unknown and we have an observation $A$, but the 
theoretical guarantees of this method applied to the PABM are unknown. In 
particular, the method requires spherical Gaussian noise. The authors of this 
paper then use point estimators for $\{\lambda^{(kl)}\}$ using cluster labels 
obtained via SSC. 

# Connecting the Popularity Adjusted Block Model to the Generalized Random Dot Product Graph

## The popularity adjusted block model (PABM) \cite{noroozi2019estimation}

**Definition 1**.
Let $G = (V, E)$ be an undirected, unweighted random graph with corresponding 
affinity matrix $A \in \{0, 1\}^{n \times n}$. Then $A$ is a random matrix 
with corresponding edge probability matrix $P$ such that 
$A_{ij} \stackrel{indep}{\sim} Bernoulli(P_{ij})$ for $i > j$ 
($A_{ji} = A_{ij}$ and $A_{ii} = 0$). Let there exist $K$ underlying 
communities in $G$, and let $n_{k}$ be the size of the $k$^th^ community in 
$G$ such that $\sum_{k=1}^K n_k = n$.

If $A$ and $P$ are organized such that $n_k \times n_l$ blocks $A^{(kl)}$ and 
$P^{(kl)}$ describe the edges and edge probabilities between communities $k$ 
and $l$, then $P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top$ for a set of 
fixed vectors $\{\lambda^{(st)}\}_{s, t = 1, ..., K}$. Each $\lambda^{(st)}$ 
for $s, t = 1, ..., K$ is a column vector of length $n_s$ (i.e., the community 
corresponding to the first index provides the vector length).

We will use the notation $A \sim PABM(\{\lambda^{(kl)}\}_K)$ to denote a random 
affinity matrix $A$ drawn from a PABM with parameters $\lambda^{(kl)}$ 
consisting of $K$ underlying clusters/communities.

## The generalized random dot product graph (GRDPG) \cite{rubindelanchy2017statistical}

**Definition 2**.
Let $X \in \mathbb{R}^{n \times d}$ be latent positions of the vertices of a 
graph $G$. $X$ consists of row vectors $x_i^\top$. Let 
$A \in \{0, 1\}^{n \times n}$ be the corresponding affinity matrix. 

Fix $p$, $q$ such that $p + q = d$ and define 
$I_{pq} = \begin{bmatrix} I_p & 0 \\ 0 & -I_q \end{bmatrix}$. 

Then $G = (V, E)$ is a generalized random dot product graph with signature 
$(p, q)$ and patent positions $X$ iff its random affinity matrix can be 
described as $A_{ij} \stackrel{indep}{\sim} Bernoulli(P_{ij})$ where 
$P_{ij} = x_i^\top I_{pq} x_j$.

We will use the notation $A \sim GRDPG_{p,q}(X)$ to denote a random affinity 
matrix $A$ drawn from latent positions $X$ and signature $(p, q)$.

**Remark**. 
Like the RDPG, the latent positions of a GRDPG are not unique. More 
specifically, if $P_{ij} = x_i^\top I_{pq} x_j$, then we also have for any 
$Q \in \{R : R^\top I_{pq} R = I_{pq} \}$, 
$(Q x_i)^\top I_{pq} (Q x_j) = x_i^\top (Q^\top I_{pq} Q) x_j = 
x_i^\top I_{pq} x_j = P_{ij}$ (this is the indefinite orthogonal group, 
$\mathbb{O}(p, q)$).
Unlike in the RDPG case, transforming the latent positions by multiplication 
with $Q \in \mathbb{O}(p, q)$ does not necessarily maintain interpoint angles or 
distances. 

## Connecting the PABM to the GRDPG

### Case where $K = 2$

**Theorem 1**. 
Let $X = \begin{bmatrix} 
\lambda^{(11)} & \lambda^{(12)} & 0 & 0 \\ 
0 & 0 & \lambda^{(21)} & \lambda^{(22)} 
\end{bmatrix}$, and let $U = \begin{bmatrix} 1 & 0 & 0 & 0 \\
0 & 0 & 1 / \sqrt{2} & 1 / \sqrt{2} \\
0 & 0 & 1 / \sqrt{2} & - 1 / \sqrt{2} \\
0 & 1 & 0 & 0 \end{bmatrix}$, as in Definition 1. 
Then $A \sim GRDPG_{3, 1}(X U)$ and $A \sim PABM(\{(\lambda^{(kl)}\}_K)$ are 
equivalent.

*Proof*.
Let $X = \begin{bmatrix} 
\lambda^{(11)} & \lambda^{(12)} & 0 & 0 \\ 
0 & 0 & \lambda^{(21)} & \lambda^{(22)} 
\end{bmatrix}$ and 
$Y = \begin{bmatrix} \lambda^{(11)} & 0 & \lambda^{(12)} & 0 \\
0 & \lambda^{(21)} & 0 & \lambda^{(22)} \end{bmatrix}$. 
Then $P = X Y^\top$. 

We can note that $Y = X \Pi$ where $\Pi$ is the permutation matrix 
$\Pi = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 
0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}$. 
Therefore, $P = X \Pi X^\top$. 

Taking the spectral decomposition of $\Pi = U D U^\top$, we can see that 
$P = (X U) D (X U)^\top$. We can then denote $\Sigma = |D|^{1/2}$, the 
square root of the absolute values of the (diagonal) entries of $D$ and 
obtain $P = (X U \Sigma) I_{pq} (X U \Sigma)^\top$ where $p$ and $q$ correspond 
to the number of positive and negative eigenvalues of $\Pi$, respectively. 
Therefore, the PABM with $K = 2$ is a special case of the GRDPG. We can however 
expand upon this a bit further.

The permutation described by $\Pi$ has two fixed points and one cycle of order 
2. The two fixed points are at positions $1$ and $4$, so $\Pi$ has two 
eigenvalues equal to $1$ and corresponding eigenvectors $e_1$ and $e_4$. The 
cycle of order 2 switching positions $2$ and $3$ corresponds to eigenvalues 
$1$ and $-1$ with corresponding eigenvalues $(e_2 + e_3) / \sqrt{2}$ and 
$(e_2 - e_3) / \sqrt{2}$ respectively. Therefore, 
$D = \begin{bmatrix} 
1 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & -1
\end{bmatrix} = I_{3, 1}$ and 
$U = \begin{bmatrix} 1 & 0 & 0 & 0 \\
0 & 0 & 1 / \sqrt{2} & 1 / \sqrt{2} \\
0 & 0 & 1 / \sqrt{2} & - 1 / \sqrt{2} \\
0 & 1 & 0 & 0 \end{bmatrix}$.

Putting it all together, we get $P = (X U) I_{3, 1} (X U)^\top$. Therefore, 
the PABM with $K = 2$ is a GRDPG with $p = 3$, $q = 1$, $d = K^2 = 4$, and 
latent positions 
$X U = \begin{bmatrix} 
\lambda^{(11)} & 0 & \lambda^{(12)} / \sqrt{2} & \lambda^{(12)} / \sqrt{2} \\
0 & \lambda^{(22)} & \lambda^{(21)} / \sqrt{2} & -\lambda^{(21)} / \sqrt{2}
\end{bmatrix}$.

### Generalization to $K > 2$

**Theorem 2**. There exists a block diagonal matrix 
$X \in \mathbb{R}^{n \times K^2}$ defined by PABM parameters 
$\{\lambda^{(kl)}\}_K$ and $U \in \mathbb{R}^{K^2 \times K^2}$ that is fixed 
for each $K$ such that $A \sim GRDPG_{K (K+1) / 2, K (K-1) / 2}(XU)$ and 
$A \sim PABM(\{(\lambda^{(kl)}\})_K)$ are equivalent.

*Proof*
Let $\Lambda^{(k)} = 
\begin{bmatrix} \lambda^{(k,1)} & \cdots & \lambda^{(k, K)} \end{bmatrix}
\in \mathbb{R}^{n_k \times K}$.  
Let $X$ be a block diagonal matrix 
$X = \diag(\Lambda^{(1)}, ..., \Lambda^{(K)}) \in \mathbb{R}^{n \times K^2}$.  
Let $L^{(k)}$ be a block diagonal matrix of column vectors $\lambda^{(lk)}$ for 
$l = 1, ..., K$. $L^{(k)} = \diag(\lambda^{(1k)}, ..., \lambda^{(Kk)}) \in 
\mathbb{R}^{n \times K}$.  
Let $Y = \begin{bmatrix} L^{(1)} & \cdots & L^{(K)} \end{bmatrix} \in 
\mathbb{R}^{n \times K^2}$.  
Then $P = X Y^\top$.  
Similar to the $K = 2$ case, we again have $Y = X \Pi$ for a permutation matrix
$\Pi$, so $P = X \Pi X^\top$.  
The permutation described by $\Pi$ has $K$ fixed points, which correspond to 
$K$ eigenvalues equal to $1$ with corresponding eigenvectors $e_k$ where 
$k = r (K + 1) + 1$ for $r = 0, ..., K - 1$. It also has 
$\binom{K}{2} = K (K - 1) / 2$ cycles of order $2$. Each cycle corresponds to 
a pair of eigenvalues $+1$ and $-1$ and a pair of eigenvectors 
$(e_s + e_t) / \sqrt{2}$ and $(e_s - e_t) / \sqrt{2}$.

So $\Pi$ has $K (K + 1) / 2$ eigenvalues equal to $1$ and $K (K - 1) / 2$ 
eigenvalues equal to $-1$. $\Pi$ has the decomposed form 
$\Pi = U I_{K (K + 1) / 2, K (K - 1) / 2} U^\top$, and we can describe the 
PABM with $K$ communities as a GRDPG with latent positions $X U$ with signature 
$\Big( K (K + 1) / 2, K (K - 1) / 2 \Big)$.

**Example** for $K = 3$. Using the same notation as before:

$X = \begin{bmatrix} 
\lambda^{(11)} & \lambda^{(12)} & \lambda^{(13)} & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & \lambda^{(21)} & \lambda^{(22)} & \lambda^{(23)} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & \lambda^{(31)} & \lambda^{(32)} & \lambda^{(33)}
\end{bmatrix}$

$Y = \begin{bmatrix} 
\lambda^{(11)} & 0 & 0 & \lambda^{(12)} & 0 & 0 & \lambda^{(13)} & 0 & 0 \\
0 & \lambda^{(21)} & 0 & 0 & \lambda^{(22)} & 0 & 0 & \lambda^{(23)} & 0 \\
0 & 0 & \lambda^{(31)} & 0 & 0 & \lambda^{(32)} & 0 & 0 & \lambda^{(33)}
\end{bmatrix}$

Then $P = X Y^\top$ and $Y = X \Pi$ where 
$\Pi = \begin{bmatrix} 
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1
\end{bmatrix}$

Another way to look at this is:

* Positions 1, 5, 9 are fixed
* The cycles of order 2 are
    * $(2, 4)$
    * $(3, 7)$
    * $(6, 8)$
    
Therefore, we can decompose $\Pi = U I_{6, 3} U^\top$ where the first three 
columns of $U$ consist of $e_1$, $e_5$, and $e_9$ corresponding to the fixed 
positions $1$, $5$, and $9$, the next three columns consist of eigenvectors 
$(e_k + e_l) / \sqrt{2}$, and the last three columns consist of eigenvectors 
$(e_k - e_l) / \sqrt{2}$, where pairs $(k, l)$ correspond to the cycles of 
order 2 described above.

The latent positions are the rows of  
$XU = \begin{bmatrix}
\lambda^{(11)} & 0 & 0 & \lambda^{(12)} / \sqrt{2} & \lambda^{(13)} / \sqrt{2} & 0 & \lambda^{(12)} / \sqrt{2} & \lambda^{(13)} / \sqrt{2} & 0 \\
0 & \lambda^{(22)} & 0 & \lambda^{(21)} / \sqrt{2} & 0 & \lambda^{(23)} / \sqrt{2} & -\lambda^{(21)} / \sqrt{2} & 0 & \lambda^{(23)} / \sqrt{2} \\
0 & 0 & \lambda^{(33)} & 0 & \lambda^{(31)} / \sqrt{2} & \lambda^{(32)} / \sqrt{2} & 0 & -\lambda^{(31)} / \sqrt{2} & -\lambda^{(32)} / \sqrt{2}
\end{bmatrix}$.

# Methods

Two inference objectives arise from the PABM:

1. Cluster membership identification (up to permutation).
2. Parameter estimation (estimating $\lambda^{(kl)}$'s).

Here, we will focus more on (1) and pose possible methods for (2). In our 
methods, we assume that $K$, the number of clusters, is known beforehand and 
does not require estimation.

## Clustering

### Using edge probability matrix $P$

We previously stated one possible set of latent positions that result in the 
edge probability matrix of a PABM, $P = (XU) I_{pq} (XU)^\top$. If we have 
(or can estimate) $XU$ directly, then both the clustering and parameter 
identification problem are trivial since $U$ is orthonormal and fixed for each 
value of $K$. However, direct identification or estimation of $XU$ is not 
possible \cite{rubindelanchy2017statistical}. 

If we decompose $P = Z I_{pq} Z^\top$, then
$\exists Q \in \mathbb{O}(p, q)$ such that $XU = Z Q$. Even if we start with 
the exact edge probability matrix, we cannot recover the "original" latent 
positions $XU$. Note that unlike in the case of the RDPG, 
$Q$ is not an orthogonal matrix. If $z_i$'s are the rows of $XU$, then 
$||z_i - z_j||^2 \neq ||Q z_i - Q z_j||^2$, and 
$\langle z_i, z_j \rangle \neq \langle Q z_i, Q z_j \rangle$. This prevents 
us from using the properties of $XU$ directly. In particular, if 
$Q \in \mathbb{O}(n)$, then we could use the fact that 
$\langle z_i, z_j \rangle = \langle Q z_i, Q z_j \rangle = 0$ if vertices $i$ 
and $j$ are in different clusters. 

We can note from the explicit form of $XU$ that it represents points in 
$\mathbb{R}^{K^2}$ such that points within each cluster lie on $K$-dimensional 
subspaces. Furthermore, the subspaces are orthogonal to each other. 
Multiplication by $Q \in \mathbb{O}(p, q)$ removes the orthogonality property 
but retains the property that each cluster is represented by a $K$-dimensional 
subspace. Using this property, previous work proposes the use of subspace 
clustering while acknowledging some of its shortcomings 
\cite{noroozi2019estimation} \cite{soltanolkotabi2014}. 

**Theorem 3.** 
Let $P = V D V^\top$ be the spectral decomposition of the edge probability
matrix. Let $B = V V^\top$. Then $B_{ij} = 0$ if vertices $i$ and $j$ are 
of different clusters.

*Proof (sketch)*
By projection, $V V^\top = X (X^\top X)^{-1} X^\top$ where $X$ is defined as 
in Theorem 2. Since $X$ is block diagonal with each block corresponding 
to one cluster, $X (X^\top X)^{-1} X^\top$ is also a block diagonal matrix 
with each block corresponding to a cluster and zeros elsewhere. Therefore, 
if vertices $i$ and $j$ belong to different clusters, then the $ij$^th^ element 
of $X (X^\top X)^{-1} X^\top = V V^\top = B$ is 0.

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwData{Edge probability matrix $P$, number of clusters $K$}
  \KwResult{Cluster assignments $1, ..., K$}
    Compute the spectral decomposition $P = V D V^\top$.\;
    Compute the inner product matrix $B = |V V^\top|$, applying $|\cdot|$ 
    entry-wise.\;
    Construct graph $G$ using $B$ as edge similarities.\;
    Identify the connected components of $G$ and map each to cluster labels 
    $1, ..., K.$\;
  \caption{PABM clustering on the edge probability matrix.}
\end{algorithm}

### Using adjacency matrix $A$

The adjacency embedding of $A$ approaches latent positions that form $P$ as 
the number of vertices $n$ increases. More 
precisely, let $\{\lambda^{(kl)}\}_K \sim \mathcal{F}_K$ for some joint 
distribution consisting of $K$ underlying clusters $\mathcal{F}_K$. Then the 
latent positions $XU \sim \mathcal{G}_K$ for some related joint distribution 
with $K$ underlying clusters $\mathcal{G}_K$. Denote $Z_n$ as a sample of size 
$n$ from $\mathcal{G}_K$ and adjacency matrix $A_n$ as one draw from edge 
probability matrix $P_n = Z_n I_{pq} Z_n^\top$. Let $\hat{Z}_n$ be the 
adjacency embedding of $A_n$ with rows $(\hat{z}_i^{(n)})^\top$. Then by
\citet*{rubindelanchy2017statistical}, 

$$\max\limits_{i \in \{1, ..., n\}} ||Q_n \hat{z}_i^{(n)} - z_i^{(n)}|| = O_P\bigg(\frac{(\log n)^c}{n^{1/2}} \bigg)$$

for some $c>0$ and sequence of $Q_n \in \mathbb{O}(p, q)$. In addition, 
\citeauthor{rubindelanchy2017statistical} produce a central limit theorem 
result.

**Theorem 4**. Let $\hat{V}^{(n)} \in \mathbb{R}^{n \times K^2}$ be the matrix 
of $K^2$ eigenvectors of $A_n$ corresponding to the $K (K+1) / 2$ most positive 
eigenvalues and $K (K-1) / 2$ most negative eigenvalues with rows
$(\hat{v}_i^{(n)})^\top$. Let $(i, j)$ correspond to pairs belonging to 
different clusters. Then for some $c > 0$, 

$$\max\limits_{i, j} ||(\hat{v}^{(n)}_i)^\top \hat{v}_j^{(n)}|| = O_P\bigg( \frac{(\log n)^c}{n}\bigg)$$

In addition, by the central limit theorem, 
$(\hat{v}^{(n)}_i)^\top \hat{v}_j^{(n)}$ converge to a normal distribution 
centered at $0$. 

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwData{Adjacency matrix $A$, number of clusters $K$}
  \KwResult{Cluster assignments $1, ..., K$}
    Compute the eigenvectors of $A$ that correspond to the $K (K+1) / 2$ most 
    positive eigenvalues and $K (K-1) / 2$ most negative eigenvalues. Construct 
    $V$ using these eigenvectors as its columns.\;
    Compute $B = |V V^\top|$, applying $|\cdot|$ entry-wise.\;
    Construct graph $G$ using $B$ as its similarity matrix.\;
    Partition $G$ into $K$ disconnected components 
    (e.g., using edge thresholding or spectral clustering).\;
    Map each partition to the cluster labels $1, ..., K$.\;
  \caption{PABM clustering on the adjacency matrix.}
\end{algorithm}

## Parameter estimation

For any $P$ edge probability matrix for the PABM such that the rows and 
columns are organized by cluster, the $kl$^th^ block is an outer product 
of two vectors, i.e., $P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top$. 
Therefore, given $P^{(kl)}$, $\lambda%{(kl)}$ and $\lambda^{(lk)}$ are 
solvable exactly (up to multiplication by $-1$) using singular value 
decomposition. More specifically, let $P = \sigma^2 u v^\top$ be the 
singular value decomposition of $P$. $u \in \mathbb{R}^{n_k}$ and 
$v \in \mathbb{R}^{n_l}$ are vectors and 
$\sigma^2 > 0$ is a scalar. Then $\lambda^{(kl)} = \pm \sigma u$ and 
$\lambda^{(lk)} = \pm \sigma v$.

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetAlgoLined
  \caption{PABM parameter estimation using the edge probability matrix.}
  \KwData{Edge probability matrix $P$, cluster assignments $1, ..., K$.}
  \KwResult{PABM parameters $\{\lambda^{(kl)}\}_K$}
  Arrange the rows and columns of $P$ by cluster such that each $P^{(kl)}$ 
  block consists of edge probabilities between clusters $k$ and $l$.\; 
  \For{$k, l = 1, ..., K$, $k \geq l$} {
    Compute $P^{(kl)} = (\sigma^{(kl)})^2 u^{(kl)} (v^{(kl)})^\top$, the SVD of 
    the $kl^{th}$ block.\;
    Assign $\lambda^{(kl)} \leftarrow \pm \sigma^{(kl)} u^{(kl)}$ and 
    $\lambda^{(lk)} \leftarrow \pm \sigma^{(kl)} v^{(kl)}$.
  }
\end{algorithm}

A similar method can be applied using $\hat{Z}$, the adjacency spectral 
embedding of $A$.

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetAlgoLined
  \caption{PABM parameter estimation using the adjacency matrix.}
  \KwData{Adjacency matrix $A$, cluster assignments $1, ..., K$}
  \KwResult{PABM parameter estimates $\{\hat{\lambda}^{(kl)}\}_K$.}
  Assign $p \leftarrow K (K + 1) / 2$ and $q \leftarrow K (K - 1) / 2$.\;
  Compute $\hat{Z}$, the adjacency spectral embedding of $A$ using signature
  $(p, q)$.\;
  Compute $\hat{P} = \hat{Z} I_{pq} \hat{Z}^\top$, the estimate of the edge 
  probability matrix based on the adjacency spectral embedding.\;
  Arrange the rows and columns of $\hat{P}$ by cluster such that each 
  $\hat{P}^{(kl)}$ block consists of estimated edge probabilities between 
  clusters $k$ and $l$.\;
  \For{$k, l = 1, ..., K$, $k \leq l$} {
    Compute $\hat{P}^{(kl)} = U \Sigma V^\top$, the SVD of the $kl^{th}$ 
    block.\;
    Assign $u^{(kl)}$ and $v^{(kl)}$ as the first columns of $U$ and $V$. 
    Assign $\sigma^2 \leftarrow \Sigma_{11}$.\;
    Assign $\hat{\lambda}^{(kl)} \leftarrow \pm \sigma^{(kl)} u^{(kl)}$ and 
    $\hat{\lambda}^{(lk)} \leftarrow \pm \sigma^{(kl)} v^{(kl)}$.
  }
\end{algorithm}

**Theorem 5**. Let $\lambda^{(kl)}_{n_k} \sim F_{n_k}$ for $k, l = 1, ..., K$, 
and let $\{\hat{\lambda}^{(kl)}_{n_k}\}_K$ be the set of estimates computed by 
Algorithm 4. 
Then ... ***TODO***

\begin{comment}

### Using Adjacency Spectral Embedding

For this section, we will focus on the case $K = 2$. Under this condition, the 
PABM is equivalent to the GRDPG with signature $(3, 1)$. 

The adjacency spectral embeddings of both $A$ and $P$ are not unique. In 
particular, let $Z$ be the ASE of $P$. Then $P = Z I_{3, 1} Z^\top$. However, 
for any $Q \in \mathbb{O}(3, 1)$, 
$(ZQ) I_{3, 1} (ZQ)^\top = Z (Q I_{3, 1} Q^\top) Z^\top = Z I_{3, 1} Z^\top = P$,
so $ZQ$ is also a valid ASE Of $P$. If we can find $Q \in \mathbb{O}(3, 1)$ 
such that $ZQ = XU$, we can compute the parameters $\{\lambda^{(kl)}\}$ 
directly. Furthermore, if we instead use the ASE of the adjacency matrix $A$, 
$\exists Q \in \mathbb{O}(3, 1)$ such that $\max\limits_i ||Q \hat{z}_i - XU||$ 
is minimized (and goes to zero under a probabilistic model for
$\{\lambda^{(kl)}\}$'s). 

Thus if we can identify $Q$ such that the $ZQ = XU$ in the case of embedding 
$P$ or $\hat{Z} Q - XU$ is minimized in the case of embedding $A$, we can 
identify or estimate the PABM parameters directly from the embedding. $XU$ is 
unknown, but we can still obtain an embedding $ZQ$ that follows the properties 
of $XU$, which will yield estimates $\{\hat{\lambda}^{(kl)}\}$ that are valid 
in that they will produce the edge probability matrix $P$.[^lambdas] 

[^lambdas]: Note that the set $\{\lambda^{(kl)}\}_K$ that produces a unique 
$P$ is not unique \citep{noroozi2019estimation}.

In particular, we can note the following properties of $XU$:

1. If row $i$ is in cluster 1, then its second index is 0. If it is in cluster 
2, its first index is 0.
2. If row $i$ is in cluster 1, then its third and fourth indices are equal. If 
it is in cluster 2, the fourth index is the negative of its third index.
3. If rows $i$ and $j$ are in different clusters, their dot product is 0.

If we have cluster memberships (either known *a priori* or estimated using a 
clustering method), then we can estimate $XU$ by picking
$Q \in \mathbb{O}(3, 1)$ such that $ZQ$ best fits these properties. 

$\mathbb{O}(3, 1)$ happens to be the Lorentz group, and each 
$Q \in \mathbb{O}(3, 1)$ can be represented as the product of six matrices that 
depend on a total of four parameters \cite{10.1088/978-1-6817-4254-0}. This 
lends itself as an optimization problem:

\begin{equation}
\begin{split}
  \min_{\theta, \phi, \psi, \tau} & ||\xi^{(1, 2)}||^2 + ||\xi^{(2, 1)}||^2 + 
  ||\xi^{(1, 3)} - \xi^{(1, 4)}||^2 + ||\xi^{(2, 3)} + \xi^{(2, 4)}||^2 \\
  \text{s.t. } & \Xi = Z Q(\theta, \phi, \psi, \tau) \\
  & \xi^{(i, j)} \text{ is the } j^{th} \text{ column of } \Xi \text{ with rows from the } i^{th} \text{ cluster}
\end{split}
\end{equation}

Note that if $Q$ can be properly estimated, the asymptotic results from 
\citet{rubindelanchy2017statistical} can be applied here.

\end{comment}

# Simulated Examples

For these examples, we will set the following parameters:

* $K = 2$
* Mixture parameter $\alpha = .5$
* $\lambda^{(kk)} \stackrel{indep}{\sim} Beta(2, 1)$
* $\lambda^{(kl)} \stackrel{indep}{\sim} Beta(1, 2)$ for $k \neq l$

## Clustering

In this part, we will assess Algorithm 2's performance for sample sizes 
$n = 64, 128, 256, 512, 1024$. For each sample size $n$, $100$ sets of 
$\lambda^{(kl)}$'s are drawn and for each set of parameters, one adjacency 
matrix $A$ is drawn and clustered. 

We will not consider clustering using the edge probability matrix $P$ since 
this will always result exact recovery of the original clusters.

```{r fig.cap = 'IQR clustering error using Algorithm 1 for sample sizes from 64 to 1024. Simulations were repeated 100 times for each sample size.', cache = FALSE}
clustering.df <- readr::read_csv('clustering.csv')
clustering.df %>%
  dplyr::group_by(n) %>%
  dplyr::summarise(med.err = median(error),
                   first.q = quantile(error, .25),
                   third.q = quantile(error, .75)) %>%
  ggplot() +
  # scale_y_log10() +
  labs(y = 'error') +
  geom_line(aes(x = n, y = med.err)) +
  geom_errorbar(aes(x = n, ymin = first.q, ymax = third.q))
```

We can also examine how the distribution of $(\hat{v}_i)^\top \hat{v_j}$ varies 
with $n$: 

```{r, fig.cap = 'Between-cluster inner products of the eigenvectors of $A$ for varying sample sizes.', cache = FALSE}
densities.df <- readr::read_csv('densities.csv')

ggplot(densities.df) + 
  geom_density(aes(x = inner.prods, colour = factor(n))) + 
  labs(x = 'between cluster inner products',
       colour = 'n') + 
  scale_colour_brewer(palette = 'Set1')
```

## Parameter estimation

\begin{comment}

```{r}
set.seed(123456)

n <- 64
z <- sample(c(1, 2), n, prob = c(alpha, 1 - alpha), replace = TRUE)
z <- sort(z)

n1 <- sum(z == 1)
n2 <- sum(z == 2)

lambda11 <- rbeta(n1, a1, b1)
lambda22 <- rbeta(n2, a1, b1)
lambda12 <- rbeta(n1, a2, b2)
lambda21 <- rbeta(n2, a2, b2)

X <- cbind(c(lambda11, rep(0, n2)),
           c(lambda12, rep(0, n2)),
           c(rep(0, n1), lambda21),
           c(rep(0, n1), lambda22))
Y <- cbind(c(lambda11, rep(0, n2)),
           c(rep(0, n1), lambda21),
           c(lambda12, rep(0, n2)),
           c(rep(0, n1), lambda22))
P <- X %*% t(Y)
A <- draw.graph(P)
Z <- embedding(P)
Zhat <- embedding(A, 3, 1)
```

Setting $n = 64$ and generating one example set of $\{\lambda^{(kl)}\}$, we can 
see the latent positions $XU$ follow the expected properties 
(Fig. \ref{fig:xu}). 

```{r xu, fig.cap = 'Latent positions $XU$ used to construct the PABM edge probability matrix $P$ under the GRDPG framework. Note that the the points follow the properties outlined in Section 3.2.'}
perm <- cbind(c(1, 0, 0, 0),
              c(0, 0, 1, 0),
              c(0, 1, 0, 0),
              c(0, 0, 0, 1))
U <- eigen(perm)$vectors[, c(3, 1, 2, 4)]
XU <- X %*% U

pairs(XU, asp = 1, col = z)
```

If we instead take the ASE of $P = X \Pi X^\top$, we fail to obtain an 
embedding that has the same properties (Fig. \ref{fig:ase_of_p}).

```{r ase_of_p, fig.cap = 'The adjacency spectral embedding of the edge probability matrix $P$. Note that even though $P = (XU) I_{p, q} (XU)^\\top$, the resulting embedding does not follow the same properties of $XU$, even up to rotation.'}
pairs(Z, asp = 1, col = z)
```

Denoting $Z$ as the ASE of $P$, we can recover $ZQ$ that is equivalent to $XU$ 
(Fig. \ref{fig:q_trans}).

```{r q_trans, fig.cap = 'Pair plots of $ZQ$ where $Q$ was found by minimizing objective function (1). This embedding is equivalent to $XU$ up to 90-degree rotations and reflection along axes.', cache = TRUE}
XU.prime <- transform.embedding(Z, z, runif(4, -pi, pi))
pairs(XU.prime, col = z, asp = 1)
```

The ASE of the adjacency matrix $A$ (as one realization of $P$) has the 
resulting pairs plot:

```{r a_ase, fig.cap = 'Pair plots of the adjacency spectral embedding of $A$ Since $K=2$ is fixed, we use the signature $(3, 1)$ in this embedding.'}
pairs(Zhat, col = z, asp = 1)
```

Minimizing the objective function in (1) results in the following embedding:

```{r q_from_a, cache = TRUE, fig.cap = 'Pair plots of $\\hat{Z} $ where $\\hat{Z}$ is the ASE of an adjacency matrix of a PABM and $Q$ is estimated by minimizing objective function (1).'}
XU.hat <- transform.embedding(Zhat, z, runif(4, -pi, pi))
pairs(XU.hat, col = z, asp = 1)

# X.hat <- XU.hat %*% t(U)
# plot(abs(X.hat) > .25)
```

\end{comment}

```{r}
estimate.lambda.block <- function(P.block, within = FALSE) {
  if (within) {
    P.svd <- svd(P.block)
    u <- P.svd$u[, 1]
    if (mean(u) < 0) u <- -u
    s <- sqrt(P.svd$d[1])
    return(s * u)
  } else {
    P.svd <- svd(P.block)
    u <- P.svd$u[, 1]
    if (mean(u) < 0) u <- -u
    v <- P.svd$v[, 1]
    if (mean(v) < 0) v <- -v
    s <- sqrt(P.svd$d[1])
    return(list(s * u, s * v))
  }
}

lambda.rmse <- function(lambda.matrix, Phat, clustering) {
  K <- max(clustering)
  n <- nrow(Phat)
  n.vector <- sapply(seq(K), function(k) sum(clustering == k))
  sapply(seq(K), function(k) {
    low.ind.k <- ifelse(k == 1, 1, sum(n.vector[seq(k - 1)]) + 1)
    high.ind.k <- sum(n.vector[seq(k)])
    sapply(seq(k), function(l) {
      P.block <- Phat[clustering == k, clustering == l]
      if (k == l) {
        lambda <- as.numeric(lambda.matrix[seq(low.ind.k, high.ind.k), l])
        lambda.hat <- estimate.lambda.block(P.block, TRUE)
        return(sum((lambda - lambda.hat) ** 2))
      } else {
        low.ind.l <- ifelse(l == 1, 1, sum(n.vector[seq(l = 1)]) + 1)
        high.ind.l <- sum(n.vector[seq(l)])
        lambda.kl <- as.numeric(lambda.matrix[seq(low.ind.k, high.ind.k), l])
        lambda.lk <- as.numeric(lambda.matrix[seq(low.ind.l, high.ind.l), k])
        lambda.hats <- estimate.lambda.block(P.block)
        return(sum((lambda.kl - lambda.hats[[1]]) ** 2) + 
                 sum((lambda.lk - lambda.hats[[2]]) ** 2))
      }
    }) %>% 
      sum() %>% 
      return()
  }) %>% 
    sum() %>% 
    magrittr::divide_by(n * K) %>% 
    sqrt() %>% 
    return()
}
```

Figure \ref{fig:lambda_est} shows the medians and interquartile ranges of root 
mean square errors for Algorithm 4 over 100 simulations using the same 
parameters as before. 

```{r lambda_est, cache = TRUE, fig.cap = 'Median and IQR RMSE from Algorithm 4 using sample sizes from 64 to 1024. Simulations were repeated 100 times for each sample size.'}
rmse.df <- readr::read_csv('rmse.csv')

rmse.df %>% 
  dplyr::group_by(n) %>% 
  dplyr::summarise(median.rmse = median(rmse),
                   q1.rmse = quantile(rmse, .25),
                   q3.rmse = quantile(rmse, .75)) %>% 
  dplyr::ungroup() %>% 
  ggplot() + 
  scale_y_log10() + 
  scale_x_log10() +
  geom_line(aes(x = n, y = median.rmse)) + 
  geom_errorbar(aes(x = n, ymin = q1.rmse, ymax = q3.rmse)) + 
  labs(y = 'RMSE')
```

# References