---
title: "Connecting the Popularity Adjusted Block Model to the Generalized Random Dot Product Graph for Community Detection and Parameter Estimation"
authors: John Koo, Minh Tang, Michael Trosset
# author: John Koo
# author2: Minh Tang
# author3: Michael Trosset
output:
  pdf_document:
    citation_package: natbib
    number_sections: yes
# output:
#   bookdown::pdf_document2:
#     citation_package: natbib
#     number_citations: yes
# output: html_document
# output: rticles::rss_article
fontsize: 12pt
# geometry: "left=1in,right=1in,top=1in,bottom=1in"
urlcolor: blue
header-includes:
- \usepackage{float}
- \usepackage{mathtools}
- \usepackage{natbib}
- \usepackage[linesnumbered,ruled,vlined]{algorithm2e} 
- \setcitestyle{numbers,square}
- \usepackage{verbatim}
- \usepackage{amsthm}
- \usepackage{comment}
bibliography: misc.bib
abstract: |
  We connect two random graph models, the Popularity Adjusted Block Model 
  (PABM) and the Generalized Random Dot Product Graph (GRDPG), demonstrating
  that a PABM is a GRDPG in which communities correspond to certain mutually
  orthogonal subspaces of latent positions. This insight leads to the
  construction of improved algorithms for community detection and parameter
  estimation with PABM. Using established asymptotic properties of Adjacency
  Spectral Embedding (ASE) for GRDPG, we derive asymptotic properties of these
  algorithms, including algorithms that rely on Sparse Subspace Clustering
  (SSC). We illustrate these properties via simulation.
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      # eval = FALSE,
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 3, 
                      fig.width = 5, 
                      fig.dpi = 300)

options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')
```

```{r}
import::from(magrittr, `%>%`)
library(plot.matrix)
library(ggplot2)
library(mclust)

theme_set(theme_bw())
```

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\newcommand{\diag}{\text{diag}}
\newcommand{\tr}{\text{Tr}}
\newcommand{\blockdiag}{\text{blockdiag}}
\newcommand{\indep}{\stackrel{\text{indep}}{\sim}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\Bernoulli}{\text{Bernoulli}}
\newcommand{\Betadist}{\text{Beta}}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{example}
\newtheorem*{example}{Example}

# Introduction

Statistical analysis on graphs or networks often involves the partitioning of 
a graph into disconnected subgraphs or clusters. This is often motivated by the 
assumption that there exist underlying and unobserved communities to which each 
vertex of the graph belongs, and edges between pairs of vertices are determined 
by drawing from a probability distribution based on the community relationships 
between each pair. The goal of the analysis then is population community 
detection, or the recovery of the true underlying community labels for each 
vertex, up to permutation (with some additional parameter estimation being of 
possible interest), assuming some underlying probability model. One such model
is the Stochastic Block Model (SBM), first proposed by 
\citet{doi:10.1080/0022250X.1971.9989788}, which assumes that the edge 
probability from one vertex to another follows a Beronulli distribution with 
fixed probabilities for each pair of community labels. Other random 
graph models have been proposed and studied, such as the Degree-Corrected Block 
Model (DCBM), introduced by \citet{Karrer_2011}, which is a generalization of 
the SBM. The Popularity Adjusted Block Model (PABM) was then introduced by 
\citet*{307cbeb9b1be48299388437423d94bf1} as a generalization of the DCBM to 
address the heterogeneity of edge probabilities within and between communities 
while still maintaining distinct community structure. 

The underlying similarity among the SBM, PABM, and other such models is that 
they involve a symmetric edge probability matrix $P \in [0, 1]^{n \times n}$ 
where $n$ is the number of vertices in the graph. An undirected and unweighted 
graph is then drawn from this edge probability matrix such that the existence 
of an edge between each pair of vertices $i$ and $j$ is given by 
$\Bernoulli(P_{ij})$. For example, for the SBM with two communities for which 
the within-community edge probability is $\xi$ and the between-community edge 
probability is $\eta$, the entries of $P$ consist of $P_{ij} = \xi$ if $i$ and 
$j$ are in the same community and $i \neq j$, $P_{ij} = \eta$ if $i$ and $j$ 
belong to separate communities, and $P_{ii} = 0$. 

The Random Dot Product Graph (RDPG) model proposed by 
\citet*{10.1007/978-3-540-77004-6_11} is another graph model with Bernoulli 
edge probabilities. Under this model, each vertex of the graph can be
represented by a point in some latent space such that the edge
probability between any pair of vertices is given by their corresponding dot
product in the latent space, i.e., given a latent positions
$x_1, ..., x_n \in \mathbb{R}^d$, the edge probability matrix is 
$P = X X^\top$ where 
$X = \begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix}^\top$. The SBM is 
equivalent to a special case of the RDPG model in which all vertices of a 
given community share the same position in the latent space 
\cite{lyzinski2014}. It has also been shown that similar 
random graph models, including the DCBM, can be represented in this way
\cite{lyzinski2014} \cite{rubindelanchy2017consistency}. An analogous 
property exists for the PABM but not for the RDPG model but under the 
*Generalized* Random Dot Product Graph (GRDPG) model. This relationship will be 
explored in this paper and exploited to construct algorithms for community 
detection and parameter estimation for the PABM. 

In this paper, we will only consider undirected graphs, that is the edge weight 
from vertex $i$ to vertex $j$ is equal to the edge weight in the opposite 
direction, from vertex $j$ to vertex $i$. Furthermore, we will only consider 
unweighted graphs with binary ($0$ or $1$) edge weights We will also assume that 
graphs are hollow, i.e., there are no edges from a vertex to itself. All such 
graphs can be represented by a symmetric adjacency matrix 
$A \in \{0, 1\}^{n \times n}$ for which $A_{ij} = 1$ if there exists an edge 
between vertices $i$ and $j$ and $0$ otherwise, and $A$ is an element-wise 
independent Bernoulli draw from a symmetric edge probability matrix 
$P \in [0, 1]^{n \times n}$. 

# Connecting the Popularity Adjusted Block Model to the Generalized Random Dot Product Graph

## The popularity adjusted block model and the generalized random dot product graph

\begin{definition}[Popularity Adjusted Block Model]
\label{pabm}
Let $P \in [0, 1]^{n \times n}$ be a symmetric edge probability matrix for a 
set of $n$ 
vertices, $V$. Each vertex has a community label $1, ..., K$, and the rows and 
columns of $P$ are arranged by community label such that $n_k \times n_l$ block 
$P^{(kl)}$ describes the edge probabilities between vertices in communities 
$k$ and $l$ ($P^{(lk)} = (P^{(kl)})^\top$). 
Let graph $G = (V, E)$ be an undirected, unweighted graph such 
that its corresponding adjacency matrix $A \in \{0, 1\}^{n \times n}$ is a 
realization of $\Bernoulli(P)$, i.e., 
$A_{ij} \indep \Bernoulli(P_{ij})$ for $i > j$ 
($A_{ij} = A_{ji}$ and $A_{ii} = 0$). 

If each block $P^{(kl)}$ can be written as the outer product of two vectors:

\begin{equation} \label{eq:pabm}
  P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top
\end{equation}

for a set of $K^2$ fixed vectors $\{\lambda^{(st)}\}_{s, t = 1}^K$ where each 
$\lambda^{(st)}$ is a column vector 
of dimension $n_s$, then graph $G$ and its corresponding adjacency matrix $A$ 
is a realization of a popularity adjusted block model with parameters 
$\{\lambda^{(st)}\}_{s, t = 1}^K$. 
\end{definition}

We will use the notation $A \sim PABM(\{\lambda^{(kl)}\}_K)$ to denote a random 
adjacency matrix $A$ drawn from a PABM with parameters $\lambda^{(kl)}$ 
consisting of $K$ underlying communities.

\begin{definition}[Generalized Random Dot Product Graph] 
\label{grdpg} 
Let $P \in [0, 1]^{n \times n}$ be a symmetric edge probability matrix for a 
set of $n$ vertices, $V$. If $\exists X \in \mathbb{R}^{n \times d}$ such that 

\begin{equation} \label{eq:grdpg}
  P = X I_{pq} X^\top
\end{equation}

for some $d, p, q \in \mathbb{N}$ and $p + q = d$, then 
graph $G = (V, E)$ with adjacency matrix $A$ such that 
$A_{ij} \indep \Bernoulli(P_{ij})$ for $i > j$ ($A_{ij} = A_{ji}$ and 
$A_{ii} = 0$) is a draw from the generalized random dot product graph model 
with latent positions $X$ and signature $(p, q)$. More precisely, if vertices 
$i$ and $j$ have latent positions $x_i$ and $x_j$ respectively, then the edge 
probability between the two is $P_{ij} = x_i^\top I_{pq} x_j$, and $X$ contains 
the latent positions as rows $x_i^\top$. 
\end{definition}

We will use the notation $A \sim GRDPG_{p,q}(X)$ to denote a random adjacency 
matrix $A$ drawn from latent positions $X$ and signature $(p, q)$.

\begin{definition}[Indefinite Orthogonal Group] 
The indefinite orthogonal group with signature $(p, q)$ is 
the set $\{Q \in \mathbb{R}^{d \times d} : Q I_{pq} Q^{\top} = I_{pq}\}$, 
denoted as $\mathbb{O}(p, q)$.
\end{definition}

\begin{remark}
Like the RDPG, the latent positions of a GRDPG are not unique 
\cite{rubindelanchy2017statistical}. 
More specifically, if $P_{ij} = x_i^\top I_{pq} x_j$, then we also have for any 
$Q \in \mathbb{O}(p, q)$, 
$(Q x_i)^\top I_{pq} (Q x_j) = x_i^\top (Q^\top I_{pq} Q) x_j = 
x_i^\top I_{pq} x_j = P_{ij}$. 
Unlike in the RDPG case, transforming the latent positions via multiplication 
by $Q \in \mathbb{O}(p, q)$ does not necessarily maintain interpoint angles or 
distances. 
\end{remark}

## Connecting the PABM to the GRDPG

\begin{theorem}[Connecting the PABM to the GRDPG for $K = 2$]
\label{theorem1}  
Let 

$$X = \begin{bmatrix} 
\lambda^{(11)} & \lambda^{(12)} & 0 & 0 \\ 
0 & 0 & \lambda^{(21)} & \lambda^{(22)} 
\end{bmatrix}$$

$$U = \begin{bmatrix} 1 & 0 & 0 & 0 \\
0 & 0 & 1 / \sqrt{2} & 1 / \sqrt{2} \\
0 & 0 & 1 / \sqrt{2} & - 1 / \sqrt{2} \\
0 & 1 & 0 & 0 \end{bmatrix}$$

where each $\lambda^{(kl)}$ is a vector as in Definition 1. 
Then $A \sim GRDPG_{3, 1}(X U)$ and $A \sim PABM(\{(\lambda^{(kl)}\}_2)$ are 
equivalent.
\end{theorem}

\begin{theorem}[Generalization to $K > 2$] 
\label{theorem2}
There exists a block diagonal matrix 
$X \in \mathbb{R}^{n \times K^2}$ defined by PABM parameters 
$\{\lambda^{(kl)}\}_K$ and orthonormal matrix 
$U \in \mathbb{R}^{K^2 \times K^2}$ that is fixed 
for each $K$ such that $A \sim GRDPG_{K (K+1) / 2, K (K-1) / 2}(XU)$ and 
$A \sim PABM(\{(\lambda^{(kl)}\})_K)$ are equivalent.
\end{theorem}

\begin{proof}
Define the following matrices from $\{\lambda^{(kl)}\}_K$: 

$$\Lambda^{(k)} = 
\begin{bmatrix} \lambda^{(k,1)} & \cdots & \lambda^{(k, K)} \end{bmatrix}
\in \mathbb{R}^{n_k \times K}$$

\begin{equation} \label{eq:xy}
X = \blockdiag(\Lambda^{(1)}, ..., \Lambda^{(K)}) \in \mathbb{R}^{n \times K^2}
\end{equation}

$$L^{(k)} = \blockdiag(\lambda^{(1k)}, ..., \lambda^{(Kk)}) \in 
\mathbb{R}^{n \times K}$$

$$Y = \begin{bmatrix} L^{(1)} & \cdots & L^{(K)} \end{bmatrix} \in 
\mathbb{R}^{n \times K^2}$$

Then $P = X Y^\top$.

Similar to the $K = 2$ case, we have $Y = X \Pi$ for a permutation matrix
$\Pi$, resulting in $P = X \Pi X^\top$.  
The permutation described by $\Pi$ has $K$ fixed points, which correspond to 
$K$ eigenvalues equal to $1$ with corresponding eigenvectors $e_k$ where 
$k = r (K + 1) + 1$ for $r = 0, ..., K - 1$. It also has 
$\binom{K}{2} = K (K - 1) / 2$ cycles of order $2$. Each cycle corresponds to 
a pair of eigenvalues $+1$ and $-1$ and a pair of eigenvectors 
$(e_s + e_t) / \sqrt{2}$ and $(e_s - e_t) / \sqrt{2}$.

Then $\Pi$ has $K (K + 1) / 2$ eigenvalues equal to $1$ and $K (K - 1) / 2$ 
eigenvalues equal to $-1$. $\Pi$ has the decomposed form 

\begin{equation} \label{eq:permutation}
\Pi = U I_{K (K + 1) / 2, K (K - 1) / 2} U^\top
\end{equation}

The edge probability matrix then can be written as:

\begin{equation} \label{eq:pabm-grdpg}
P = X U I_{p, q} (X U)^\top
\end{equation}

\begin{equation} \label{eq:p}
p = K (K + 1) / 2
\end{equation}

\begin{equation} \label{eq:q}
q = K (K - 1) / 2
\end{equation}

and we can describe the PABM with $K$ communities as a GRDPG with latent 
positions $X U$ with signature $\big( K (K + 1) / 2, K (K - 1) / 2 \big)$.
\end{proof}

\begin{example}[$K = 3$] Using the same notation as in Theorem \ref{theorem2}:

$$X = \begin{bmatrix} 
\lambda^{(11)} & \lambda^{(12)} & \lambda^{(13)} & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & \lambda^{(21)} & \lambda^{(22)} & \lambda^{(23)} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & \lambda^{(31)} & \lambda^{(32)} & \lambda^{(33)}
\end{bmatrix}$$

$$Y = \begin{bmatrix} 
\lambda^{(11)} & 0 & 0 & \lambda^{(12)} & 0 & 0 & \lambda^{(13)} & 0 & 0 \\
0 & \lambda^{(21)} & 0 & 0 & \lambda^{(22)} & 0 & 0 & \lambda^{(23)} & 0 \\
0 & 0 & \lambda^{(31)} & 0 & 0 & \lambda^{(32)} & 0 & 0 & \lambda^{(33)}
\end{bmatrix}$$

Then $P = X Y^\top$ and $Y = X \Pi$ where $\Pi$ is a permutation matrix 
consisting of $3$ fixed points and $3$ cycles of order 2:

$$\Pi = \begin{bmatrix} 
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1
\end{bmatrix}$$

* Positions 1, 5, 9 are fixed.

* The cycles of order 2 are $(2, 4)$, $(3, 7)$, and $(6, 8)$.
    
Therefore, we can decompose $\Pi = U I_{6, 3} U^\top$ where the first three 
columns of $U$ consist of $e_1$, $e_5$, and $e_9$ corresponding to the fixed 
positions $1$, $5$, and $9$, the next three columns consist of eigenvectors 
$(e_k + e_l) / \sqrt{2}$, and the last three columns consist of eigenvectors 
$(e_k - e_l) / \sqrt{2}$, where pairs $(k, l)$ correspond to the cycles of 
order 2 described above.

The latent positions are the rows of  
$$XU = \begin{bmatrix}
  \lambda^{(11)} & 0 & 0 & 
  \lambda^{(12)} / \sqrt{2} & \lambda^{(13)} / \sqrt{2} & 0 & 
  \lambda^{(12)} / \sqrt{2} & \lambda^{(13)} / \sqrt{2} & 0 \\
  0 & \lambda^{(22)} & 0 & 
  \lambda^{(21)} / \sqrt{2} & 0 & \lambda^{(23)} / \sqrt{2} & 
  -\lambda^{(21)} / \sqrt{2} & 0 & \lambda^{(23)} / \sqrt{2} \\
  0 & 0 & \lambda^{(33)} & 
  0 & \lambda^{(31)} / \sqrt{2} & \lambda^{(32)} / \sqrt{2} & 
  0 & -\lambda^{(31)} / \sqrt{2} & -\lambda^{(32)} / \sqrt{2}
\end{bmatrix}$$
\end{example}

# Methods

Two inference objectives arise from the PABM:

1. Community membership identification (up to permutation).
2. Parameter estimation (estimating $\lambda^{(kl)}$'s).

In our methods, we assume that $K$, the number of communities, is known 
beforehand and does not require estimation.

## Related work

\citeauthor{307cbeb9b1be48299388437423d94bf1}, who first proposed the PABM, 
used Modularity Maximization (MM) and the Extreme Points
(EP) algorithm \cite{le2016} for community detection and parameter estimation. 
They were able to show that as the sample size increases, the proportion of
misclassified community labels (up to permutation) goes to 0. 

\citet*{noroozi2019estimation} used Sparse Subspace Clustering (SSC) for 
community detection in the PABM. 
SSC is performed by solving an optimization problem for each observed point.
Given $X \in \mathbb{R}^{n \times d}$ with vectors 
$x_i^\top \in \mathbb{R}^d$ as rows of $X$, the optimization problem 
$\min_{c_i} ||c_i||_1$ subject to $x_i = X c_i$ and $\beta_i = 0$ is solved 
for each $i = 1, ..., n$. The solutions are collected collected into matrix 
$C = \begin{bmatrix} c_1 & \cdots & c_n \end{bmatrix}^\top$ to construct an 
affinity matrix $B = |C| + |C^\top|$. If each $x_i$ lie perfectly on one of $K$ 
subspaces, $B$ describes an undirected graph consisting of $K$ disjoint 
subgraphs, i.e., $B_{ij} = 0$ if $x_i, x_j$ are in different subspaces. 
If $X$ instead represents points near $K$ subspaces with some noise, a final 
graph partitioning step is performed (e.g., edge thresholding or spectral 
clustering). 

In practice, SSC is often performed by solving the LASSO problems 

\begin{equation} \label{eq:ssc}
c_i = \arg\min_c \frac{1}{2} ||x_i - X_{-i} c||^2_2 + \lambda ||c||_1
\end{equation}

for some sparsity parameter $\lambda > 0$. The $c_i$ vectors are then collected 
into $C$ and $B$ as before. 

\begin{definition}[Subspace Detection Property] 
Let $X = \begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix}^\top$ be noisy 
points sampled from $K$ subspaces. Let $C$ and $B$ be constructed from the 
solutions of LASSO problems as described in (\ref{eq:ssc}). If each column of 
$C$ has nonzero norm and $B_{ij} = 0$ $\forall$ $x_i$ and $x_j$ sampled from 
different subspaces, then $X$ obeys the subspace detection property. 
\end{definition}

\begin{remark} 
In practice, a noisy sample $X$ often does not obey the subspace detection
property. In such cases, $B$ is treated as an affinity matrix for a graph which 
is then partitioned into $K$ subgraphs to obtain the clustering. On the other 
hand, if $X$ does obey the subspace detection property, $B$ describes a graph 
with at least $K$ disconnected subgraphs. Ideally, when the subspace detection
property holds, there are exactly $K$ subgraphs which map to each subspace, 
but it could be the case that some of the subspaces are represented by 
multiple disconnected subgraphs. The subspace detection property is contingent 
on choosing a sufficiently large sparsity parameter $\lambda$. 
\end{remark}

Theorem \ref{theorem2} suggests that SSC is appropriate for community 
detection for the PABM. 
More precisely, Theorem \ref{theorem2} says that each community consists of a
$K$-dimensional subspace, and together the subspaces lie in $\mathbb{R}^{K^2}$. 
The natural approach then is to perform SSC on the ASE of $P$ or $A$.
\citeauthor{noroozi2019estimation} instead applied SSC to $P$ and $A$, 
foregoing embedding altogether. 

Using results from \citet{soltanolkotabi2012}, it can be easily shown that 
the subspace detection property holds for $XU$, which is an ASE of $P$. More 
specifically, if points lie exactly on mutually orthogonal subspaces, then the 
subspace detection property will hold with probability 1, and this is exactly 
the case for the PABM (Theorem \ref{theorem2}). 
Much of our work is then built on
\citeauthor{rubindelanchy2017statistical}, who describe the convergence 
behavior of the ASE of $A$ to the ASE of $P$, and \citet{jmlr-v28-wang13},
who show the necessary conditions for the subspace detection property to hold 
in noisy cases where the points lie near subspaces. 

## Community detection

\begin{algorithm}[t]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwData{Adjacency matrix $A$, number of communities $K$}
  \KwResult{Community assignments $1, ..., K$}
    Compute the eigenvectors of $A$ that correspond to the $K (K+1) / 2$ most 
    positive eigenvalues and $K (K-1) / 2$ most negative eigenvalues. Construct 
    $V$ using these eigenvectors as its columns.\;
    Compute $B = |n V V^\top|$, applying $|\cdot|$ entry-wise.\;
    Construct graph $G$ using $B$ as its similarity matrix.\;
    Partition $G$ into $K$ disconnected subgraphs  
    (e.g., using edge thresholding or spectral clustering).\;
    Map each partition to the community labels $1, ..., K$.\;
  \caption{Orthogonal Spectral Clustering.}
\end{algorithm}

We previously stated one possible set of latent positions that result in the 
edge probability matrix of a PABM, $P = (XU) I_{pq} (XU)^\top$. If we have 
(or can estimate) $XU$ directly, then both the community detection and 
parameter identification problem are trivial since $U$ is orthonormal and 
fixed for each value of $K$. However, direct identification or estimation of
$XU$ is not possible \cite{rubindelanchy2017statistical}. 

If we decompose $P = Z I_{pq} Z^\top$, then
$\exists Q \in \mathbb{O}(p, q)$ such that $XU = Z Q$. Even if we start with 
the exact edge probability matrix, we cannot recover the "original" latent 
positions $XU$. Note that unlike in the case of the RDPG, $Q$ is not 
necessarily an orthogonal matrix. If $z_i$'s are the rows of $XU$, then 
$||z_i - z_j||^2 \neq ||Q z_i - Q z_j||^2$, and 
$\langle z_i, z_j \rangle \neq \langle Q z_i, Q z_j \rangle$. This prevents 
us from using the properties of $XU$ directly. In particular, if 
$Q \in \mathbb{O}(n)$, then we could use the fact that 
$\langle z_i, z_j \rangle = \langle Q z_i, Q z_j \rangle = 0$ if vertices $i$ 
and $j$ are in different communities. 

The explicit form of $XU$ represents points in 
$\mathbb{R}^{K^2}$ such that points within each community lie on 
$K$-dimensional orthogonal subspaces. 
Multiplication by $Q \in \mathbb{O}(p, q)$ removes the orthogonality property 
but retains the property that each community is represented by a 
$K$-dimensional subspace. Therefore, the ASE of $P$ results in subspaces that 
correspond to each community, suggesting the use of SSC. In this paper, we will 
use a different and leave the properties of SSC on the ASE of $A$ as future
work.

\begin{theorem}
\label{theorem3}
Let $P = V D V^\top$ be the spectral decomposition of the edge probability
matrix. Let $B = n V V^\top$. Then $B_{ij} = 0$ if vertices $i$ and $j$ are 
from different communities.
\end{theorem}

Theorem \ref{theorem3} provides perfect community detection given $P$. 
Letting $|B|$ be the affinity matrix for graph $G$, $G$ is partitioned into 
at least $K$ disjoint subgraphs since each of the $K$ communities have no 
edges between them. Similar to the subspace detection property, it could be 
the case that some of the communities are represented by multiple disjoint
subgraphs in $G$, in which case additional reconstruction is required to
identify the communities exactly. 

Using $A$ instead of $P$ introduces error, which converges to $0$ 
almost surely:

\begin{theorem}
\label{theorem4} 
Let $\hat{B}_n$ with entries $\hat{B}_n^{(ij)}$ be the affinity matrix from OSC 
(Alg. 1). Then $\forall$ pairs $(i, j)$ belonging to different communities 
and sparsity factor satisfying $n \rho_n = \omega\{(\log n)^{4c}\}$, 

\begin{equation} \label{eq:thm4}
\max_{i, j} |n (\hat{v}_n^{(i)})^\top \hat{v}_n^{(j)}| = 
O_P \Big( \frac{(\log n)^c}{\sqrt{n \rho_n}} \Big)
\end{equation}

This provides the result that for $i, j$ in different communities, 
$\hat{B}_n^{(ij)} \stackrel{a.s.}{\to} 0$.
\end{theorem}

Theorems \ref{theorem2}, \ref{theorem3}, and \ref{theorem4} 
also provide a very natural path toward using SSC for 
community detection for the PABM. We established in Theorem \ref{theorem2}
that an ASE of the edge probability matrix $P$ can be constructed 
such that the communities lie on mutually orthogonal subspaces,
and this property can be recovered from the eigenvectors of $P$. 
Then Theorems \ref{theorem3} and \ref{theorem4} show that this property holds 
for the unscaled ASE of $A$ drawn from $P$ as $n \to \infty$. 

\begin{algorithm}[t]
  \DontPrintSemicolon
  \SetAlgoLined
  \caption{Sparse Subspace Clustering using LASSO \cite{jmlr-v28-wang13}.}
  \KwData{Adjacency matrix $A$, number of communities $K$, 
  hyperparameter $\lambda$}
  \KwResult{Community assignments $1, ..., K$}
    Find $V$, the matrix of eigenvectors of $A$ 
    corresponding to the $K (K + 1) / 2$ most positive 
    and the $K (K - 1) / 2$ most negative eigenvalues.\;
    \For {$i = 1, ..., n$} {
      Assign $v_i^\top$ as the $i^{th}$ row of $V$. 
      Assign $V_{-i} = \begin{bmatrix} 
      v_1 & \cdots & v_{i-1} & v_{i+1} & \cdots & v_n \end{bmatrix}^\top$.\;
      Solve the LASSO problem 
      $c_i = \arg\min_{\beta} 
      \frac{1}{2} ||v_i - V_{-i} \beta||_2^2 + \lambda ||\beta||_1$.\;
      Assign $\tilde{c}_i = \begin{bmatrix} 
      c_i^{(1)} & \cdots & c_i^{(i-1)} & 0 & c_i^{(i)} & \cdots & c_i^{(n-1)}
      \end{bmatrix}^\top$ such that the superscript is the index of 
      $\tilde{c}_i$.\;
    }
    Assign 
    $C = \begin{bmatrix} \tilde{c}_1 & \cdots & \tilde{c}_n \end{bmatrix}$.\;
    Compute the affinity matrix $B = \sqrt{n} (|C| + |C^\top|)$.\;
    Construct graph $G$ using $B$ as its similarity matrix.\;
    Partition $G$ into $K$ disconnected subgraphs (e.g., using edge 
    thresholding or spectral clustering).\;
    Map each partition to the community labels $1, ..., K$. 
\end{algorithm}

\begin{theorem}
\label{theorem5}
Let $P_n$ describe the edge probability matrix of the PABM with 
$n$ vertices, and let $A_n \sim \Bernoulli(P_n)$.  Let $\hat{V}_n$ be the 
matrix of eigenvectors of $A_n$ corresponding to the $K (K + 1) / 2$ most
positive and $K (K - 1) / 2$ most negative eigenvalues. Then 
$\exists \lambda > 0$ such that as $n \to \infty$, $\hat{V}_n$ obeys the 
subspace detection property with probability 1.
\end{theorem}

## Parameter estimation

For any edge probability matrix $P$ for the PABM such that the rows and 
columns are organized by community, the $kl$^th^ block is an outer product 
of two vectors, i.e., $P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top$. 
Therefore, given $P^{(kl)}$, $\lambda^{(kl)}$ and $\lambda^{(lk)}$ are 
solvable exactly (up to multiplication by $-1$) using singular value 
decomposition. More specifically, let $P^{(kl)} = \sigma^2 u v^\top$ be the 
singular value decomposition of $P^{(kl)}$. $u \in \mathbb{R}^{n_k}$ and 
$v \in \mathbb{R}^{n_l}$ are vectors and 
$\sigma^2 > 0$ is a scalar. Then $\lambda^{(kl)} = \pm \sigma u$ and 
$\lambda^{(lk)} = \pm \sigma v$. 
Given the adjacency matrix $A$ instead of edge probability matrix $P$, we can 
simply use plug-in estimators (algorithm 3), which converge to the true 
parameters.

\begin{algorithm}[t]
  \DontPrintSemicolon
  \SetAlgoLined
  \caption{PABM parameter estimation.}
  \KwData{Adjacency matrix $A$, community assignments $1, ..., K$}
  \KwResult{PABM parameter estimates $\{\hat{\lambda}^{(kl)}\}_K$.}
  Arrange the rows and columns of $A$ by community such that each 
  $A^{(kl)}$ block consists of estimated edge probabilities between 
  communities $k$ and $l$.\;
  \For {$k, l = 1, ..., K$, $k \leq l$} {
    Compute $A^{(kl)} = U \Sigma V^\top$, the SVD of the $kl^{th}$ 
    block.\;
    Assign $u^{(kl)}$ and $v^{(kl)}$ as the first columns of $U$ and $V$. 
    Assign $(\sigma^{(kl)})^2 \leftarrow \Sigma_{11}$.\;
    Assign $\hat{\lambda}^{(kl)} \leftarrow \pm \sigma^{(kl)} u^{(kl)}$ and 
    $\hat{\lambda}^{(lk)} \leftarrow \pm \sigma^{(kl)} v^{(kl)}$.
  }
\end{algorithm}

\begin{theorem}
\label{theorem6}
Under regularity and sparsity assumptions, given fixed $K$, 

\begin{equation} \label{eq:thm6}
\max_{k, l \in \{1, ..., K\}} 
||\hat{\lambda}^{(kl)} - \lambda^{(kl)}|| = 
O_P \bigg(\frac{(\log n_k)^c}{\sqrt{n_k}} \bigg)
\end{equation}
\end{theorem}

# Simulated Examples

For each simulation, community labels are drawn from a multinomial 
distribution, the popularity vectors $\{\lambda^{(kl)}\}_K$ are drawn 
from two types of joint distributions depending on whether $k = l$, the edge
probability matrix $P$ is constructed using 
the popularity vectors, and finally an unweighted and undirected adjacency 
matrix $A$ is drawn from $P$. OSC is then used for community detection, 
and this method is compared against SSC \cite{noroozi2019estimation}
\cite{soltanolkotabi2014} and MM \cite{igraph}
\cite{307cbeb9b1be48299388437423d94bf1}. True 
community labels are used with Algorithm 3 to estimate the popularity 
vectors $\{\lambda^{(kl)}\}_K$, and this method is then compared against an
MLE-based estimator described in \citeauthor{noroozi2019estimation} and
\citeauthor{307cbeb9b1be48299388437423d94bf1}. 

Modularity Maximization is NP-hard, so
\citeauthor{307cbeb9b1be48299388437423d94bf1} 
used the Extreme Points (EP) algorithm \cite{le2016}, which is $O(n^{K - 1})$, 
as a greedy relaxation of the optimization problem. 
For these simulations, the Louvain algorithm was used, as the EP algorithm 
proved to be prohibitively computationally expensive for $K > 2$. For $K = 2$, 
it was verified that EP and Louvain produce comparable results. 

Two implementations of SSC are shown here. The first method, denoted as 
SSC-A, treats the columns of the adjacency matrix $A$ as points in
$\mathbb{R}^n$, as described in \citeauthor{noroozi2019estimation}. The second 
method, denoted as SSC-ASE, first embeds $A$ and then performs SSC on the 
embedding, as described in algorithm 2. The sparsity parameter $\lambda$ was 
chosen via a preliminary cross-validation experiment. For the final clustering 
step, a Gaussian Mixture Model was fit on the normalized Laplacian eigenmap of 
the affinity matrix $B$. 

For comparing methods, we define the community detection error as:

$$L_c(\hat{\sigma}, \sigma; \{v_i\}) = 
\min_\pi \sum_i I(\pi \circ \hat{\sigma}(v_i) = \sigma{v_i})$$

where $\sigma(v_i)$ is the true community label of vertex $v_i$, 
$\hat{\sigma}(v_i)$ is a community assignment function, and $\pi$ is a 
permutation operator. This is effectively the "misclustering count" of
$\hat{\sigma}$. 

We then define parameter estimation error as the MSE up to sign flip: 

$$MSE(\{\hat{\lambda}^{(kl)}\}, \{\lambda^{(kl)}\}) = 
\min_{s \in \{-1, 1\}} 
\sqrt{\frac{1}{n} \sum_{k < l} \sum_{i = 1}^{n_k} 
(s \hat{\lambda}^{(kl)}_i - \lambda^{(kl)}_i)^2}$$

## Balanced communities

In each simulation, community labels $z_1, ..., z_n$ were drawn 
from a multinomial distribution with mixture parameters 
$\{\alpha_1, ..., \alpha_K\}$, then $\{\lambda^{(kl)}\}_K$ according to the 
drawn community labels, $P$ was constructed using the drawn 
$\{\lambda^{(kl)}\}_K$, and $A$ was drawn from $P$ by 
$A_{ij} \stackrel{indep}{\sim} Bernoulli(P_{ij})$. Each simulation has a unique
edge probability matrix $P$. 

For these examples, we set the following parameters:

* Number of vertices $n = 128, 256, 512, 1024, 2048, 4096$
* Number of underlying communities $K = 2, 3, 4$
* Mixture parameters $\alpha_k = 1 / K$ for $k = 1, ..., K$, (i.e., each 
community label has an equal probability of being drawn)
* Community labels 
$z_k \iid Multinomial(\alpha_1, ..., \alpha_K)$
* Within-group popularities $\lambda^{(kk)} \iid Beta(2, 1)$
* Between-group popularities 
$\lambda^{(kl)} \iid Beta(1, 2)$ for $k \neq l$

$50$ simulations were performed for each $(n, K)$ pair. 

\begin{comment}

Fig. \ref{fig:clust_err_k} show that for large $n$, OSC results in 
a misclustering rate of 0. 

```{r clust_err_k, fig.cap = 'IQR of community detection error error using OSC (blue) compared against SSC on the ASE of A (purple), MM (red), and SSC on the adjacency matrix (green). Communities are approximately balanced. Simulations were repeated 50 times for each sample size.', fig.width = 10}
clustering.df <- readr::read_csv('clustering-k.csv')
ssc.df <- readr::read_csv('clustering-ssc-k.csv')
clustering.df %>%
  dplyr::group_by(n, K) %>%
  dplyr::summarise(
    med.err = median(error),
    first.q = quantile(error, .25),
    third.q = quantile(error, .75),
    med.err.ssc = median(error.ssc),
    first.q.ssc = quantile(error.ssc, .25),
    third.q.ssc = quantile(error.ssc, .75),
    med.err.ep = median(error.ep, na.rm = TRUE),
    first.q.ep = quantile(error.ep, .25, na.rm = TRUE),
    third.q.ep = quantile(error.ep, .75, na.rm = TRUE),
    med.err.louvain = median(error.louvain),
    first.q.louvain = quantile(error.louvain, .25),
    third.q.louvain = quantile(error.louvain, .75)
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::inner_join(
    ssc.df %>% 
      dplyr::group_by(n, K) %>% 
      dplyr::summarise(med.err.ssc2 = median(error.ssc2),
                       first.q.ssc2 = quantile(error.ssc2, .25),
                       third.q.ssc2 = quantile(error.ssc2, .75)) %>% 
      dplyr::ungroup()
  ) %>% 
  ggplot() +
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096)) +
  # scale_x_continuous(breaks = c(128, 256, 512, 1024, 2048, 4096)) + 
  # scale_y_log10() + 
  labs(y = 'community detection error rate', 
       colour = NULL) +
  geom_line(aes(x = n, y = med.err,
                colour = 'OSC')) +
  geom_errorbar(aes(x = n, ymin = first.q, ymax = third.q,
                    colour = 'OSC'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc,
                colour = 'SSC-ASE')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc, ymax = third.q.ssc,
                    colour = 'SSC-ASE'), width = .1) + 
  # geom_line(aes(x = n, y = med.err.ep,
  #               colour = 'MM-EP')) + 
  # geom_errorbar(aes(x = n, ymin = first.q.ep, ymax = third.q.ep,
  #                   colour = 'MM-EP'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc2,
                colour = 'SSC-A')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc2, ymax = third.q.ssc2,
                    colour = 'SSC-A'), width = .1) + 
  geom_line(aes(x = n, y = med.err.louvain,
                colour = 'MM-Louvain')) + 
  geom_errorbar(aes(x = n, ymin = first.q.louvain, ymax = third.q.louvain,
                    colour = 'MM-Louvain'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  facet_wrap(~ K, labeller = 'label_both')
```

Theorem \ref{theorem4} implies that OSC will result in not just in the error
rate converging to 0 but the error *count* as well.
We explore this in Fig. \ref{fig:clust_err_ct_sim}.

\end{comment}

Fig \ref{fig:clust_err_ct_sim} shows OSC's community detection error going to 0 
for large $n$. SSC on both the embedding and on the adjacency 
matrix produces similar results for $K > 2$. Weaker performance of SSC for 
$K = 2$ can be attributed to the final spectral clustering step of the 
affinity matrix. A GMM was fit to the Laplacian eigenmap, but visual 
inspection suggests that the communities are not distributed as a mixture 
of Gaussians in the eigenmap. While the subspace detection property is 
guaranteed for large $n$, in our simulations, setting a large enough 
sparsity parameter for SSC resulted in more than $K$ disconnected subgraphs. 

```{r clust_err_ct_sim, fig.cap = 'Median and IQR of community detection error. Communities are approximately balanced. Simulations were repeated 50 times for each sample size.', cache = FALSE, fig.width = 10}
clustering.df <- readr::read_csv('clustering-k.csv')
ssc.df <- readr::read_csv('clustering-ssc-k.csv')
clustering.df %>%
  dplyr::group_by(n, K) %>%
  dplyr::summarise(
    med.err = median(error),
    first.q = quantile(error, .25),
    third.q = quantile(error, .75),
    med.err.ssc = median(error.ssc),
    first.q.ssc = quantile(error.ssc, .25),
    third.q.ssc = quantile(error.ssc, .75),
    med.err.ep = median(error.ep, na.rm = TRUE),
    first.q.ep = quantile(error.ep, .25, na.rm = TRUE),
    third.q.ep = quantile(error.ep, .75, na.rm = TRUE),
    med.err.louvain = median(error.louvain),
    first.q.louvain = quantile(error.louvain, .25),
    third.q.louvain = quantile(error.louvain, .75)
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::inner_join(
    ssc.df %>% 
      dplyr::group_by(n, K) %>% 
      dplyr::summarise(med.err.ssc2 = median(error.ssc2),
                       first.q.ssc2 = quantile(error.ssc2, .25),
                       third.q.ssc2 = quantile(error.ssc2, .75)) %>% 
      dplyr::ungroup()
  ) %>% 
  ggplot() +
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096)) +
  # scale_x_continuous(breaks = c(128, 256, 512, 1024, 2048, 4096)) + 
  scale_y_log10() +
  labs(y = 'community detection error count', 
       colour = NULL) +
  geom_line(aes(x = n, y = med.err * n,
                colour = 'OSC')) +
  geom_errorbar(aes(x = n, ymin = first.q * n, ymax = third.q * n,
                    colour = 'OSC'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc * n,
                colour = 'SSC-ASE')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc * n, ymax = third.q.ssc * n,
                    colour = 'SSC-ASE'), width = .1) + 
  # geom_line(aes(x = n, y = med.err.ep * n,
  #               colour = 'MM-EP')) + 
  # geom_errorbar(aes(x = n, ymin = first.q.ep * n, ymax = third.q.ep * n,
  #                   colour = 'MM-EP'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc2 * n,
                colour = 'SSC-A')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc2 * n, ymax = third.q.ssc2 * n,
                    colour = 'SSC-A'), width = .1) + 
  geom_line(aes(x = n, y = med.err.louvain * n,
                colour = 'MM-Louvain')) + 
  geom_errorbar(aes(x = n, 
                    ymin = first.q.louvain * n, ymax = third.q.louvain * n,
                    colour = 'MM-Louvain'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  facet_wrap(~ K, labeller = 'label_both')
```

Given ground truth community labels, Algorithm 3 and the MLE-based plug-in 
estimators \cite{307cbeb9b1be48299388437423d94bf1} \cite{noroozi2019estimation} 
perform similarly, with root mean square error decaying at rate approximately 
$n^{-1/2}$. 

```{r lambda_est_k, cache = FALSE, fig.cap = 'Median and IQR RMSE from Algorithm 3 (red) compared against an MLE-based method (blue). Simulations were repeated 50 times for each sample size. Communities were drawn to be approximately balanced.', fig.width = 10}
rmse.df <- readr::read_csv('rmse-k.csv')

rmse.df %>% 
  na.omit() %>% 
  dplyr::group_by(K, n) %>% 
  dplyr::summarise(median.rmse = median(rmse),
                   q1.rmse = quantile(rmse, .25),
                   q3.rmse = quantile(rmse, .75),
                   median.rmse.mle = median(rmse.mle),
                   q1.rmse.mle = quantile(rmse.mle, .25),
                   q3.rmse.mle = quantile(rmse.mle, .75)) %>% 
  dplyr::ungroup() %>% 
  ggplot() +
  geom_line(aes(x = n, y = median.rmse, colour = 'Algorithm 3')) + 
  geom_errorbar(aes(x = n, ymin = q1.rmse, ymax = q3.rmse,
                    colour = 'Algorithm 3'), width = .1) + 
  geom_line(aes(x = n, y = median.rmse.mle, colour = 'MLE-based')) + 
  geom_errorbar(aes(x = n, ymin = q1.rmse.mle, ymax = q3.rmse.mle,
                   colour = 'MLE-based'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  labs(y = 'RMSE', colour = NULL) + 
  facet_wrap(~ K, labeller = 'label_both') + 
  scale_y_log10() + 
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096))
```

## Imbalanced communities

Simulations performed in this section are similar to those in the previous 
section with the exception of the mixture parameters 
$\{\alpha_1, ..., \alpha_K\}$ used to draw community labels from the 
multinomial distribution. For these examples, we set the following parameters:

* Number of vertices $n = 128, 256, 512, 1024, 2048, 4096$
* Number of underlying communities $K = 2, 3, 4$
* Mixture parameters $\alpha_k = \frac{k^{-1}}{\sum_{l=1}^K l^{-1}}$ for 
$k = 1, ..., K$
* Community labels 
$z_k \iid Multinomial(\alpha_1, ..., \alpha_K)$
* Within-group popularities $\lambda^{(kk)} \iid Beta(2, 1)$
* Between-group popularities 
$\lambda^{(kl)} \iid Beta(1, 2)$ for $k \neq l$

$50$ simulations were performed for each $(n, K)$ pair. 

\begin{comment}

Fig. \ref{fig:clust_err_k_imba} and \ref{fig:clust_err_ct_sim_imba} show 
similar results as in the balanced communities case, with both OSC and 
SSC resulting in no misclustered vertices for a 
sufficiently large sample size. However, Fig. \ref{fig:lambda_est_k_imba} 
suggests that while Algorithm 3 retains $\sqrt{n}$ efficiency, the MLE-based 
plug-in estimator is less efficient for this setup. 

```{r clust_err_k_imba, fig.cap = 'Median and IQR of community detection error using OSC (blue) compared against SSC on the ASE of A (purple), MM (red), and SSC on the adjacency matrix (green). Communities are imbalanced. Simulations were repeated 50 times for each sample size.', fig.width = 10}
clustering.df <- readr::read_csv('clustering-k-imbalanced.csv')
ssc.df <- readr::read_csv('clustering-ssc-k-imbalanced.csv')
clustering.df %>%
  dplyr::group_by(n, K) %>%
  dplyr::summarise(
    med.err = median(error),
    first.q = quantile(error, .25),
    third.q = quantile(error, .75),
    med.err.ssc = median(error.ssc),
    first.q.ssc = quantile(error.ssc, .25),
    third.q.ssc = quantile(error.ssc, .75),
    med.err.ep = median(error.ep, na.rm = TRUE),
    first.q.ep = quantile(error.ep, .25, na.rm = TRUE),
    third.q.ep = quantile(error.ep, .75, na.rm = TRUE),
    med.err.louvain = median(error.louvain),
    first.q.louvain = quantile(error.louvain, .25),
    third.q.louvain = quantile(error.louvain, .75)
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::inner_join(
    ssc.df %>% 
      dplyr::group_by(n, K) %>% 
      dplyr::summarise(med.err.ssc2 = median(error.ssc2),
                       first.q.ssc2 = quantile(error.ssc2, .25),
                       third.q.ssc2 = quantile(error.ssc2, .75)) %>% 
      dplyr::ungroup()
  ) %>% 
  ggplot() +
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096)) +
  # scale_x_continuous(breaks = c(128, 256, 512, 1024, 2048, 4096)) + 
  labs(y = 'community detection error rate', 
       colour = NULL) +
  geom_line(aes(x = n, y = med.err,
                colour = 'OSC')) +
  geom_errorbar(aes(x = n, ymin = first.q, ymax = third.q,
                    colour = 'OSC'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc,
                colour = 'SSC-ASE')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc, ymax = third.q.ssc,
                    colour = 'SSC-ASE'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ep,
                colour = 'MM-EP')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ep, ymax = third.q.ep,
                    colour = 'MM-EP'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc2,
                colour = 'SSC-A')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc2, ymax = third.q.ssc2,
                    colour = 'SSC-A'), width = .1) + 
  geom_line(aes(x = n, y = med.err.louvain,
                colour = 'MM-Louvain')) + 
  geom_errorbar(aes(x = n, ymin = first.q.louvain, ymax = third.q.louvain,
                    colour = 'MM-Louvain'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  facet_wrap(~ K, labeller = 'label_both') + 
  scale_y_log10()
```

\end{comment}

We again see community detection error trending to 0 for OSC, as well as for 
SSC when $K > 2$ (Fig. \ref{fig:clust_err_ct_sim_imba}). Alg. 3 continues to 
see $n^{-1/2}$ decay in parameter estimation error 
(\ref{fig:lambda_est_k_imba}).

```{r clust_err_ct_sim_imba, fig.cap = 'Median and IQR of community detection error. Communities are imbalanced. Simulations were repeated 50 times for each sample size.', cache = FALSE, fig.width = 10}
clustering.df <- readr::read_csv('clustering-k-imbalanced.csv')
ssc.df <- readr::read_csv('clustering-ssc-k-imbalanced.csv')
clustering.df %>%
  dplyr::group_by(n, K) %>%
  dplyr::summarise(
    med.err = median(error),
    first.q = quantile(error, .25),
    third.q = quantile(error, .75),
    med.err.ssc = median(error.ssc),
    first.q.ssc = quantile(error.ssc, .25),
    third.q.ssc = quantile(error.ssc, .75),
    med.err.ep = median(error.ep, na.rm = TRUE),
    first.q.ep = quantile(error.ep, .25, na.rm = TRUE),
    third.q.ep = quantile(error.ep, .75, na.rm = TRUE),
    med.err.louvain = median(error.louvain),
    first.q.louvain = quantile(error.louvain, .25),
    third.q.louvain = quantile(error.louvain, .75)
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::inner_join(
    ssc.df %>% 
      dplyr::group_by(n, K) %>% 
      dplyr::summarise(med.err.ssc2 = median(error.ssc2),
                       first.q.ssc2 = quantile(error.ssc2, .25),
                       third.q.ssc2 = quantile(error.ssc2, .75)) %>% 
      dplyr::ungroup()
  ) %>% 
  ggplot() +
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096)) +
  # scale_x_continuous(breaks = c(128, 256, 512, 1024, 2048, 4096)) + 
  scale_y_log10() +
  labs(y = 'community detection error count', 
       colour = NULL) +
  geom_line(aes(x = n, y = med.err * n,
                colour = 'OSC')) +
  geom_errorbar(aes(x = n, ymin = first.q * n, ymax = third.q * n,
                    colour = 'OSC'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc * n,
                colour = 'SSC-ASE')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc * n, ymax = third.q.ssc * n,
                    colour = 'SSC-ASE'), width = .1) + 
  # geom_line(aes(x = n, y = med.err.ep * n,
  #               colour = 'MM-EP')) + 
  # geom_errorbar(aes(x = n, ymin = first.q.ep * n, ymax = third.q.ep * n,
  #                   colour = 'MM-EP'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc2 * n,
                colour = 'SSC-A')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc2 * n, ymax = third.q.ssc2 * n,
                    colour = 'SSC-A'), width = .1) + 
  geom_line(aes(x = n, y = med.err.louvain * n,
                colour = 'MM-Louvain')) + 
  geom_errorbar(aes(x = n, 
                    ymin = first.q.louvain * n, ymax = third.q.louvain * n,
                    colour = 'MM-Louvain'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  facet_wrap(~ K, labeller = 'label_both')
```

```{r lambda_est_k_imba, cache = FALSE, fig.cap = 'Median and IQR RMSE from Algorithm 3 (red) compared against an MLE-based method (blue). Simulations were repeated 50 times for each sample size. Communities were drawn to be imbalanced.', fig.width = 10}
rmse.df <- readr::read_csv('rmse-k-imbalanced.csv')

rmse.df %>% 
  na.omit() %>% 
  dplyr::group_by(K, n) %>% 
  dplyr::summarise(median.rmse = median(rmse),
                   q1.rmse = quantile(rmse, .25),
                   q3.rmse = quantile(rmse, .75),
                   median.rmse.mle = median(rmse.mle),
                   q1.rmse.mle = quantile(rmse.mle, .25),
                   q3.rmse.mle = quantile(rmse.mle, .75)) %>% 
  dplyr::ungroup() %>% 
  ggplot() +
  geom_line(aes(x = n, y = median.rmse, colour = 'Algorithm 3')) + 
  geom_errorbar(aes(x = n, ymin = q1.rmse, ymax = q3.rmse,
                    colour = 'Algorithm 3'), width = .1) + 
  geom_line(aes(x = n, y = median.rmse.mle, colour = 'MLE-based')) + 
  geom_errorbar(aes(x = n, ymin = q1.rmse.mle, ymax = q3.rmse.mle,
                   colour = 'MLE-based'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  labs(y = 'RMSE', colour = NULL) + 
  facet_wrap(~ K, labeller = 'label_both') + 
  scale_y_log10() + 
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096))
```

\begin{comment}

## Additional experiments

Using the same set of parameters for generating $P$ and $A$ as in the 
balanced communities examples for $K = 2$, we generated one instance of $A$ for
each $n$ and constructed $B$ according to Algorithm 3 to verify that 
as $n \to \infty$, $(\hat{v}_i)^\top \hat{v_j} \stackrel{a.s.}{\to} 0$ for $i, j$ 
in different clusters. Furthermore, the distribution of these inner products 
should be approximately normal. 

```{r, fig.cap = 'Between-cluster inner products of the eigenvectors of $A$ for varying sample sizes.', fig.height = 2, fig.width = 4}
densities.df <- readr::read_csv('densities.csv')

ggplot(densities.df) + 
  geom_density(aes(x = inner.prods * n, colour = factor(n))) + 
  labs(x = expression(n * v[i]^T * v[j]),
       colour = 'n') + 
  scale_x_continuous(breaks = seq(-5, 5)) + 
  scale_colour_brewer(palette = 'Set1')
```

\end{comment}

# Real data examples

```{r}
import::here(cluster.pabm, ssc, plot.A, .from = '~/dev/pabm-grdpg/functions.R')

n.edges <- 20566

butterfly <- R.matlab::readMat('data/Raw_butterfly_network.mat')

A <- butterfly$W.butterfly0
z <- butterfly$labels

keep <- c(6, 2, 9, 4)
A <- A[z %in% keep, z %in% keep]
z <- z[z %in% keep]
n <- length(z)
K <- 4

delta <- quantile(A[upper.tri(A)], 1 - n.edges / (n * (n - 1) / 2))
A <- (A > delta) * 1

z.hat <- cluster.pabm(A, K)

# z.hat.mm <- A %>% 
#   igraph::graph_from_adjacency_matrix(mode = 'undirected') %>% 
#   igraph::cluster_louvain() %>% 
#   igraph::membership()

# ari <- round(fossil::adj.rand.index(z, z.hat) * 100)
```

In the first real data example, we applied OSC to the Leeds Butterfly 
dataset \cite{Wang_2018} consisting of visual similarity measurements among 832 
butterflies across 10 species. The graph was modified to match the example 
from \citeauthor{noroozi2019estimation}: Only the 4 most frequent species were 
considered, and the similarities were discretized to $\{0, 1\}$ via 
thresholding. Fig. \ref{fig:butterfly} shows a sorted adjacency matrix sorted 
by the resultant clustering. 

Comparing against the ground truth species labels, OSC achieves an 
accuracy of 63\% and an adjusted Rand index of 73\%. In comparison, 
\citeauthor{noroozi2019estimation} achieved an adjusted Rand index of 73\% 
using sparse subspace clustering on the same dataset.

```{r butterfly, fig.height = 2, fig.width = 2, fig.cap = 'Adjacency matrix of the Leeds Butterfly dataset after sorting by the clustering outputted by OSC.', cache = FALSE}
plot.A(A, z.hat)
```

```{r}
# this portion is from the code provided by Sengupta and Chen
# the data were generated by scripts from S&C, then saved as RDS files

bmp <- readRDS('data/bmp.rds')
A.bmp <- bmp$A
z.bmp <- bmp$z

pb <- readRDS('data/pb.rds')
A.pb <- pb$A
z.pb <- pb$z

dblp <- readRDS('data/dblp.rds')
A.dblp <- dblp$A
z.dblp <- dblp$z
```

```{r, cache = TRUE}
zhat.bmp <- cluster.pabm(A.bmp, 2, use.all = TRUE, normalize = FALSE)
bmp.acc <- mean(zhat.bmp == z.bmp)
bmp.acc <- ifelse(bmp.acc < .5, 1 - bmp.acc, bmp.acc)

zhat.pb <- cluster.pabm(A.pb, 2, use.all = TRUE, normalize = FALSE, 
                        p = 2, q = 0, d.eigenmap = 2)
pb.acc <- mean(zhat.pb == z.pb)
pb.acc <- ifelse(pb.acc < .5, 1 - pb.acc, pb.acc)

zhat.dblp <- cluster.pabm(A.dblp, 2, use.all = TRUE, normalize = FALSE)
dblp.acc <- mean(zhat.dblp == z.dblp)
dblp.acc <- ifelse(dblp.acc < .5, 1 - dblp.acc, dblp.acc)

zhat.bmp.ssc <- ssc(A.bmp, 2, 1e-3, normalize = TRUE)
bmp.acc.ssc <- mean(zhat.bmp.ssc == z.bmp)
bmp.acc.ssc <- ifelse(bmp.acc.ssc < .5, 1 - bmp.acc.ssc, bmp.acc.ssc)

zhat.pb.ssc <- ssc(A.pb, 2, .008, normalize = TRUE)
pb.acc.ssc <- mean(zhat.pb.ssc == z.pb)
pb.acc.ssc <- ifelse(pb.acc.ssc < .5, 1 - pb.acc.ssc, pb.acc.ssc)

zhat.dblp.ssc <- ssc(A.dblp, 2, 1e-2, normalize = FALSE)
dblp.acc.ssc <- mean(zhat.dblp.ssc == z.dblp)
dblp.acc.ssc <- ifelse(dblp.acc.ssc < .5, 1 - dblp.acc.ssc, dblp.acc.ssc)

```

In the second example, we applied OSC to the British MPs Twitter 
network \cite{greene2013producing}, the Political Blogs network 
\cite{10.1145/1134271.1134277}, and the DBLP network \cite{NIPS2009_3855} 
\cite{10.1007/978-3-642-15880-3_42}. For this data analysis, we subsetted the 
data as described by 
\citeauthor{307cbeb9b1be48299388437423d94bf1} for their analysis of the same 
networks. Our methods underperformed compared to modularity maximization, 
although performance is comparable. In 
addition, OSC's runtime is much lower than that of modularity 
maximization.

```{r}
dplyr::tibble(Network = c('British MPs', 'Political blogs', 'DBLP'),
              # Vertices = c(nrow(A.bmp), nrow(A.pb), nrow(A.dblp)),
              `MM` = c(round(1 / 329, 3),
                       round(61 / 1222, 3),
                       round(62 / 2203, 3)),
              `SSC-ASE` = round(1 - c(bmp.acc.ssc,
                                      pb.acc.ssc,
                                      dblp.acc.ssc), 3),
              `OSC` = round(1 - c(bmp.acc, 
                                  pb.acc,
                                  dblp.acc), 3)) %>% 
  knitr::kable(caption = paste('Community detection error rates', 
                               'for modularity maximization,',
                               'sparse subspace clustering, ',
                               'and OSC.'),
               format = 'latex')
```

```{r mp, cache = FALSE, fig.width = 8, fig.height = 2, fig.cap = 'Adjacency matrices of (from left to right) the British MPs, Political Blogs, and DBLP networks after sorting by the clustering outputted by OSC.', fig.dpi = 10}
gridExtra::grid.arrange(plot.A(A.bmp, zhat.bmp),
                        plot.A(A.pb, zhat.pb),
                        plot.A(A.dblp, zhat.dblp),
                        ncol = 3)
```

In the third example, we consider the Karantaka villages data studied by 
\citet{DVN/U3BIHX_2013}. For this example, we chose the `visitgo` networks from 
villages 12, 31, and 46 at the household level. The label of interest is the 
religious affiliation. The networks were truncated to religions "1" and "2", 
and vertices of degree 0 were removed. 

```{r, cache = FALSE, fig.cap = 'Adjacency matrix of the Karnataka villages data, arranged by the clustering produced by OSC (left). The villages studied here are, from left to right, 12, 31, and 46.', fig.height = 2, fig.width = 8, eval = TRUE}
K <- 2
hh.12 <- readRDS('data/village-hh-12.rds')
hh.31 <- readRDS('data/village-hh-31.rds')
hh.46 <- readRDS('data/village-hh-46.rds')

# this part of code taken from sengupta & chen
setwd('~/dev/pabm-grdpg/codes_and_data')
require(MASS);require("clue")
source("EPalgosim.R") 
source("functions.R")
mm <- function(A.hh) {
  b.can = EPalgo(A.hh,eps=0) # EP algorithm (no perturbation)
  Q.PA.can = rep(NA, ncol(b.can))	# array to store Q values
  for (i in 1:ncol(b.can)){
    #check if any cluster is empty
    foo = rep(NA, 2)
    for (clus in 1:2) {foo[clus]=sum(b.can[,i]==clus)}
    if (min(foo)==0) {stop('Empty groups are not allowed')} 
    Q.PA.can[i] = Q.PA(A.hh, b=b.can[,i])   # fit PABM
  } # end of i for loop
  foo1 = order(-Q.PA.can)[1] 
  return(b.can[,foo1])   # community assignment that maximises Q.PA
}

A.hh.12 <- hh.12$A
z.hh.12 <- hh.12$z
zhat.hh.12 <- cluster.pabm(A.hh.12, K,
                           use.all = TRUE, normalize = FALSE, p = 3, q = 1)
zhat.hh.ssc.12 <- ssc(A.hh.12, K, lambda = .1)
zhat.hh.mm.12 <- mm(A.hh.12)

A.hh.31 <- hh.31$A
z.hh.31 <- hh.31$z
zhat.hh.31 <- cluster.pabm(A.hh.31, K,
                           use.all = TRUE, normalize = FALSE, p = 3, q = 1)
zhat.hh.ssc.31 <- ssc(A.hh.31, K, lambda = .05)
zhat.hh.mm.31 <- mm(A.hh.31)

A.hh.46 <- hh.46$A
z.hh.46 <- hh.46$z
zhat.hh.46 <- cluster.pabm(A.hh.46, K,
                           use.all = TRUE, normalize = FALSE, p = 2, q = 0)
zhat.hh.ssc.46 <- ssc(A.hh.46, K, lambda = .001)
zhat.hh.mm.46 <- mm(A.hh.46)

gridExtra::grid.arrange(plot.A(A.hh.12, zhat.hh.12),
                        plot.A(A.hh.31, zhat.hh.31),
                        plot.A(A.hh.46, zhat.hh.46),
                        nrow = 1)
```

```{r, eval = TRUE}
hh.acc.12 <- mean(zhat.hh.12 == z.hh.12)
hh.acc.12 <- ifelse(hh.acc.12 < .5, 1 - hh.acc.12, hh.acc.12)
hh.acc.ssc.12 <- mean(zhat.hh.ssc.12 == z.hh.12)
hh.acc.ssc.12 <- ifelse(hh.acc.ssc.12 < .5, 1 - hh.acc.ssc.12, hh.acc.ssc.12)
hh.acc.mm.12 <- mean(zhat.hh.mm.12 == z.hh.12)
hh.acc.mm.12 <- ifelse(hh.acc.mm.12 < .5, 1 - hh.acc.mm.12, hh.acc.mm.12)

hh.acc.31 <- mean(zhat.hh.31 == z.hh.31)
hh.acc.31 <- ifelse(hh.acc.31 < .5, 1 - hh.acc.31, hh.acc.31)
hh.acc.ssc.31 <- mean(zhat.hh.ssc.31 == z.hh.31)
hh.acc.ssc.31 <- ifelse(hh.acc.ssc.31 < .5, 1 - hh.acc.ssc.31, hh.acc.ssc.31)
hh.acc.mm.31 <- mean(zhat.hh.mm.31 == z.hh.31)
hh.acc.mm.31 <- ifelse(hh.acc.mm.31 < .5, 1 - hh.acc.mm.31, hh.acc.mm.31)

hh.acc.46 <- mean(zhat.hh.46 == z.hh.46)
hh.acc.46 <- ifelse(hh.acc.46 < .5, 1 - hh.acc.46, hh.acc.46)
hh.acc.ssc.46 <- mean(zhat.hh.ssc.46 == z.hh.46)
hh.acc.ssc.46 <- ifelse(hh.acc.ssc.46 < .5, 1 - hh.acc.ssc.46, hh.acc.ssc.46)
hh.acc.mm.46 <- mean(zhat.hh.mm.46 == z.hh.46)
hh.acc.mm.46 <- ifelse(hh.acc.mm.46 < .5, 1 - hh.acc.mm.46, hh.acc.mm.46)

# knitr::kable(dplyr::tibble(
#   Method = c('OSC', 'SSC-ASE', 'Mod. Max.'),
#   `Village 12` = round(1 - c(hh.acc.12, hh.acc.ssc.12, hh.acc.mm.12), 3),
#   `Village 31` = round(1 - c(hh.acc.31, hh.acc.ssc.31, hh.acc.mm.31), 3),
#   `Village 46` = round(1 - c(hh.acc.46, hh.acc.ssc.46, hh.acc.mm.46), 3)),
#   caption = paste('Community detection error rates for identifying',
#                   'household religion.'))

knitr::kable(dplyr::tibble(
  Network = c('Village 12', 'Village 31', 'Village 46'),
  MM = round(1 - c(hh.acc.mm.12, hh.acc.mm.31, hh.acc.mm.46), 3),
  `SSC-ASE` = round(1 - c(hh.acc.ssc.12, hh.acc.ssc.31, hh.acc.ssc.46), 3),
  `OSC` = round(1 - c(hh.acc.12, hh.acc.31, hh.acc.46), 3)
),
caption = paste('Community detection error rates for identifying',
                  'household religion.'),
format = 'latex')
```

# Discussion

This paper shows the connection between the PABM and the GRDPG, namely that 
a PABM graph can be represented as a union of orthogonal subspaces in an 
embedding under the GRDPG framework. We then exploited this relationship to 
develop community detection and parameter estimation methods. In fact, we can 
represent any graph with Bernoulli edges as a GRDPG, and in the PABM case, 
it turns out that this relationship leads to a very straightforward 
applications of previous work from \citeauthor{rubindelanchy2017statistical}, 
\citeauthor{soltanolkotabi2012}, and \citeauthor{jmlr-v28-wang13}, which lead 
to asymptotically correct solutions with high probability. Similar methods can 
be applied for other models, such as the Nested Block Model
\cite{noroozi2021hierarchy}. 

# Proofs

**Proof of Theorem \ref{theorem1}**. 
This is given by straightforward matrix multiplication. It suffices to show 
that 

$$X U I_{3, 1} U^\top X^\top = 
\begin{bmatrix} 
  \lambda^{(11)} (\lambda^{(11)})^\top & \lambda^{(12)} (\lambda^{(21)})^\top \\
  \lambda^{(21)} (\lambda^{(12)})^\top & \lambda^{(22)} (\lambda^{(22)})^\top
\end{bmatrix}$$

\begin{remark} 
While we can just perform the matrix multiplication to show the 
equivalence, it is more illustrative to look at a few intermediate steps. Note 
that the product of the three inner matrices results in a permutation matrix 
with fixed points at positions $1$ and $4$ and a cycle of order 2 swapping 
positions $2$ and $3$: 

$$U I_{3, 1} U^\top = \begin{bmatrix} 
  1 & 0 & 0 & 0 \\ 
  0 & 0 & 1 & 0 \\ 
  0 & 1 & 0 & 0 \\ 
  0 & 0 & 0 & 1 
\end{bmatrix} = \Pi$$

Since $U$ is orthonormal and $I_{3, 1}$ is diagonal, $\Pi = U I_{3, 1} U^\top$ 
is a spectral decomposition of this permutation matrix. Note that the two fixed 
points result in eigenvalues of $+1$ with corresponding eigenvectors $e_i$ 
where $i = 1, 4$ corresponding to the locations of the fixed points, and the 
cycle of order two results in two eigenvalues $\pm 1$ with corresponding 
eigenvectors $(e_i \pm e_j) / \sqrt{2}$ where $i = 2, j = 3$, pair that is 
swapped.
\end{remark}

\begin{lemma}
\label{lemma1}
Let $P = V D V^\top$ be the spectral decomposition of the edge probability 
matrix for a PABM. Then $V V^\top = X (X^\top X)^{-1} X^\top$ where $X$ is 
defined as in (\ref{eq:xy}).
\end{lemma}

*Proof*. 
By Theorem 2, $P = X U I_{p, q} U^\top X^\top$, where $X$ is defined as in 
(\ref{eq:xy}) and $p$ and $q$ are defined as in equations (\ref{eq:p}) and
(\ref{eq:q}). Alternatively, the spectral decomposition can be written as 
$P = V D V^\top = V |D|^{1/2} I_{p, q} |D|^{1/2} V^\top$ for the same $(p, q)$ 
and $|\cdot|^{1/2}$ is applied entry-wise. Thus for some 
$Q \in \mathbb{O}(p, q)$, 

$$X U Q = V |D|^{1/2}$$

Therefore, using the fact that $U U^\top = I$ and $V^\top V = I$,

$$(V |D|^{1/2}) ((V |D|^{1/2})^\top (V |D|^{1/2}))^{-1} (V |D|^{1/2})^\top = 
(X U Q) ((X U Q)^\top (X U Q))^{-1} (X U Q)^\top$$

The right-hand side becomes

$$\begin{split}
  (X U Q) ((X U Q)^\top (X U Q))^{-1} (X U Q)^\top & 
    = X U Q Q^{-1} U^\top (X^\top X)^{-1} 
    U (Q^\top)^{-1} Q^\top U^\top X^\top \\
  & = X U U^\top (X^\top X)^{-1} U U^\top X^\top \\
  & = X (X^\top X)^{-1} X^\top
\end{split}$$

The left-hand side becomes:

$$\begin{split}
  (V |D|^{1/2}) ((V |D|^{1/2})^\top (V |D|^{1/2}))^{-1} (V |D|^{1/2})^\top & =
    V |D|^{1/2} |D|^{-1/2} (V^\top V)^{-1} |D|^{-1/2} |D|^{1/2} V^\top \\
  & = V V^\top
\end{split}$$

**Proof of Theorem \ref{theorem3}**.
By Lemma \ref{lemma1}, $V V^\top = X (X^\top X)^{-1} X^\top$ where $X$ is 
defined as in (\ref{eq:xy}). Since $X$ is block diagonal with each 
block corresponding to one community, $X (X^\top X)^{-1} X^\top$ is also a 
block diagonal matrix with each block corresponding to a community and zeros
elsewhere. Therefore, if vertices $i$ and $j$ belong to different communities,
then the $ij$^th^ element of $n X (X^\top X)^{-1} X^\top = n V V^\top = B$ 
is 0.

**Proof of Theorem \ref{theorem4}**. 
Let $V_n$ and $\hat{V}_n$ be the eigenvectors of $P$ and $A$ corresponding 
to the $K (K + 1) / 2$ most positive and $K (K - 1) / 2$ most negative
eigenvalues. By \citeauthor{rubindelanchy2017statistical}, for some 
$W \in \mathbb{O}(K^2)$, and $c > 0$,  
$||\hat{V} W - V||_{2 \to \infty} = 
O_P \big(\frac{(\log n)^c}{n \sqrt{\rho_n}} \big)$. 
We furthermore have $||V||_{2 \to \infty} = O_P(n^{-1/2})$. Then if
$(v_n^{(i)})^\top$ and $(\hat{v}_n^{(i)})^\top$ correspond to the rows of 
$V_n$ and $\hat{V}_n$, for $i$ and $j$ in different communities, using the 
fact that $(v_n^{(i)})^\top v_n^{(j)} = 0$, 

$$\begin{split}
\max_{i, j} |(\hat{v}_n^{(i)})^\top \hat{v}_n^{(j)}| &
= \max_{i, j} |(\hat{v}_n^{(i)})^\top \hat{v}_n^{(j)} - 
(v_n^{(i)})^\top v_n^{(j)}| \\
& = \max_{i, j} |(\hat{v}_n^{(i)})^\top W W^\top \hat{v}_n^{(j)} -
(v_n^{(i)})^\top v_n^{(j)}| \\
& = ||\hat{V}_n W^\top W \hat{V}_n - V_n V_n^\top||_{2 \to \infty} \\
& = ||2 \hat{V}_n W V_n^\top - 2 V_n V_n^\top + \hat{V}_n W W^\top V_n^\top - 
2 \hat{V}_n W V_n^\top + V_n V_n^\top||_{2 \to \infty} \\
& = ||2 (\hat{V}_n W - V_n) V_n^\top + 
(\hat{V}_n W - V_n) (\hat{V}_n W - V_n)^\top ||_{2 \to \infty} \\
& \leq 2 ||\hat{V}_n W - V_n||_{2 \to \infty} ||V_n||_{2 \to \infty} +
||\hat{V}_n W - V_n||_{2 \to \infty}^2 \\
& = O_P \Big( \frac{(\log n)^c}{n^{3/2} \rho_n^{1/2}} \Big)
\end{split}$$

Then scaling by $n$, we get 
$|n (\hat{v}_n^{(i)})^\top \hat{v}_n^{(j)}| = 
O_P \big( \frac{(\log n)^c}{\sqrt{n \rho_n}} \big)$.

\begin{definition}[Inradius \cite{soltanolkotabi2012} \cite{jmlr-v28-wang13}]
The inradius of a convex body $\mathcal{P}$, denoted by $r(\mathcal{P})$, is 
defined as the radius of the largest Euclidean ball inscribed in $\mathcal{P}$.
In addition, $r(X)$ for data matrix $X$ with rows $x_i^\top$ represents 
the inradius of the symmetric convex hull of $X$. 
\end{definition}

\begin{lemma}
\label{lemma2}
Let $P_n = V_n \Lambda_n V_n^\top$ be the edge probability matrix of a PABM 
with $n$ vertices. Let $A_n \sim \Bernoulli(P_n)$ be the adjacency matrix 
drawn from $P_n$, and let $A_n = \hat{V}_n \hat{\Lambda}_n \hat{V}_n^\top$ be 
its approximate spectral decomposition using the $K (K+1) / 2$ most positive 
and $K (K-1) / 2$ most negative eigenvalues and their corresponding 
eigenvectors.Without loss of generality, order the rows of $V_n$ and 
$\hat{V}_n$ by community, and let $V_n^{(k)}$ and $\hat{V}_n^{(k)}$ be the 
$n_k \times K^2$ matrix of the rows of the $k^{th}$ community in $V_n$ and 
$\hat{V}_n$ respectively. 
Let $\mu_n^{(k)} = \mu(V_n^{(k)})$ be the subspace incoherence of $V_n^{(k)}$ 
as defined by \citeauthor{soltanolkotabi2012}. Then $\mu_n^{(k)} = 0$ 
with probability 1 $\forall n, k$. 

Furthermore, Let $\hat{\mu}_n^{(k)} = \mu(\hat{V}_n^{(k)})$ using the 
extended definition of $\mu(\cdot)$ from \citeauthor{jmlr-v28-wang13}. Then 

\begin{equation} \label{eq:mu-conv}
\hat{\mu}_n^{(k)} = 0
\end{equation}
\end{lemma}

*Proof*. 
For the first part, 
since $v_n^{(k)}$ is orthogonal to $v_n^{(l)}$ $\forall k \neq l$ 
(Lemma \ref{lemma1}), 
this falls directly out of Theorem 2.8 of \citeauthor{soltanolkotabi2012}. 

For the second part, 
$\mu(\hat{V}_n^{(k)}) = 
\max_{v_n^{(i)} \in V_n^{(-k)}} ||U_n^{(k)} v_n^{(i)}||_\infty$ 
where the rows of $U_n^{(k)}$ consist of vectors lying on the subspace spanned 
by $V_n^{(k)}$. Since 
$u_n^{(i)} \in \text{span}(V_n^{(k)}) \implies u_n^{(i)} \perp v_n^{(j)}$ 
$\forall v_n^{(j)} \in V_n^{(-k)}$ by Theorem \ref{theorem2}, this quantity 
must be 0.

\begin{lemma}
\label{lemma3}
Let $(v_n^{(i)})^\top$ and $(\hat{v}_n^{(i)})^\top$ be the rows of $V_n$ and
$\hat{V}_n$ respectively. By \citeauthor{rubindelanchy2017statistical}, 

\begin{equation} \label{eq:delta-conv}
\delta_n = \max_i ||\hat{v}_n^{(i)} - v_n^{(i)}|| \stackrel{a.s.}{\to} 0
\end{equation}
\end{lemma}

**Proof of Theorem \ref{theorem5}**. 
The basis of this proof is Theorem 6 from \citeauthor{jmlr-v28-wang13}, which 
states that the subspace detection property holds if the noise is small enough 
and the subspace inradius is greater than the subspace incoherence for each 
community $k$.

Let $V_{n, -i}^{(k)}$ be $V_n^{(k)}$ with the $i$^th^ entry removed. Suppose 
that for each community $k$, there are enough vertices such that for each $i$, 
$V_{n, -i}^{(k)}$ spans its corresponding subspace (Theorem 2). 
Then $r_n^{(k)} = \min\limits_i r(V_{n, -i}^{(k)}) > 0$. 
Thus by (\ref{eq:mu-conv}), for each $k$, $r_n^{(k)} > \mu_n^{(k)}$ where 
$\mu_n^{(k)} = \mu(\hat{V}_n^{(k)})$ and $n$ is large enough such that 
$\min\limits_{k, i} rank(V_{n, -i}^{(k)}) = K$.

Let $r_n = \min_k r_n^{(k)}$. By (\ref{eq:delta-conv}), 
$\delta_n \stackrel{a.s.}{\to} 0$. Then as $n\to \infty$, 
$\delta_n < \min_k \frac{r_n (r_n^{(k)} - \mu_n^{(k)})}{2 + 7 r_n^{(k)}}$ 
with probability 1. 

Thus the conditions for the subspace detection property from Theorem 6 from
\citeauthor{jmlr-v28-wang13} are satisfied with probability 1 as $n \to \infty$.

**Proof of Theorem \ref{theorem6}**. 
Let $P$ and $A$ be organized by community such that the elements of blocks 
$P^{(kl)}$ and $A^{(kl)}$ correspond to the edges between communities $k$ and 
$l$. 

*Case $k = l$*. $P^{(kk)}$ and $A^{(kk)}$ represent within-community edge 
probabilities and edges for community $k$.  
By definition, 
$P^{(kk)} = \lambda^{(kk)} (\lambda^{(kk)})^\top$. This implies that the 
signular value decomposition 
$P^{(kk)} = \sigma_{kk}^2 u^{(kk)} (u^{(kk)})^\top$ has one singular value and 
one pair of singular vectors ($P^{(kk)}$ is symmetric, so the left and right 
singular vectors are identical). Then $\lambda^{(kk)} = \sigma_{kk} u^{(kk)}$.  
Let $\hat{U}^{(kk)} \hat{\Sigma}^{(kk)} (\hat{U}^{(kk)})^\top$ be the singular 
value decomposition of $A^{(kk)}$, and let 
$\hat{\sigma}_{kk}^2 \hat{u}^{(kk)} (\hat{u}^{(kk)})^\top$ be its 
one-dimensional approximation. Define 
$\hat{\lambda}^{(kk)} = \hat{\sigma}_{kk} \hat{u}^{(kk)}$. Then 
$\hat{\lambda}^{(kk)}$ is the adjacency spectral embedding approximation of 
$\lambda^{(kk)}$.  
Then by Theorem 5 of \citeauthor{rubindelanchy2017statistical}, the adjacency 
spectral embedding $\hat{\lambda}^{(kk)}$ approximates $\lambda^{(kk)}$ at rate 
$\frac{(\log n_k)^c}{\sqrt{n_k}}$.

*Case $k \neq l$*. $P^{(kl)}$ and $A^{(kl)}$ represent edge probabilities and 
edges between communities $k$ and $l$. Note that $P^{(kl)} = (P^{(lk)})^\top$.  
By definition, $P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top$. As in the 
$k = l$ case, we note that the singular value decomposition 
$P^{(kl)} = \sigma_{kl}^2 u^{(kl)} (v^{(kl)})^\top$ is one-dimensional and 
$\lambda^{(kl)} = \sigma_{kl} u^{(kl)}$. (We can also note that the SVD of 
$P^{(lk)} = \sigma_{kl}^2 v^{(kl)} (u^{(kl)})^\top$, i.e., 
$\sigma_{kl} = \sigma_{lk}$, $u^{(kl)} = v^{(lk)}$, and 
$v^{(kl)} = u^{(lk)}$.)  
Now consider the Hermitian dilation 

$$M^{(kl)} = 2 \begin{bmatrix} 0 & P^{(kl)} \\ P^{(lk)} & 0 \end{bmatrix}$$

which is a symmetric $(n_k + n_l) \times (n_k + n_l)$ matrix. It can be shown 
that the spectral decomposition of $M^{(kl)}$ is

$$M^{(kl)} = 
\begin{bmatrix} u^{(kl)} & -u^{(kl)} \\ v^{(kl)} & v^{(kl)} \end{bmatrix} \times 
\begin{bmatrix} \sigma^2_{kl} & 0 \\ 0 & -\sigma^2_{kl} \end{bmatrix} \times
\begin{bmatrix} u^{(kl)} & -u^{(kl)} \\ v^{(kl)} & v^{(kl)} \end{bmatrix}^\top$$

Thus treating $M^{(kl)}$ as the edge probability matrix of a GRDPG, we have 
latent positions in $\mathbb{R}^2$ given by 

$$\begin{bmatrix} 
  \sigma_{kl} u^{(kl)} & \sigma_{kl} u^{(kl)} \\ 
  \sigma_{kl} v^{(kl)} & -\sigma_{kl} v^{(kl)} 
\end{bmatrix} = 
\begin{bmatrix} 
  \lambda^{(kl)} & \lambda^{(kl)} \\ 
  \lambda^{(lk)} & -\lambda^{(lk)} 
\end{bmatrix}$$

Now consider 

$$\hat{M}^{(kl)} = \begin{bmatrix} 0 & A^{(kl)} \\ A^{(lk)} & 0 \end{bmatrix}$$ 
Then $\hat{M}^{(kl)} = M^{(kl)} + E'$ where 

$$E' = \begin{bmatrix} 0 & E \\ E^\top & 0 \end{bmatrix}$$

and $E$ is the 
$n_k \times n_l$ matrix of independent noise (to generate the Bernoulli entries 
in $A^{(kl)}$.  
Then $\hat{M}^{(kl)}$ is an adjacency matrix drawn from $M^{(kl)}$, so its adjacency 
spectral embedding, given by 

$$\begin{bmatrix} 
  \hat{\lambda}^{(kl)} & \hat{\lambda}^{(kl)} \\ 
  \hat{\lambda}^{(lk)} & -\hat{\lambda}^{(lk)} 
\end{bmatrix}$$

where each $\hat{\lambda}^{(kl)}$ is defined as in Algorithm 3, approximates 
the latent positions of $M^{(kl)}$ up to indefinite orthogonal transformation 
by the rate given in Theorem 5 of \citeauthor{rubindelanchy2017statistical}.  
In this case, the indefinite orthogonal transformation $W_*$ in the GRDPG 
result \cite{rubindelanchy2017statistical} is of the form 
$U^\top \hat{U}$. The eigenvalues of $M$ are distinct since the signature 
for this GRDPG is $(1, 1)$, and $U^\top \hat{U}$ is block diagonal, 
resulting in $W_* \stackrel{a.s.}{\to} I$. Therefore, the adjacency spectral
embedding of $\hat{M}^{(kl)}$ is a direct estimation of the specific latent 
positions outlined for $M^{(kl)}$, up to sign flip. 

# References