---
title: "Connecting the Popularity Adjusted Block Model to the Generalized Random Dot Product Graph for Clustering and Parameter Estimation"
author: John Koo, Minh Tang, Michael Trosset
output:
  pdf_document:
    citation_package: natbib
    number_sections: yes
# output: html_document
fontsize: 12pt
# geometry: "left=1cm,right=1cm,top=1cm,bottom=1.5cm"
urlcolor: blue
header-includes:
- \usepackage{float}
- \usepackage{mathtools}
- \usepackage{natbib}
- \usepackage[linesnumbered,ruled,vlined]{algorithm2e} 
- \setcitestyle{numbers,square}
- \usepackage{verbatim}
bibliography: misc.bib
abstract: |
  In this paper, we connect two probabilistic models for graphs, the Popularity 
  Adjusted Block Model (PABM) and the Generalized Random Dot Product Graph 
  (GRDPG) and use properties established in this connection to aid in 
  clustering and parameter estimation. In particular, we note that the PABM 
  can be represented as latent positions such that points within the same 
  cluster lie on a subspace, and the subspaces that represent each 
  cluster are orthogonal to one another. Using this property as well as the 
  asymptotic properties of Adjacency Spectral Embedding (ASE) of the GRDPG, we 
  are able to establish theoretical asymptotic results of our clustering and 
  parameter estimation methods for the PABM. 
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 3, 
                      fig.width = 5, 
                      fig.dpi = 300)

options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')
```

```{r}
import::from(magrittr, `%>%`)
library(plot.matrix)
library(ggplot2)
library(plot.matrix)

theme_set(theme_bw())
```

\newcommand{\diag}{\text{diag}}
\newcommand{\tr}{\text{Tr}}

# Introduction

Statistical analysis on graphs or networks often involves the partitioning of 
a graph into disconnected subgraphs or clusters. This is often motivated by the 
belief that there exist underlying and unobserved communities to which each 
vertex of the graph belongs, and edges between pairs of vertices are determined 
by drawing from a probability distribution based on the community relationships 
between each pair. The goal of this analysis then is population community 
detection, or the recovery of the true underlying community labels for each 
vertex, up to permutation (with some additional parameter estimation being of 
possible interest), assuming some probabilistic graph model. One such model is 
the Stochastic Block Model (SBM), which assumes that the edge probability from 
one vertex to another follows a Beronulli distribution with fixed probabilities 
for each pair of community labels. The Popularity Adjusted Block Model (PABM) 
was then introduced by \citet*{307cbeb9b1be48299388437423d94bf1} as a 
generalization of the SBM to address the heterogeneity of edge probabilities 
within and between communities while still maintaining the community structure. 

The Random Dot Product Graph (RDPG) model \cite{athreya2017statistical} is 
another graph model with Beronulli edge probabilities. Under this model, each 
vertex of the graph can be represented by a point in a latent Euclidean space 
such that the edge probability between any pair of vertices is given by their 
corresponding dot product in the latent space. The SBM is equivalent to a 
special case of the RDPG model in which all vertices of a given community share 
the same position in the latent space. It has also been shown that similar 
probabilistic graph models, such as the Mixed Membership Stochastic Block 
Model, can be represented in this way \cite{rubindelanchy2017statistical}. An 
analogous property exists for the PABM, not under the RDPG model but under the *Generalized* Random Dot Product Graph (GRDPG) model. This relationship will be 
explored in this paper and exploited to construct algorithms for community 
detection and parameter estimation for the PABM. 

In this paper, we will only consider undirected graphs, that is the edge weight 
from vertex $i$ to vertex $j$ is equal to the edge weight in the opposite 
direction, from vertex $j$ to vertex $i$. Furthermore, we will only consider 
unweighted graphs with binary ($0, 1$) edge weights We will also assume that 
graphs are hollow, i.e., there are no edges from a vertex to itself. 

# Connecting the Popularity Adjusted Block Model to the Generalized Random Dot Product Graph

## The popularity adjusted block model (PABM) \cite{noroozi2019estimation} and the generalized random dot product graph \cite{rubindelanchy2017statistical}

**Definition 1**. 
Let $P \in [0, 1]^{n \times n}$ be a symmetric edge probability matrix for a 
set of $n$ 
vertices, $V$. Each vertex has a community label $1, ..., K$, and the rows and 
columns of $P$ are arranged by community label such that $n_k \times n_l$ block 
$P^{(kl})$ describes the edge probabilities between vertices in communities 
$k$ and $l$. 
Let graph $G = (V, E)$ be an undirected, unweighted graph such 
that its corresponding adjacency matrix $A \in \{0, 1\}^{n \times n}$ is a 
realization of $Bernoulli(P)$, i.e., 
$A_{ij} \stackrel{indep}{\sim} Bernoulli(P_{ij})$ for $i > j$ 
($A_{ij} = A_{ji}$ and $A_{ii} = 0$). 

If each block $P^{(kl)}$ can be written as the outer product of two vectors 
$\lambda^{(kl)} (\lambda^{(lk)})^\top$ for a set of fixed vectors 
$\{\lambda^{(st)}\}_{s, t = 1}^K$ where each $\lambda^{(st)}$ is a column vector 
of dimension $n_s$, then graph $G$ and its corresponding adjacency matrix $A$ 
is a realization of a popularity adjusted block model with parameters 
$\{\lambda^{(st)}\}_{s, t = 1}^K$. 

We will use the notation $A \sim PABM(\{\lambda^{(kl)}\}_K)$ to denote a random 
adjacency matrix $A$ drawn from a PABM with parameters $\lambda^{(kl)}$ 
consisting of $K$ underlying communities.

**Definition 2**.
Let $P \in [0, 1]^{n \times n}$ be a symmetric edge probability matrix for a 
set of $n$ vertices, $V$. If $\exists X \in \mathbb{R}^{n \times d}$ such that 
$P = X I_{pq} X^\top$ for some $d, p, q \in \mathbb{N}$ and $p + q = d$, then 
graph $G = (V, E)$ with adjacency matrix $A$ such that $A_{ij} \stackrel{indep}{\sim} Bernoulli(P_{ij})$ for $i > j$ ($A_{ij} = A_{ji}$ and $A_{ii} = 0$) is a draw from the generalized random dot product graph model with latent positions $X$ and signature $(p, q)$. More precisely, if vertices $i$ and $j$ have latent positions $x_i$ and $x_j$ respectively, then the edge probability between the two is $P_{ij} = x_i^\top I_{pq} x_j$, and $X$ contains the latent positions as rows $x_i^\top$. 

We will use the notation $A \sim GRDPG_{p,q}(X)$ to denote a random adjacency 
matrix $A$ drawn from latent positions $X$ and signature $(p, q)$.

**Definition 3**. The indefinite orthogonal group with signature $(p, q)$ is 
the set $\{Q \in \mathbb{R}^{d \times d} : Q I_{pq} Q^{\top} = I_{pq}\}$, 
denoted as $\mathbb{O}(p, q)$ \cite{rubindelanchy2017statistical}.

**Remark**. 
Like the RDPG, the latent positions of a GRDPG are not unique 
\cite{rubindelanchy2017statistical}. 
More specifically, if $P_{ij} = x_i^\top I_{pq} x_j$, then we also have for any 
$Q \in \mathbb{O}(p, q)$, 
$(Q x_i)^\top I_{pq} (Q x_j) = x_i^\top (Q^\top I_{pq} Q) x_j = 
x_i^\top I_{pq} x_j = P_{ij}$. 
Unlike in the RDPG case, transforming the latent positions by multiplication 
with $Q \in \mathbb{O}(p, q)$ does not necessarily maintain interpoint angles or 
distances. 

## Connecting the PABM to the GRDPG

### Case where $K = 2$

**Theorem 1**. 
Let $X = \begin{bmatrix} 
\lambda^{(11)} & \lambda^{(12)} & 0 & 0 \\ 
0 & 0 & \lambda^{(21)} & \lambda^{(22)} 
\end{bmatrix}$, and let $U = \begin{bmatrix} 1 & 0 & 0 & 0 \\
0 & 0 & 1 / \sqrt{2} & 1 / \sqrt{2} \\
0 & 0 & 1 / \sqrt{2} & - 1 / \sqrt{2} \\
0 & 1 & 0 & 0 \end{bmatrix}$, as in Definition 1. 
Then $A \sim GRDPG_{3, 1}(X U)$ and $A \sim PABM(\{(\lambda^{(kl)}\}_K)$ are 
equivalent.

*Proof*.
Let $X = \begin{bmatrix} 
\lambda^{(11)} & \lambda^{(12)} & 0 & 0 \\ 
0 & 0 & \lambda^{(21)} & \lambda^{(22)} 
\end{bmatrix}$ and 
$Y = \begin{bmatrix} \lambda^{(11)} & 0 & \lambda^{(12)} & 0 \\
0 & \lambda^{(21)} & 0 & \lambda^{(22)} \end{bmatrix}$. 
Then $P = X Y^\top$. 

We can note that $Y = X \Pi$ where $\Pi$ is the permutation matrix 
$\Pi = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 
0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}$. 
Therefore, $P = X \Pi X^\top$. 

Taking the spectral decomposition of $\Pi = U D U^\top$, we can see that 
$P = (X U) D (X U)^\top$. We can then denote $\Sigma = |D|^{1/2}$, the 
square root of the absolute values of the (diagonal) entries of $D$ and 
obtain $P = (X U \Sigma) I_{pq} (X U \Sigma)^\top$ where $p$ and $q$ correspond 
to the number of positive and negative eigenvalues of $\Pi$, respectively. 
Therefore, the PABM with $K = 2$ is a special case of the GRDPG. We can however 
expand upon this a bit further.

The permutation described by $\Pi$ has two fixed points and one cycle of order 
2. The two fixed points are at positions $1$ and $4$, so $\Pi$ has two 
eigenvalues equal to $1$ and corresponding eigenvectors $e_1$ and $e_4$. The 
cycle of order 2 switching positions $2$ and $3$ corresponds to eigenvalues 
$1$ and $-1$ with corresponding eigenvalues $(e_2 + e_3) / \sqrt{2}$ and 
$(e_2 - e_3) / \sqrt{2}$ respectively. Therefore, 
$D = \begin{bmatrix} 
1 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & -1
\end{bmatrix} = I_{3, 1}$ and 
$U = \begin{bmatrix} 1 & 0 & 0 & 0 \\
0 & 0 & 1 / \sqrt{2} & 1 / \sqrt{2} \\
0 & 0 & 1 / \sqrt{2} & - 1 / \sqrt{2} \\
0 & 1 & 0 & 0 \end{bmatrix}$.

Putting it all together, we get $P = (X U) I_{3, 1} (X U)^\top$. Therefore, 
the PABM with $K = 2$ is a GRDPG with $p = 3$, $q = 1$, $d = K^2 = 4$, and 
latent positions 
$X U = \begin{bmatrix} 
\lambda^{(11)} & 0 & \lambda^{(12)} / \sqrt{2} & \lambda^{(12)} / \sqrt{2} \\
0 & \lambda^{(22)} & \lambda^{(21)} / \sqrt{2} & -\lambda^{(21)} / \sqrt{2}
\end{bmatrix}$.

### Generalization to $K > 2$

**Theorem 2**. There exists a block diagonal matrix 
$X \in \mathbb{R}^{n \times K^2}$ defined by PABM parameters 
$\{\lambda^{(kl)}\}_K$ and $U \in \mathbb{R}^{K^2 \times K^2}$ that is fixed 
for each $K$ such that $A \sim GRDPG_{K (K+1) / 2, K (K-1) / 2}(XU)$ and 
$A \sim PABM(\{(\lambda^{(kl)}\})_K)$ are equivalent.

*Proof*. 
Let $\Lambda^{(k)} = 
\begin{bmatrix} \lambda^{(k,1)} & \cdots & \lambda^{(k, K)} \end{bmatrix}
\in \mathbb{R}^{n_k \times K}$.  
Let $X$ be a block diagonal matrix 
$X = \diag(\Lambda^{(1)}, ..., \Lambda^{(K)}) \in \mathbb{R}^{n \times K^2}$.  
Let $L^{(k)}$ be a block diagonal matrix of column vectors $\lambda^{(lk)}$ for 
$l = 1, ..., K$. $L^{(k)} = \diag(\lambda^{(1k)}, ..., \lambda^{(Kk)}) \in 
\mathbb{R}^{n \times K}$.  
Let $Y = \begin{bmatrix} L^{(1)} & \cdots & L^{(K)} \end{bmatrix} \in 
\mathbb{R}^{n \times K^2}$.  
Then $P = X Y^\top$.  
Similar to the $K = 2$ case, we again have $Y = X \Pi$ for a permutation matrix
$\Pi$, so $P = X \Pi X^\top$.  
The permutation described by $\Pi$ has $K$ fixed points, which correspond to 
$K$ eigenvalues equal to $1$ with corresponding eigenvectors $e_k$ where 
$k = r (K + 1) + 1$ for $r = 0, ..., K - 1$. It also has 
$\binom{K}{2} = K (K - 1) / 2$ cycles of order $2$. Each cycle corresponds to 
a pair of eigenvalues $+1$ and $-1$ and a pair of eigenvectors 
$(e_s + e_t) / \sqrt{2}$ and $(e_s - e_t) / \sqrt{2}$.

So $\Pi$ has $K (K + 1) / 2$ eigenvalues equal to $1$ and $K (K - 1) / 2$ 
eigenvalues equal to $-1$. $\Pi$ has the decomposed form 
$\Pi = U I_{K (K + 1) / 2, K (K - 1) / 2} U^\top$, and we can describe the 
PABM with $K$ communities as a GRDPG with latent positions $X U$ with signature 
$\Big( K (K + 1) / 2, K (K - 1) / 2 \Big)$.

**Example** for $K = 3$. Using the same notation as before:

$X = \begin{bmatrix} 
\lambda^{(11)} & \lambda^{(12)} & \lambda^{(13)} & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & \lambda^{(21)} & \lambda^{(22)} & \lambda^{(23)} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & \lambda^{(31)} & \lambda^{(32)} & \lambda^{(33)}
\end{bmatrix}$

$Y = \begin{bmatrix} 
\lambda^{(11)} & 0 & 0 & \lambda^{(12)} & 0 & 0 & \lambda^{(13)} & 0 & 0 \\
0 & \lambda^{(21)} & 0 & 0 & \lambda^{(22)} & 0 & 0 & \lambda^{(23)} & 0 \\
0 & 0 & \lambda^{(31)} & 0 & 0 & \lambda^{(32)} & 0 & 0 & \lambda^{(33)}
\end{bmatrix}$

Then $P = X Y^\top$ and $Y = X \Pi$ where 
$\Pi = \begin{bmatrix} 
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1
\end{bmatrix}$

Another way to look at this is:

* Positions 1, 5, 9 are fixed.
* The cycles of order 2 are $(2, 4)$, $(3, 7)$, and $(6, 8)$.
    
Therefore, we can decompose $\Pi = U I_{6, 3} U^\top$ where the first three 
columns of $U$ consist of $e_1$, $e_5$, and $e_9$ corresponding to the fixed 
positions $1$, $5$, and $9$, the next three columns consist of eigenvectors 
$(e_k + e_l) / \sqrt{2}$, and the last three columns consist of eigenvectors 
$(e_k - e_l) / \sqrt{2}$, where pairs $(k, l)$ correspond to the cycles of 
order 2 described above.

The latent positions are the rows of  
$XU = \begin{bmatrix}
\lambda^{(11)} & 0 & 0 & \lambda^{(12)} / \sqrt{2} & \lambda^{(13)} / \sqrt{2} & 0 & \lambda^{(12)} / \sqrt{2} & \lambda^{(13)} / \sqrt{2} & 0 \\
0 & \lambda^{(22)} & 0 & \lambda^{(21)} / \sqrt{2} & 0 & \lambda^{(23)} / \sqrt{2} & -\lambda^{(21)} / \sqrt{2} & 0 & \lambda^{(23)} / \sqrt{2} \\
0 & 0 & \lambda^{(33)} & 0 & \lambda^{(31)} / \sqrt{2} & \lambda^{(32)} / \sqrt{2} & 0 & -\lambda^{(31)} / \sqrt{2} & -\lambda^{(32)} / \sqrt{2}
\end{bmatrix}$.

# Methods

Two inference objectives arise from the PABM:

1. Community membership identification (up to permutation).
2. Parameter estimation (estimating $\lambda^{(kl)}$'s).

Here, we will focus more on (1) and pose possible methods for (2). In our 
methods, we assume that $K$, the number of communities, is known beforehand and 
does not require estimation.

## Previous work

\citet*{noroozi2019estimation} proposed using sparse subspace clustering (SSC) 
to identify the community memberships given either an edge probability matrix $P$ 
or an adjacency matrix $A$. In the case that $P$ is known, the community 
memberships can be identified exactly (up to permutation). A similar procedure 
can be applied if $P$ is unknown and we have an observation $A$, but the 
theoretical guarantees of this method applied to the PABM are unknown. In 
particular, the method requires spherical Gaussian noise. The authors of this 
paper then use point estimators for $\{\lambda^{(kl)}\}$ using cluster labels 
obtained via SSC. 

## Community detection

### Using edge probability matrix $P$

We previously stated one possible set of latent positions that result in the 
edge probability matrix of a PABM, $P = (XU) I_{pq} (XU)^\top$. If we have 
(or can estimate) $XU$ directly, then both the community detection and parameter 
identification problem are trivial since $U$ is orthonormal and fixed for each 
value of $K$. However, direct identification or estimation of $XU$ is not 
possible \cite{rubindelanchy2017statistical}. 

If we decompose $P = Z I_{pq} Z^\top$, then
$\exists Q \in \mathbb{O}(p, q)$ such that $XU = Z Q$. Even if we start with 
the exact edge probability matrix, we cannot recover the "original" latent 
positions $XU$. Note that unlike in the case of the RDPG, 
$Q$ is not an orthogonal matrix. If $z_i$'s are the rows of $XU$, then 
$||z_i - z_j||^2 \neq ||Q z_i - Q z_j||^2$, and 
$\langle z_i, z_j \rangle \neq \langle Q z_i, Q z_j \rangle$. This prevents 
us from using the properties of $XU$ directly. In particular, if 
$Q \in \mathbb{O}(n)$, then we could use the fact that 
$\langle z_i, z_j \rangle = \langle Q z_i, Q z_j \rangle = 0$ if vertices $i$ 
and $j$ are in different communities. 

We can note from the explicit form of $XU$ that it represents points in 
$\mathbb{R}^{K^2}$ such that points within each community lie on $K$-dimensional 
subspaces. Furthermore, the subspaces are orthogonal to each other. 
Multiplication by $Q \in \mathbb{O}(p, q)$ removes the orthogonality property 
but retains the property that each community is represented by a $K$-dimensional 
subspace. Using this property, previous work proposes the use of subspace 
clustering while acknowledging some of its shortcomings 
\cite{noroozi2019estimation} \cite{soltanolkotabi2014}. 

**Theorem 3.** 
Let $P = V D V^\top$ be the spectral decomposition of the edge probability
matrix. Let $B = V V^\top$. Then $B_{ij} = 0$ if vertices $i$ and $j$ are 
of different communities.

*Proof (sketch)*
By projection, $V V^\top = X (X^\top X)^{-1} X^\top$ where $X$ is defined as 
in Theorem 2. Since $X$ is block diagonal with each block corresponding 
to one community, $X (X^\top X)^{-1} X^\top$ is also a block diagonal matrix 
with each block corresponding to a community and zeros elsewhere. Therefore, 
if vertices $i$ and $j$ belong to different communities, then the $ij$^th^ 
element of $X (X^\top X)^{-1} X^\top = V V^\top = B$ is 0.

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwData{Edge probability matrix $P$, number of communities $K$}
  \KwResult{Community labels $1, ..., K$}
    Compute the spectral decomposition $P = V D V^\top$.\;
    Compute the inner product matrix $B = |V V^\top|$, applying $|\cdot|$ 
    entry-wise.\;
    Construct graph $G$ using $B$ as edge similarities.\;
    Identify the connected components of $G$ and map each to community labels 
    $1, ..., K.$\;
  \caption{PABM clustering on the edge probability matrix.}
\end{algorithm}

### Using adjacency matrix $A$

The adjacency embedding of $A$ approaches latent positions that form $P$ as 
the number of vertices $n$ increases. More 
precisely, let $\{\lambda^{(kl)}\}_K \sim \mathcal{F}_K$ for some joint 
distribution consisting of $K$ underlying communities $\mathcal{F}_K$. Then the 
latent positions $XU \sim \mathcal{G}_K$ for some related joint distribution 
with $K$ underlying communities $\mathcal{G}_K$. Denote $Z_n$ as a sample of size 
$n$ from $\mathcal{G}_K$ and adjacency matrix $A_n$ as one draw from edge 
probability matrix $P_n = Z_n I_{pq} Z_n^\top$. Let $\hat{Z}_n$ be the 
adjacency embedding of $A_n$ with rows $(\hat{z}_i^{(n)})^\top$. Then by
\citet*{rubindelanchy2017statistical}, 

$$\max\limits_{i \in \{1, ..., n\}} ||Q_n \hat{z}_i^{(n)} - z_i^{(n)}|| = O_P\bigg(\frac{(\log n)^c}{n^{1/2}} \bigg)$$

for some $c>0$ and sequence of $Q_n \in \mathbb{O}(p, q)$. In addition, 
\citeauthor{rubindelanchy2017statistical} produce a central limit theorem 
result.

**Theorem 4**. Let $\hat{V}^{(n)} \in \mathbb{R}^{n \times K^2}$ be the matrix 
of $K^2$ eigenvectors of $A_n$ corresponding to the $K (K+1) / 2$ most positive 
eigenvalues and $K (K-1) / 2$ most negative eigenvalues with rows
$(\hat{v}_i^{(n)})^\top$. Let $(i, j)$ correspond to pairs belonging to 
different communities. Then for some $c > 0$, 

$$\max\limits_{i, j} ||(\hat{v}^{(n)}_i)^\top \hat{v}_j^{(n)}|| = O_P\bigg( \frac{(\log n)^c}{n \sqrt{\rho_n}}\bigg)$$
where $\lim\limits_{n \to 0} \rho_n = 0$ is a sparsity parameter introduced 
by \citet{307cbeb9b1be48299388437423d94bf1}. More precisely, 
$\rho_n = o(\log n / \sqrt{n})$.

In addition, by the central limit theorem, 
$(\hat{v}^{(n)}_i)^\top \hat{v}_j^{(n)}$ converge to a normal distribution 
centered at $0$. 

This leads to the following algorithm: 

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwData{Adjacency matrix $A$, number of communities $K$}
  \KwResult{Community assignments $1, ..., K$}
    Compute the eigenvectors of $A$ that correspond to the $K (K+1) / 2$ most 
    positive eigenvalues and $K (K-1) / 2$ most negative eigenvalues. Construct 
    $V$ using these eigenvectors as its columns.\;
    Compute $B = |V V^\top|$, applying $|\cdot|$ entry-wise.\;
    Construct graph $G$ using $B$ as its similarity matrix.\;
    Partition $G$ into $K$ disconnected components 
    (e.g., using edge thresholding or spectral clustering).\;
    Map each partition to the community labels $1, ..., K$.\;
  \caption{PABM clustering on the adjacency matrix.}
\end{algorithm}

Theorem 4 implies that as $n \to \infty$, the number (not just proportion) of 
misclassified vertices, up to permutation, outputted by algorithm 2 goes to 0. 

## Parameter estimation

For any $P$ edge probability matrix for the PABM such that the rows and 
columns are organized by community, the $kl$^th^ block is an outer product 
of two vectors, i.e., $P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top$. 
Therefore, given $P^{(kl)}$, $\lambda%{(kl)}$ and $\lambda^{(lk)}$ are 
solvable exactly (up to multiplication by $-1$) using singular value 
decomposition. More specifically, let $P = \sigma^2 u v^\top$ be the 
singular value decomposition of $P$. $u \in \mathbb{R}^{n_k}$ and 
$v \in \mathbb{R}^{n_l}$ are vectors and 
$\sigma^2 > 0$ is a scalar. Then $\lambda^{(kl)} = \pm \sigma u$ and 
$\lambda^{(lk)} = \pm \sigma v$.

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetAlgoLined
  \caption{PABM parameter estimation using the edge probability matrix.}
  \KwData{Edge probability matrix $P$, community assignments $1, ..., K$.}
  \KwResult{PABM parameters $\{\lambda^{(kl)}\}_K$}
  Arrange the rows and columns of $P$ by community such that each $P^{(kl)}$ 
  block consists of edge probabilities between communities $k$ and $l$.\; 
  \For{$k, l = 1, ..., K$, $k \geq l$} {
    Compute $P^{(kl)} = (\sigma^{(kl)})^2 u^{(kl)} (v^{(kl)})^\top$, the SVD of 
    the $kl^{th}$ block.\;
    Assign $\lambda^{(kl)} \leftarrow \pm \sigma^{(kl)} u^{(kl)}$ and 
    $\lambda^{(lk)} \leftarrow \pm \sigma^{(kl)} v^{(kl)}$.
  }
\end{algorithm}

A similar method can be applied using $\hat{Z}$, the adjacency spectral 
embedding of $A$.

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetAlgoLined
  \caption{PABM parameter estimation using the adjacency matrix.}
  \KwData{Adjacency matrix $A$, community assignments $1, ..., K$}
  \KwResult{PABM parameter estimates $\{\hat{\lambda}^{(kl)}\}_K$.}
  Arrange the rows and columns of $A$ by community such that each 
  $A^{(kl)}$ block consists of estimated edge probabilities between 
  communities $k$ and $l$.\;
  \For{$k, l = 1, ..., K$, $k \leq l$} {
    Compute $A^{(kl)} = U \Sigma V^\top$, the SVD of the $kl^{th}$ 
    block.\;
    Assign $u^{(kl)}$ and $v^{(kl)}$ as the first columns of $U$ and $V$. 
    Assign $\sigma^2 \leftarrow \Sigma_{11}$.\;
    Assign $\hat{\lambda}^{(kl)} \leftarrow \pm \sigma^{(kl)} u^{(kl)}$ and 
    $\hat{\lambda}^{(lk)} \leftarrow \pm \sigma^{(kl)} v^{(kl)}$.
  }
\end{algorithm}

**Theorem 5**. Under regularity and sparsity assumptions, and under the further 
assumption that $K$ is fixed and community labels are known, 

$$\max_{k, l \in \{1, ..., K\}} 
||\hat{\lambda}^{(kl)} - \lambda^{(kl)}|| = 
O_P \bigg(\frac{(\log n_k)^c}{\sqrt{n_k}} \bigg)$$

*Proof (sketch)*. 
Let $P$ and $A$ be organized by community such that the elements of blocks 
$P^{(kl)}$ and $A^{(kl)}$ correspond to the edges between communities $k$ and 
$l$. 

* *Case $k = l$*. $P^{(kk)}$ and $A^{(kk)}$ represent within-community edge 
probabilities and edges for community $k$.  
By definition, 
$P^{(kk)} = \lambda^{(kk)} (\lambda^{(kk)})^\top$. This implies that the 
signular value decomposition 
$P^{(kk)} = \sigma_{kk}^2 u^{(kk)} (u^{(kk)})^\top$ has one singular value and 
one pair of singular vectors ($P^{(kk)}$ is symmetric, so the left and right 
singular vectors are identical). Then $\lambda^{(kk)} = \sigma_{kk} u^{(kk)}$.  
Let $\hat{U}^{(kk)} \hat{\Sigma}^{(kk)} (\hat{U}^{(kk)})^\top$ be the singular 
value decomposition of $A^{(kk)}$, and let 
$\hat{\sigma}_{kk}^2 \hat{u}^{(kk)} (\hat{u}^{(kk)})^\top$ be its 
one-dimensional approximation. Define 
$\hat{\lambda}^{(kk)} = \hat{\sigma}_{kk} \hat{u}^{(kk)}$. Then 
$\hat{\lambda}^{(kk)}$ is the adjacency spectral embedding approximation of 
$\lambda^{(kk)}$.  
Then by Theorem 5 from \citet{rubindelanchy2017statistical}, the adjacency 
spectral embedding $\hat{\lambda}^{(kk)}$ approximates $\lambda^{(kk)}$ at rate 
$\frac{(\log n_k)^c}{\sqrt{n_k}}$.

* *Case $k \neq l$*. $P^{(kl)}$ and $A^{(kl)}$ represent edge probabilities and 
edges between communities $k$ and $l$.  
Let $R^{(kl)} = P^{(kl)} P^{(lk)}$. By definition, 
$P^{(kl)} P^{(lk)} = 
\lambda^{(kl)} (\lambda^{(lk)})^\top \lambda^{(lk)} (\lambda^{(kl)})^\top$. 
Noting that the two middle terms compose a scalar, we can rewrite 
$R^{(kk)} = c \lambda^{(kl)} (\lambda^{(kl)})^\top$, where $c = \tr(P^{(lk)})$. 
As in the $k = l$ case, we note that the singular value decomposition 
$R^{(kl)} = \sigma_{kl}^2 u^{(kl)} (u^{(kl)})^\top$ is one-dimensional and 
$\lambda^{(kl)} = \sigma_{kl} u^{(kl)} / c$.  
Let $B^{(kl)} = A^{(kl)} A^{(lk)}$, and let 
$\hat{U}^{(kl)} \hat{\Sigma}^{(kl)} (\hat{U}^{(kl)})^\top$ be the singular 
value decomposition of $B^{(kl)}$. Let 
$\hat{\sigma}_{kl}^2 \hat{u}^{kl} (\hat{u}^{(kl)})^\top$ be the one-dimensional 
approximation of $B^{(kl)}$ based on the singular value decomposition. Define 
$\hat{\lambda}^{(kl)} = \hat{\sigma}_{kl} \hat{u}^{(kl)} / \tr(A^{(lk)})$. Then 
$\hat{\lambda}^{(kl)}$ is the adjacency spectral embedding approximation of 
$\lambda^{(kl)}$.  
Then by Theorem 5 from \citet{rubindelanchy2017statistical}, the adjacency 
spectral embedding $\hat{\lambda}^{(kl)}$ approximates $\lambda^{(kl)}$ at rate 
$\frac{(\log n_k)^c}{\sqrt{n_k}}$.

# Simulated Examples

## Case $K = 2$

For these examples, we set the following parameters:

* Number of underlying communities $K = 2$
* Mixture parameter $\alpha = .5$
* Within-group popularities $\lambda^{(kk)} \stackrel{iid}{\sim} Beta(2, 1)$
* Between-group popularities 
$\lambda^{(kl)} \stackrel{iid}{\sim} Beta(1, 2)$ for $k \neq l$

### Community detection

In this part, we will assess Algorithm 2's performance for sample sizes 
$n = 64, 128, 256, 512, 1024, 2048$. For each sample size $n$, $100$ sets of 
$\lambda^{(kl)}$'s are drawn and for each set of parameters, one adjacency 
matrix $A$ is drawn and clustered. We will not consider community detection using the 
edge probability matrix $P$ since this will always result exact recovery of the
original communities (up to permutation).

```{r clust_err_sim, fig.cap = 'IQR clustering error using Algorithm 2 (red) compared against subspace clustering (blue) for sample sizes from 64 to 2048. Simulations were repeated 100 times for each sample size. The $y$-axis is shown on a logarithmic scale.', cache = FALSE}
clustering.df <- readr::read_csv('clustering.csv')
clustering.df %>%
  dplyr::group_by(n) %>%
  dplyr::summarise(med.err = median(error),
                   first.q = quantile(error, .25),
                   third.q = quantile(error, .75),
                   med.err.ssc = median(error.ssc),
                   first.q.ssc = quantile(error.ssc, .25),
                   third.q.ssc = quantile(error.ssc, .75)) %>%
  dplyr::ungroup() %>% 
  ggplot() +
  scale_y_log10() +
  # scale_x_log10() + 
  labs(y = 'misclustering proportion', 
       colour = NULL) +
  geom_line(aes(x = n, y = med.err,
                colour = 'Algorithm 2')) +
  geom_errorbar(aes(x = n, ymin = first.q, ymax = third.q,
                    colour = 'Algorithm 2')) + 
  geom_line(aes(x = n, y = med.err.ssc,
                colour = 'SSC')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc, ymax = third.q.ssc,
                    colour = 'SSC')) + 
  scale_colour_brewer(palette = 'Set1')
```

Theorem 4 implies that algorithm 2 will result in not just in the error rate 
converging to 0 but the error *count* as well (Fig. \ref{fig:clust_err_ct_sim}).

```{r clust_err_ct_sim, fig.cap = 'IQR of counts of misclustered vertices using Algorithm 2 (red) compared against subspace clustering (blue) for sample sizes from 64 to 2048. Simulations were repeated 100 times for each sample size. The $y$-axis is shown on a logarithmic scale.', cache = FALSE}
clustering.df <- readr::read_csv('clustering.csv')
clustering.df %>%
  dplyr::group_by(n) %>%
  dplyr::summarise(med.err = median(error),
                   first.q = quantile(error, .25),
                   third.q = quantile(error, .75),
                   med.err.ssc = median(error.ssc),
                   first.q.ssc = quantile(error.ssc, .25),
                   third.q.ssc = quantile(error.ssc, .75)) %>%
  dplyr::ungroup() %>% 
  ggplot() +
  scale_y_log10() +
  # scale_x_log10() + 
  labs(y = 'misclustering count', 
       colour = NULL) +
  geom_line(aes(x = n, y = med.err * n,
                colour = 'Algorithm 2')) +
  geom_errorbar(aes(x = n, ymin = first.q * n, ymax = third.q * n,
                    colour = 'Algorithm 2')) + 
  geom_line(aes(x = n, y = med.err.ssc * n,
                colour = 'SSC')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc * n, ymax = third.q.ssc * n,
                    colour = 'SSC')) + 
  scale_colour_brewer(palette = 'Set1')
```

We can also examine how the distribution of $(\hat{v}_i)^\top \hat{v_j}$ varies 
with $n$: 

```{r, fig.cap = 'Between-cluster inner products of the eigenvectors of $A$ for varying sample sizes.', cache = FALSE}
densities.df <- readr::read_csv('densities.csv')

ggplot(densities.df) + 
  geom_density(aes(x = inner.prods, colour = factor(n))) + 
  labs(x = 'between cluster inner products',
       colour = 'n') + 
  scale_colour_brewer(palette = 'Set1')
```

### Parameter estimation

Figure \ref{fig:lambda_est} shows the medians and interquartile ranges of root 
mean square errors for Algorithm 4 over 100 simulations using the same 
parameters as before. Comparison against an MLE-based method 
\cite{noroozi2019estimation} suggests similar performance, close to the rate 
$n^{-1/2}$.

```{r lambda_est, cache = FALSE, fig.cap = 'Log-log plot of the median and IQR RMSE from Algorithm 4 (red) compared against an MLE-based method (blue) using sample sizes from 64 to 2048. Simulations were repeated 100 times for each sample size.'}
rmse.df <- readr::read_csv('rmse.csv')

rmse.df %>% 
  dplyr::group_by(n) %>% 
  dplyr::summarise(median.rmse = median(rmse),
                   q1.rmse = quantile(rmse, .25),
                   q3.rmse = quantile(rmse, .75),
                   median.rmse.mle = median(rmse.mle),
                   q1.rmse.mle = quantile(rmse.mle, .25),
                   q3.rmse.mle = quantile(rmse.mle, .75)) %>% 
  dplyr::ungroup() %>% 
  ggplot() + 
  scale_y_log10() + 
  scale_x_log10() +
  geom_line(aes(x = n, y = median.rmse, colour = 'Algorithm 4')) + 
  geom_errorbar(aes(x = n, ymin = q1.rmse, ymax = q3.rmse,
                    colour = 'Algorithm 4')) + 
  geom_line(aes(x = n, y = median.rmse.mle, colour = 'MLE-based')) + 
  geom_errorbar(aes(x = n, ymin = q1.rmse.mle, ymax = q3.rmse.mle,
                   colour = 'MLE-based')) + 
  scale_colour_brewer(palette = 'Set1') + 
  labs(y = 'RMSE', colour = NULL)
```

## Higher values of $K$

For these examples, we set the following parameters:

* Number of underlying communities $K = 2, 4, 8$
* Mixture parameters $\alpha_k = 1 / K$ for $k = 1, ..., K$
* Within-group popularities $\lambda^{(kk)} \stackrel{iid}{\sim} Beta(2, 1)$
* Between-group popularities 
$\lambda^{(kl)} \stackrel{iid}{\sim} Beta(1, 2)$ for $k \neq l$

### Community detection

```{r clust_err_k, fig.cap = 'IQR of clustering error using Algorithm 2 (red) compared against subspace clustering (blue) for sample sizes from 64 to 2048 and number of clusters from 2 to 8. Simulations were repeated 100 times for each sample size. The $y$-axis is shown on a logarithmic scale.', fig.width = 8}
clustering.df <- readr::read_csv('clustering-k.csv')
clustering.df %>%
  dplyr::group_by(K, n) %>%
  dplyr::summarise(med.err = median(error),
                   first.q = quantile(error, .25),
                   third.q = quantile(error, .75),
                   med.err.ssc = median(error.ssc),
                   first.q.ssc = quantile(error.ssc, .25),
                   third.q.ssc = quantile(error.ssc, .75)) %>%
  dplyr::ungroup() %>% 
  ggplot() +
  scale_y_log10() +
  # scale_x_log10() + 
  labs(y = '1 - adjusted rand index', 
       colour = NULL) +
  geom_line(aes(x = n, y = med.err,
                colour = 'Algorithm 2')) +
  geom_errorbar(aes(x = n, ymin = first.q, ymax = third.q,
                    colour = 'Algorithm 2')) + 
  geom_line(aes(x = n, y = med.err.ssc,
                colour = 'SSC')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc, ymax = third.q.ssc,
                    colour = 'SSC')) + 
  scale_colour_brewer(palette = 'Set1') + 
  facet_wrap(~ K, labeller = 'label_both')
```

### Parameter estimation

```{r lambda_est_k, cache = FALSE, fig.cap = 'Log-log plot of the median and IQR RMSE from Algorithm 4 (red) compared against an MLE-based method (blue) using sample sizes from 64 to 2048 and number of clusters from 2 to 8. Simulations were repeated 100 times for each sample size.', fig.width = 8}
rmse.df <- readr::read_csv('rmse.csv') %>% 
  dplyr::transmute(K = 2,
                   n = n,
                   rmse = rmse,
                   rmse.mle = rmse.mle) %>% 
  dplyr::bind_rows(readr::read_csv('rmse-k.csv'))

rmse.df %>% 
  na.omit() %>% 
  dplyr::group_by(K, n) %>% 
  dplyr::summarise(median.rmse = median(rmse),
                   q1.rmse = quantile(rmse, .25),
                   q3.rmse = quantile(rmse, .75),
                   median.rmse.mle = median(rmse.mle),
                   q1.rmse.mle = quantile(rmse.mle, .25),
                   q3.rmse.mle = quantile(rmse.mle, .75)) %>% 
  dplyr::ungroup() %>% 
  ggplot() + 
  scale_y_log10() + 
  scale_x_log10() +
  geom_line(aes(x = n, y = median.rmse, colour = 'Algorithm 4')) + 
  geom_errorbar(aes(x = n, ymin = q1.rmse, ymax = q3.rmse,
                    colour = 'Algorithm 4')) + 
  geom_line(aes(x = n, y = median.rmse.mle, colour = 'MLE-based')) + 
  geom_errorbar(aes(x = n, ymin = q1.rmse.mle, ymax = q3.rmse.mle,
                   colour = 'MLE-based')) + 
  scale_colour_brewer(palette = 'Set1') + 
  labs(y = 'RMSE', colour = NULL) + 
  facet_wrap(~ K, labeller = 'label_both')
```

# Real data examples

```{r}
library(mclust)
import::here(cluster.pabm, .from = '~/dev/pabm-grdpg/functions.R')

n.edges <- 20566

butterfly <- R.matlab::readMat('~/Downloads/Raw_butterfly_network.mat')

A <- butterfly$W.butterfly0
z <- butterfly$labels

keep <- c(6, 2, 9, 4)
A <- A[z %in% keep, z %in% keep]
z <- z[z %in% keep]
n <- length(z)
K <- 4

delta <- quantile(A[upper.tri(A)], 1 - n.edges / (n * (n - 1) / 2))
A <- (A > delta) * 1

z.hat <- cluster.pabm(A, K)
A.star <- A[order(z.hat), order(z.hat)]

ari <- round(fossil::adj.rand.index(z, z.hat) * 100)
```

In the first real data example, we applied Algorithm 2 to the Leeds Butterfly 
dataset \cite{Wang_2018} consisting of visual similarity measurements among 832 
butterflies across 10 species. The graph was modified to match the example 
from \citet{noroozi2019estimation}: Only the 4 most frequent species were 
considered, and the similarities were discretized to $\{0, 1\}$ via 
thresholding. Fig. \ref{fig:butterfly} shows a sorted adjacency matrix sorted 
by the resultant clustering. 

Comparing against the ground truth species labels, Algorithm 2 achieves an 
accuracy of 63\% and an adjusted Rand index of `r ari`\%. In comparison, 
\citet{noroozi2019estimation} achieved an adjusted Rand index of 73\% using 
sparse subspace clustering on the same dataset.

```{r}
plot.A <- function(A, margins = c(0, 0, 0, 0)) {
  default.mar <- par()$mar
  par(mar = margins)
  plot(A, border = NA, key = NULL, 
       xlab = '', ylab = '', main = '',
       axis.row = NULL, axis.col = NULL, asp = 1)
  par(mar = default.mar)
}
```

```{r butterfly, fig.height = 2, fig.width = 2, fig.cap = 'Adjacency matrix of the Leeds Butterfly dataset after sorting by the clustering outputted by Algorithm 2.', cache = TRUE}
plot.A(A.star)
```

```{r}
# this portion is from the code provided by Sengupta and Chen

setwd('~/Downloads/codes_and_data')
library("igraph")
library("clue")

##### Data input and pre-processing #####
foo=read.table("BritishMPcomm.txt",header=T)
mat=read.table("politicsuk-retweets.mtx",header=F)
mat = mat[-1,]
A = matrix(0,nrow(foo),nrow(foo))
for (k in 1:nrow(mat)){ #adjacency matrix with 5 parties
  edgei = mat[k,1]
  edgej = mat[k,2]
  i = which(foo[,1]==edgei)
  j = which(foo[,1]==edgej)
  A[i,j]<-1;A[j,i]<-1
}
b.true <- foo[,2]
levels(b.true) <- 1:5
b.true <- as.numeric(b.true)
nodes1 = which(b.true==1); nodes2 = which(b.true==2)
nodes = c(nodes1, nodes2)
A <- A[nodes,nodes] # Select only the two big communities
G<-graph.adjacency(A, mode="undirected")
foo1 <- clusters(G, mode="strong") 
nodes2 <- which(foo1$membership==1) # largest connected component (basically 31 nodes are disconnected)
A <- A[nodes2,nodes2] # form network with 329 nodes
b.true <- b.true[nodes2]
```

```{r}
zhat <- cluster.pabm(A, 2, use.all = TRUE, normalize = FALSE)
clust.acc <- mean(zhat == b.true)
```

```{r mp, cache = TRUE, fig.width = 2, fig.height = 2, fig.cap = 'Adjacency matrix of the British MPs dataset after sorting by the clustering outputted by Algorithm 2.'}
plot.A(A[order(zhat), order(zhat)])
```

In the second example, we applied Algorithm 2 to the British MPs Twitter 
network \cite{greene2013producing}. The original data consists of 419 vertices, 
each corresponding to a British MP belonging to one of five political parties. 
For this data analysis, we subsetted the data as described by 
\citet{307cbeb9b1be48299388437423d94bf1} for their analysis of the same 
dataset, resulting in 329 vertices and two parties ($n = 329, K = 2$). 
\citet{307cbeb9b1be48299388437423d94bf1} were able to obtain a classification 
accuracy of `r round(328 / 329 * 100, 1)`\%. Algorithm 2 obtained a lower 
classification accuracy of `r round(clust.acc * 100, 1)`\%. 

# Discussion

# References