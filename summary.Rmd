---
title: "Connecting the Popularity Adjusted Block Model to the Generalized Random Dot Product Graph for Clustering and Parameter Estimation"
author: John Koo, Minh Tang, Michael Trosset
output:
  pdf_document:
    citation_package: natbib
    number_sections: yes
# output: html_document
fontsize: 12pt
# geometry: "left=1cm,right=1cm,top=1cm,bottom=1.5cm"
urlcolor: blue
header-includes:
- \usepackage{float}
- \usepackage{mathtools}
- \usepackage{natbib}
- \usepackage[linesnumbered,ruled,vlined]{algorithm2e} 
- \setcitestyle{numbers,square}
- \usepackage{verbatim}
bibliography: misc.bib
abstract: |
  In this paper, we connect two random graph models, the Popularity 
  Adjusted Block Model (PABM) and the Generalized Random Dot Product Graph 
  (GRDPG) and use properties established in this connection to aid in 
  community detection and parameter estimation. In particular, we note that the
  PABM can be represented as latent positions such that points within the same 
  community lie on a subspace, and the subspaces that represent each 
  community are orthogonal to one another. Using this property as well as the 
  asymptotic properties of Adjacency Spectral Embedding (ASE) of the GRDPG, we 
  are able to establish theoretical asymptotic results of our community 
  detection and parameter estimation methods for the PABM. 
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      # eval = FALSE,
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 3, 
                      fig.width = 5, 
                      fig.dpi = 300)

options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')
```

```{r}
import::from(magrittr, `%>%`)
library(plot.matrix)
library(ggplot2)
library(mclust)

theme_set(theme_bw())
```

\newcommand{\diag}{\text{diag}}
\newcommand{\tr}{\text{Tr}}
\newcommand{\blockdiag}{\text{blockdiag}}
\newcommand{\indep}{\stackrel{\text{indep}}{\sim}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\Bernoulli}{\text{Bernoulli}}
\newcommand{\Betadist}{\text{Beta}}

# Introduction

Statistical analysis on graphs or networks often involves the partitioning of 
a graph into disconnected subgraphs or clusters. This is often motivated by the 
assumption that there exist underlying and unobserved communities to which each 
vertex of the graph belongs, and edges between pairs of vertices are determined 
by drawing from a probability distribution based on the community relationships 
between each pair. The goal of the analysis then is population community 
detection, or the recovery of the true underlying community labels for each 
vertex, up to permutation (with some additional parameter estimation being of 
possible interest), assuming some underlying probability model. One such model
is the Stochastic Block Model (SBM), first proposed by 
\citet{doi:10.1080/0022250X.1971.9989788}, which assumes that the edge 
probability from one vertex to another follows a Beronulli distribution with 
fixed probabilities for each pair of community labels. Other random 
graph models have been proposed and studied, such as the Degree-Corrected Block 
Model (DCBM), introduced by \citet{Karrer_2011}, which is a generalization of 
the SBM. The Popularity Adjusted Block Model (PABM) was then introduced by 
\citet*{307cbeb9b1be48299388437423d94bf1} as a generalization of the DCBM to 
address the heterogeneity of edge probabilities within and between communities 
while still maintaining distinct community structure. 

The underlying similarity among the SBM, PABM, and other such models is that 
they involve a symmetric edge probability matrix $P \in [0, 1]^{n \times n}$ 
where $n$ is the number of vertices in the graph. An undirected and unweighted 
graph is then drawn from this edge probability matrix such that the existence 
of an edge between each pair of vertices $i$ and $j$ is given by 
$\Bernoulli(P_{ij})$. For example, for the SBM with two communities for which 
the within-community edge probability is $\xi$ and the between-community edge 
probability is $\eta$, the entries of $P$ consist of $\xi$ and $\eta$. 

The Random Dot Product Graph (RDPG) model proposed by 
\citet*{10.1007/978-3-540-77004-6_11} is another graph model with Bernoulli 
edge probabilities. Under this model, each vertex of the graph can be
represented by a point in some latent space such that the edge
probability between any pair of vertices is given by their corresponding dot
product in the latent space, i.e., given a latent positions
$x_1, ..., x_n \in \mathbb{R}^d$, the edge probability matrix is 
$P = X X^\top$ where 
$X = \begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix}^\top$. The SBM is 
equivalent to a special case of the RDPG model in which all vertices of a 
given community share the same position in the latent space 
\cite{lyzinski2014}. It has also been shown that similar 
random graph models, including the DCBM, can be represented in this way
\cite{rubindelanchy2017consistency} \cite{lyzinski2014}. An analogous 
property exists for the PABM but not for the RDPG model but under the 
*Generalized* Random Dot Product Graph (GRDPG) model. This relationship will be 
explored in this paper and exploited to construct algorithms for community 
detection and parameter estimation for the PABM. 

In this paper, we will only consider undirected graphs, that is the edge weight 
from vertex $i$ to vertex $j$ is equal to the edge weight in the opposite 
direction, from vertex $j$ to vertex $i$. Furthermore, we will only consider 
unweighted graphs with binary ($0, 1$) edge weights We will also assume that 
graphs are hollow, i.e., there are no edges from a vertex to itself. All such 
graphs can be represented by a symmetric adjacency matrix 
$A \in \{0, 1\}^{n \times n}$ for which $A_{ij} = 1$ if there exists an edge 
between vertices $i$ and $j$ and $0$ otherwise, and $A$ is an element-wise 
independent Bernoulli draw from a symmetric edge probability matrix 
$P \in [0, 1]^{n \times n}$. 

# Connecting the Popularity Adjusted Block Model to the Generalized Random Dot Product Graph

## The popularity adjusted block model (PABM) and the generalized random dot product graph

**Definition 1** (Popularity Adjusted Block Model) 
\cite{307cbeb9b1be48299388437423d94bf1}. 
Let $P \in [0, 1]^{n \times n}$ be a symmetric edge probability matrix for a 
set of $n$ 
vertices, $V$. Each vertex has a community label $1, ..., K$, and the rows and 
columns of $P$ are arranged by community label such that $n_k \times n_l$ block 
$P^{(kl)}$ describes the edge probabilities between vertices in communities 
$k$ and $l$ ($P^{(lk)} = (P^{(kl)})^\top$). 
Let graph $G = (V, E)$ be an undirected, unweighted graph such 
that its corresponding adjacency matrix $A \in \{0, 1\}^{n \times n}$ is a 
realization of $\Bernoulli(P)$, i.e., 
$A_{ij} \indep \Bernoulli(P_{ij})$ for $i > j$ 
($A_{ij} = A_{ji}$ and $A_{ii} = 0$). 

If each block $P^{(kl)}$ can be written as the outer product of two vectors:

\begin{equation}
  P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top
\end{equation}

for a set of $K^2$ fixed vectors $\{\lambda^{(st)}\}_{s, t = 1}^K$ where each 
$\lambda^{(st)}$ is a column vector 
of dimension $n_s$, then graph $G$ and its corresponding adjacency matrix $A$ 
is a realization of a popularity adjusted block model with parameters 
$\{\lambda^{(st)}\}_{s, t = 1}^K$. 

We will use the notation $A \sim PABM(\{\lambda^{(kl)}\}_K)$ to denote a random 
adjacency matrix $A$ drawn from a PABM with parameters $\lambda^{(kl)}$ 
consisting of $K$ underlying communities.

**Definition 2** (Generalized Random Dot Product Graph) 
\cite{rubindelanchy2017statistical}.
Let $P \in [0, 1]^{n \times n}$ be a symmetric edge probability matrix for a 
set of $n$ vertices, $V$. If $\exists X \in \mathbb{R}^{n \times d}$ such that 

\begin{equation}
  P = X I_{pq} X^\top
\end{equation}

for some $d, p, q \in \mathbb{N}$ and $p + q = d$, then 
graph $G = (V, E)$ with adjacency matrix $A$ such that 
$A_{ij} \indep \Bernoulli(P_{ij})$ for $i > j$ ($A_{ij} = A_{ji}$ and 
$A_{ii} = 0$) is a draw from the generalized random dot product graph model 
with latent positions $X$ and signature $(p, q)$. More precisely, if vertices 
$i$ and $j$ have latent positions $x_i$ and $x_j$ respectively, then the edge 
probability between the two is $P_{ij} = x_i^\top I_{pq} x_j$, and $X$ contains 
the latent positions as rows $x_i^\top$. 

We will use the notation $A \sim GRDPG_{p,q}(X)$ to denote a random adjacency 
matrix $A$ drawn from latent positions $X$ and signature $(p, q)$.

**Definition 3**. The indefinite orthogonal group with signature $(p, q)$ is 
the set $\{Q \in \mathbb{R}^{d \times d} : Q I_{pq} Q^{\top} = I_{pq}\}$, 
denoted as $\mathbb{O}(p, q)$ \cite{rubindelanchy2017statistical}.

**Remark**. 
Like the RDPG, the latent positions of a GRDPG are not unique 
\cite{rubindelanchy2017statistical}. 
More specifically, if $P_{ij} = x_i^\top I_{pq} x_j$, then we also have for any 
$Q \in \mathbb{O}(p, q)$, 
$(Q x_i)^\top I_{pq} (Q x_j) = x_i^\top (Q^\top I_{pq} Q) x_j = 
x_i^\top I_{pq} x_j = P_{ij}$. 
Unlike in the RDPG case, transforming the latent positions via multiplication 
by $Q \in \mathbb{O}(p, q)$ does not necessarily maintain interpoint angles or 
distances. 

## Connecting the PABM to the GRDPG

**Theorem 1** (Connecting the PABM to the GRDPG for $K = 2$). 
Let 

$$X = \begin{bmatrix} 
\lambda^{(11)} & \lambda^{(12)} & 0 & 0 \\ 
0 & 0 & \lambda^{(21)} & \lambda^{(22)} 
\end{bmatrix}$$

$$U = \begin{bmatrix} 1 & 0 & 0 & 0 \\
0 & 0 & 1 / \sqrt{2} & 1 / \sqrt{2} \\
0 & 0 & 1 / \sqrt{2} & - 1 / \sqrt{2} \\
0 & 1 & 0 & 0 \end{bmatrix}$$

as in Definition 1. 
Then $A \sim GRDPG_{3, 1}(X U)$ and $A \sim PABM(\{(\lambda^{(kl)}\}_2)$ are 
equivalent.

**Theorem 2** (Generalization to $K > 2$). There exists a block diagonal matrix 
$X \in \mathbb{R}^{n \times K^2}$ defined by PABM parameters 
$\{\lambda^{(kl)}\}_K$ and $U \in \mathbb{R}^{K^2 \times K^2}$ that is fixed 
for each $K$ such that $A \sim GRDPG_{K (K+1) / 2, K (K-1) / 2}(XU)$ and 
$A \sim PABM(\{(\lambda^{(kl)}\})_K)$ are equivalent.

**Example** ($K = 3$). Using the same notation as before:

$$X = \begin{bmatrix} 
\lambda^{(11)} & \lambda^{(12)} & \lambda^{(13)} & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & \lambda^{(21)} & \lambda^{(22)} & \lambda^{(23)} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & \lambda^{(31)} & \lambda^{(32)} & \lambda^{(33)}
\end{bmatrix}$$

$$Y = \begin{bmatrix} 
\lambda^{(11)} & 0 & 0 & \lambda^{(12)} & 0 & 0 & \lambda^{(13)} & 0 & 0 \\
0 & \lambda^{(21)} & 0 & 0 & \lambda^{(22)} & 0 & 0 & \lambda^{(23)} & 0 \\
0 & 0 & \lambda^{(31)} & 0 & 0 & \lambda^{(32)} & 0 & 0 & \lambda^{(33)}
\end{bmatrix}$$

Then $P = X Y^\top$ and $Y = X \Pi$ where $\Pi$ is a permutation matrix 
consisting of $3$ fixed points and $3$ cycles of order 2:

$$\Pi = \begin{bmatrix} 
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1
\end{bmatrix}$$

* Positions 1, 5, 9 are fixed.
* The cycles of order 2 are $(2, 4)$, $(3, 7)$, and $(6, 8)$.
    
Therefore, we can decompose $\Pi = U I_{6, 3} U^\top$ where the first three 
columns of $U$ consist of $e_1$, $e_5$, and $e_9$ corresponding to the fixed 
positions $1$, $5$, and $9$, the next three columns consist of eigenvectors 
$(e_k + e_l) / \sqrt{2}$, and the last three columns consist of eigenvectors 
$(e_k - e_l) / \sqrt{2}$, where pairs $(k, l)$ correspond to the cycles of 
order 2 described above.

The latent positions are the rows of  
$$XU = \begin{bmatrix}
  \lambda^{(11)} & 0 & 0 & 
  \lambda^{(12)} / \sqrt{2} & \lambda^{(13)} / \sqrt{2} & 0 & 
  \lambda^{(12)} / \sqrt{2} & \lambda^{(13)} / \sqrt{2} & 0 \\
  0 & \lambda^{(22)} & 0 & 
  \lambda^{(21)} / \sqrt{2} & 0 & \lambda^{(23)} / \sqrt{2} & 
  -\lambda^{(21)} / \sqrt{2} & 0 & \lambda^{(23)} / \sqrt{2} \\
  0 & 0 & \lambda^{(33)} & 
  0 & \lambda^{(31)} / \sqrt{2} & \lambda^{(32)} / \sqrt{2} & 
  0 & -\lambda^{(31)} / \sqrt{2} & -\lambda^{(32)} / \sqrt{2}
\end{bmatrix}$$

# Methods

Two inference objectives arise from the PABM:

1. Community membership identification (up to permutation).
2. Parameter estimation (estimating $\lambda^{(kl)}$'s).

In our methods, we assume that $K$, the number of communities, is known 
beforehand and does not require estimation.

## Related work

\citet*{307cbeb9b1be48299388437423d94bf1}, who first proposed the PABM, used 
a relaxed version of Modularity Maximization (MM) based on the Extreme Points
(EP) algorithm \cite{le2016} for community detection and parameter estimation. 
They were able to show that as the sample size increases, the proportion of
misclassified community labels (up to permutation) goes to 0. 

\citet*{noroozi2019estimation} used Sparse Subspace Clustering (SSC) for 
community detection in the PABM. 
SSC is performed by solving an optimization problem for each observed point.
Given $X \in \mathbb{R}^{n \times d}$ with vectors 
$x_i^\top \in \mathbb{R}^d$ as rows of $X$, the optimization problem 
$\min_{c_i} ||c_i||_1$ subject to $x_i = X c_i$ and $\beta_i = 0$ is solved 
for each $i = 1, ..., n$. The solutions are collected collected into matrix 
$C = \begin{bmatrix} c_1 & \cdots & c_n \end{bmatrix}^\top$ to construct an 
affinity matrix $B = |C| + |C^\top|$. If each $x_i$ lie perfectly on one of $K$ 
subspaces, $B$ describes an undirected graph consisting of $K$ disjoint 
subgraphs, i.e., $B_{ij} = 0$ if $x_i, x_j$ are in different subspaces. 
If $X$ instead represents points near $K$ subspaces with some noise, a final 
graph partitioning step is performed (e.g., edge thresholding or spectral 
clustering). 

In practice, SSC is often performed by solving the LASSO problems 

\begin{equation}
\min_{c_i} \frac{1}{2} ||x_i - X_{-i} c_i||^2_2 + \lambda ||c_i||_1
\end{equation}

for some sparsity parameter $\lambda > 0$. The $c_i$ vectors are then collected 
into $C$ and $B$ as before.

Theorem 2 suggests that SSC is appropriate for community detection for the 
PABM. More precisely, theorem 2 says that each community consists of a
$K$-dimensional subspace, and together the subspaces lie in $\mathbb{R}^{K^2}$. 
The natural approach then is to perform SSC on the ASE of $P$ or $A$.
\citet{noroozi2019estimation} instead applied SSC to $P$ and $A$ themselves. 

Much of our work is built on top of work from 
\citet{rubindelanchy2017statistical}, in particular, the properties of the ASE 
of $A$ in relation to the ASE of $P$ built on $\{\lambda^{(kl)}\}_K$.

## Community detection

We previously stated one possible set of latent positions that result in the 
edge probability matrix of a PABM, $P = (XU) I_{pq} (XU)^\top$. If we have 
(or can estimate) $XU$ directly, then both the community detection and 
parameter identification problem are trivial since $U$ is orthonormal and 
fixed for each value of $K$. However, direct identification or estimation of
$XU$ is not possible \cite{rubindelanchy2017statistical}. 

If we decompose $P = Z I_{pq} Z^\top$, then
$\exists Q \in \mathbb{O}(p, q)$ such that $XU = Z Q$. Even if we start with 
the exact edge probability matrix, we cannot recover the "original" latent 
positions $XU$. Note that unlike in the case of the RDPG, 
$Q$ is not an orthogonal matrix. If $z_i$'s are the rows of $XU$, then 
$||z_i - z_j||^2 \neq ||Q z_i - Q z_j||^2$, and 
$\langle z_i, z_j \rangle \neq \langle Q z_i, Q z_j \rangle$. This prevents 
us from using the properties of $XU$ directly. In particular, if 
$Q \in \mathbb{O}(n)$, then we could use the fact that 
$\langle z_i, z_j \rangle = \langle Q z_i, Q z_j \rangle = 0$ if vertices $i$ 
and $j$ are in different communities. 

The explicit form of $XU$ represents points in 
$\mathbb{R}^{K^2}$ such that points within each community lie on 
$K$-dimensional orthogonal subspaces. 
Multiplication by $Q \in \mathbb{O}(p, q)$ removes the orthogonality property 
but retains the property that each community is represented by a 
$K$-dimensional subspace. Therefore, the ASE of $P$ results in subspaces that 
correspond to each community, suggesting the use of SSC. In this paper, we will 
use a different and leave the properties of SSC on the ASE of $A$ as future
work.

**Theorem 3.** 
Let $P = V D V^\top$ be the spectral decomposition of the edge probability
matrix. Let $B = V V^\top$. Then $B_{ij} = 0$ if vertices $i$ and $j$ are 
from different communities.

Theorem 3 provides perfect community detection given $P$. Letting $|B|$ be the 
affinity matrix for graph $G$, $G$ is partitioned into $K$ disjoint subgraphs 
that correspond to each community. Using $A$ introduces some error in the ASE, 
resulting in points that lie near subspaces but not exactly on them. Using 
results from \cite{rubindelanchy2017statistical}, we can show that as $n$ 
grows, the ASE of $A$ approaches the ASE of $P$ and $B_{ij} \to 0$ for $i$ and 
$j$ in different clusters. 

The ASE of $A$ approaches latent positions that form $P$ as 
the number of vertices $n$ increases. More precisely, let each 
$\lambda^{(kl)}_i \sim \mathcal{F}^{(kl)}$ for $i = 1, .., n$ and 
$k, l = 1, ..., K$. Then the corresponding latent positions 
$XU \sim \mathcal{G}_K$ for some related joint distribution with $K$ underlying
communities $\mathcal{G}_K$. Denote $Z_n$ as a sample of size $n$ from
$\mathcal{G}_K$ and adjacency matrix $A_n$ as one draw from edge probability 
matrix $P_n = Z_n I_{pq} Z_n^\top$. Let $\hat{Z}_n$ be the adjacency embedding 
of $A_n$ with rows $(\hat{z}_i^{(n)})^\top$. Then by
\citet{rubindelanchy2017statistical}, 

\begin{equation}
  \max\limits_{i \in \{1, ..., n\}} ||Q_n \hat{z}_i^{(n)} - z_i^{(n)}|| = 
  O_P\bigg(\frac{(\log n)^c}{n^{1/2}} \bigg)
\end{equation}

for some $c > 0$ and sequence of $Q_n \in \mathbb{O}(p, q)$. 

**Theorem 4**. Let $\hat{V}^{(n)} \in \mathbb{R}^{n \times K^2}$ be the matrix 
of $K^2$ eigenvectors of $A_n$ corresponding to the $K (K+1) / 2$ most positive 
eigenvalues and $K (K-1) / 2$ most negative eigenvalues with rows
$(\hat{v}_i^{(n)})^\top$. Let $(i, j)$ correspond to pairs belonging to 
different communities. Let $n \rho_n = \omega((\log n)^{4c}))$ as described in
\citet{rubindelanchy2017statistical}. Then for some $c > 0$, 

\begin{equation}
  \max\limits_{i, j} ||(\hat{v}^{(n)}_i)^\top \hat{v}_j^{(n)}|| = 
    O_P\bigg( \frac{(\log n)^c}{n \sqrt{\rho_n}}\bigg)
\end{equation}

Thus, as $n \to \infty$, the partitioning of $\hat{B}^{(n)}$ results in 
perfect community detection. 
This leads to the Orthogonal Spectral Clustering (OSE) algorithm (Alg. 1). 

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetAlgoLined
  \KwData{Adjacency matrix $A$, number of communities $K$}
  \KwResult{Community assignments $1, ..., K$}
    Compute the eigenvectors of $A$ that correspond to the $K (K+1) / 2$ most 
    positive eigenvalues and $K (K-1) / 2$ most negative eigenvalues. Construct 
    $V$ using these eigenvectors as its columns.\;
    Compute $B = |V V^\top|$, applying $|\cdot|$ entry-wise.\;
    Construct graph $G$ using $B$ as its similarity matrix.\;
    Partition $G$ into $K$ disconnected subgraphs  
    (e.g., using edge thresholding or spectral clustering).\;
    Map each partition to the community labels $1, ..., K$.\;
  \caption{Orthogonal Spectral Clustering.}
\end{algorithm}

Theorems 2, 3, and 4 also provide a very natural path toward using SSC for 
community detection for the PABM. We established in theorem 2 that an ASE of 
the edge probability matrix $P$ can be constructed such that the communities 
lie on mutually orthogonal subspaces, and this property can be recovered from
the eigenvectors of $P$. Then theorems 3 and 4 show that this property holds 
for the adjacency matrix $A$ drawn from $P$ as $n \to \infty$. 

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetAlgoLined
  \caption{Sparse Subspace Clustering using LASSO \cite{pmlr-v28-wang13}.}
  \KwData{Adjacency matrix $A$, number of communities $K$, 
  hyperparameter $\lambda$}
  \KwResult{Community assignments $1, ..., K$}
    Find $Z$, the ASE of $A$ using the $K (K + 1) / 2$ most positive 
    and the $K (K - 1) / 2$ most negative eigenvalues and eigenvectors of 
    $A$.\;
    \For {$i = 1, ..., n$} {
      Assign $z_i^\top$ as the $i^{th}$ row of $Z$. 
      Assign $Z_{-i} = \begin{bmatrix} 
      z_1 & \cdots & z_{i-1} & z_{i+1} & \cdots & z_n \end{bmatrix}^\top$.\;
      Solve the LASSO problem 
      $c_i = \arg\min_{\beta_i} 
      \frac{1}{2} ||z_i - Z_{-i} \beta_i||_2^2 + \lambda ||\beta_i||_1$.\;
      Assign $\tilde{c}_i = \begin{bmatrix} 
      c_i^{(1)} & \cdots & c_i^{(i-1)} & 0 & c_i^{(i)} & \cdots & c_i^{(n-1)}
      \end{bmatrix}^\top$ such that the superscript is the index of 
      $\tilde{c}_i$.\;
    }
    Assign 
    $C = \begin{bmatrix} \tilde{c}_1 & \cdots & \tilde{c}_n \end{bmatrix}$.\;
    Compute $B = |C| + |C^\top|$.\;
    Construct graph $G$ using $B$ as its similarity matrix.\;
    Partition $G$ into $K$ disconnected subgraphs (e.g., using edge 
    thresholding or spectral clustering).\;
    Map each partition to the community labels $1, ..., K$. 
\end{algorithm}

**Theorem 5**. Let $P_n$ describe the edge probability matrix of the PABM with 
$n$ vertices, and let $A_n \sim \Bernoulli(P_n)$. Let $Z_n$ be the ASE of 
$P_n$, and let $Z_n^{(k)}$ correspond to the embedding points that correspond 
to the $k$^th^ block. Let $\hat{Z}_n$ be the ASE of $A_n$ and $\hat{Z}_n^{(k)}$ 
be the corresponding $k$^th^ block. If $r(Z_n^{(k)})$, the inradius of 
$Z_n^{(k)}$ (as defined by \citet{soltanolkotabi2012}), is positive for each 
$k$, then the SSC detection property holds for $\hat{Z}_n$ with probability 1 
as $n \to \infty$. 

## Parameter estimation

For any edge probability matrix $P$ for the PABM such that the rows and 
columns are organized by community, the $kl$^th^ block is an outer product 
of two vectors, i.e., $P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top$. 
Therefore, given $P^{(kl)}$, $\lambda^{(kl)}$ and $\lambda^{(lk)}$ are 
solvable exactly (up to multiplication by $-1$) using singular value 
decomposition. More specifically, let $P^{(kl)} = \sigma^2 u v^\top$ be the 
singular value decomposition of $P^{(kl)}$. $u \in \mathbb{R}^{n_k}$ and 
$v \in \mathbb{R}^{n_l}$ are vectors and 
$\sigma^2 > 0$ is a scalar. Then $\lambda^{(kl)} = \pm \sigma u$ and 
$\lambda^{(lk)} = \pm \sigma v$. 
Given the adjacency matrix $A$ instead of edge probability matrix $P$, we can 
simply use plug-in estimators (algorithm 3), which converge to the true 
parameters (theorem 6).

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetAlgoLined
  \caption{PABM parameter estimation using the adjacency matrix.}
  \KwData{Adjacency matrix $A$, community assignments $1, ..., K$}
  \KwResult{PABM parameter estimates $\{\hat{\lambda}^{(kl)}\}_K$.}
  Arrange the rows and columns of $A$ by community such that each 
  $A^{(kl)}$ block consists of estimated edge probabilities between 
  communities $k$ and $l$.\;
  \For {$k, l = 1, ..., K$, $k \leq l$} {
    Compute $A^{(kl)} = U \Sigma V^\top$, the SVD of the $kl^{th}$ 
    block.\;
    Assign $u^{(kl)}$ and $v^{(kl)}$ as the first columns of $U$ and $V$. 
    Assign $(\sigma^{(kl)})^2 \leftarrow \Sigma_{11}$.\;
    Assign $\hat{\lambda}^{(kl)} \leftarrow \pm \sigma^{(kl)} u^{(kl)}$ and 
    $\hat{\lambda}^{(lk)} \leftarrow \pm \sigma^{(kl)} v^{(kl)}$.
  }
\end{algorithm}

**Theorem 6**. Under regularity and sparsity assumptions, and under the further 
assumption that $K$ is fixed and community labels are known, 

\begin{equation}
\max_{k, l \in \{1, ..., K\}} 
||\hat{\lambda}^{(kl)} - \lambda^{(kl)}|| = 
O_P \bigg(\frac{(\log n_k)^c}{\sqrt{n_k}} \bigg)
\end{equation}

# Simulated Examples

For each simulation, community labels are drawn from a multinomial 
distribution, the popularity vectors $\{\lambda^{(kl)}\}_K$ are drawn 
from two types of joint distributions depending on whether $k = l$, the edge
probability matrix $P$ is constructed using 
the popularity vectors, and finally an unweighted and undirected adjacency 
matrix $A$ is drawn from $P$. OSE is then used for community detection, 
and this method is compared against SSC \cite{noroozi2019estimation}
\cite{soltanolkotabi2014} and MM \cite{igraph}
\cite{307cbeb9b1be48299388437423d94bf1}. True 
community labels are used with Algorithm 3 to estimate the popularity 
vectors $\{\lambda^{(kl)}\}_K$, and this method is then compared against an
MLE-based estimator described in \citet{noroozi2019estimation} and
\citet{307cbeb9b1be48299388437423d94bf1}. 

Modularity Maximization is NP-hard, so \citet{307cbeb9b1be48299388437423d94bf1} 
used the Extreme Points (EP) algorithm \cite{le2016}, which is $O(n^{K - 1})$. 
For these simulations, the EP algorithm was used for $K = 2$, and for $K > 2$, 
the Louvain algorithm \cite{Blondel_2008} was used instead.

Two implementations of SSC are shown here. The first method, denoted as 
SSC-A, treats the columns of the adjacency matrix $A$ as points in
$\mathbb{R}^n$, as described in \citet{noroozi2019estimation}. The second 
method, denoted as SSC-ASE, first embeds $A$ and then performs SSC on the 
embedding, as described in algorithm 2. The sparsity parameter $\lambda$ was 
chosen via a preliminary cross-validation experiment. For the final clustering 
step, a Gaussian Mixture Model was fit on the normalized Lapacian eigenmap of 
the affinity matrix $B$. 

## Balanced communities

In each simulation, community labels $z_1, ..., z_n$ were drawn 
from a multinomial distribution with mixture parameters 
$\{\alpha_1, ..., \alpha_K\}$, then $\{\lambda^{(kl)}\}_K$ according to the 
drawn community labels, $P$ was constructed using the drawn 
$\{\lambda^{(kl)}\}_K$, and $A$ was drawn from $P$ by 
$A_{ij} \stackrel{indep}{\sim} Bernoulli(P_{ij})$. Each simulation has a unique
edge probability matrix $P$. 

For these examples, we set the following parameters:

* Number of vertices $n = 128, 256, 512, 1024, 2048, 4096$
* Number of underlying communities $K = 2, 3, 4$
* Mixture parameters $\alpha_k = 1 / K$ for $k = 1, ..., K$, (i.e., each 
community label has an equal probability of being drawn)
* Community labels 
$z_k \iid Multinomial(\alpha_1, ..., \alpha_K)$
* Within-group popularities $\lambda^{(kk)} \iid Beta(2, 1)$
* Between-group popularities 
$\lambda^{(kl)} \iid Beta(1, 2)$ for $k \neq l$

$50$ simulations were performed for each $(n, K)$ pair. 

Fig. \ref{fig:clust_err_k} show that for large $n$, OSE results in 
a misclustering rate of 0. Sparse subspace clustering produces similar results 
for $K > 2$. 

```{r clust_err_k, fig.cap = 'IQR of clustering error using OSE (blue) compared against SSC on the ASE of A (purple), MM (red), and SSC on the adjacency matrix (green). Communities are approximately balanced. Simulations were repeated 50 times for each sample size.', fig.width = 10}
clustering.df <- readr::read_csv('clustering-k.csv')
ssc.df <- readr::read_csv('clustering-ssc-k.csv')
clustering.df %>%
  dplyr::group_by(n, K) %>%
  dplyr::summarise(med.err = median(error),
                   first.q = quantile(error, .25),
                   third.q = quantile(error, .75),
                   med.err.ssc = median(error.ssc),
                   first.q.ssc = quantile(error.ssc, .25),
                   third.q.ssc = quantile(error.ssc, .75),
                   med.err.mm = median(error.mm),
                   first.q.mm = quantile(error.mm, .25),
                   third.q.mm = quantile(error.mm, .75)) %>%
  dplyr::ungroup() %>% 
  dplyr::inner_join(
    ssc.df %>% 
      dplyr::group_by(n, K) %>% 
      dplyr::summarise(med.err.ssc2 = median(error.ssc2),
                       first.q.ssc2 = quantile(error.ssc2, .25),
                       third.q.ssc2 = quantile(error.ssc2, .75)) %>% 
      dplyr::ungroup()
  ) %>% 
  ggplot() +
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096)) +
  # scale_x_continuous(breaks = c(128, 256, 512, 1024, 2048, 4096)) + 
  labs(y = 'community detection error rate', 
       colour = NULL) +
  geom_line(aes(x = n, y = med.err,
                colour = 'OSC')) +
  geom_errorbar(aes(x = n, ymin = first.q, ymax = third.q,
                    colour = 'OSC'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc,
                colour = 'SSC-ASE')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc, ymax = third.q.ssc,
                    colour = 'SSC-ASE'), width = .1) + 
  geom_line(aes(x = n, y = med.err.mm,
                colour = 'MM-EP')) + 
  geom_errorbar(aes(x = n, ymin = first.q.mm, ymax = third.q.mm,
                    colour = 'MM-EP'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc2,
                colour = 'SSC-A')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc2, ymax = third.q.ssc2,
                    colour = 'SSC-A'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  facet_wrap(~ K, labeller = 'label_both') + 
  scale_y_log10()
```

Theorem 4 implies that OSE will result in not just in the error rate 
converging to 0 but the error *count* as well.
We explore this in Fig. \ref{fig:clust_err_ct_sim}.

```{r clust_err_ct_sim, fig.cap = 'IQR of counts of misclustered vertices using OSE (blue) compared against SSC on the ASE of A (purple), MM (red), and SSC on the adjacency matrix (green). Communities are approximately balanced. Simulations were repeated 50 times for each sample size.', cache = FALSE, fig.width = 10}
clustering.df <- readr::read_csv('clustering-k.csv')
ssc.df <- readr::read_csv('clustering-ssc-k.csv')
clustering.df %>%
  dplyr::group_by(n, K) %>%
  dplyr::summarise(med.err = median(error),
                   first.q = quantile(error, .25),
                   third.q = quantile(error, .75),
                   med.err.ssc = median(error.ssc),
                   first.q.ssc = quantile(error.ssc, .25),
                   third.q.ssc = quantile(error.ssc, .75),
                   med.err.mm = median(error.mm),
                   first.q.mm = quantile(error.mm, .25),
                   third.q.mm = quantile(error.mm, .75)) %>%
  dplyr::ungroup() %>% 
  dplyr::inner_join(
    ssc.df %>% 
      dplyr::group_by(n, K) %>% 
      dplyr::summarise(med.err.ssc2 = median(error.ssc2),
                       first.q.ssc2 = quantile(error.ssc2, .25),
                       third.q.ssc2 = quantile(error.ssc2, .75)) %>% 
      dplyr::ungroup()
  ) %>% 
  ggplot() +
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096)) +
  # scale_x_continuous(breaks = c(128, 256, 512, 1024, 2048, 4096)) + 
  labs(y = 'community detection error count', 
       colour = NULL) +
  geom_line(aes(x = n, y = med.err * n,
                colour = 'OSC')) +
  geom_errorbar(aes(x = n, ymin = first.q * n, ymax = third.q * n,
                    colour = 'OSC'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc * n,
                colour = 'SSC-ASE')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc * n, ymax = third.q.ssc * n,
                    colour = 'SSC-ASE'), width = .1) + 
  geom_line(aes(x = n, y = med.err.mm * n,
                colour = 'MM-EP')) + 
  geom_errorbar(aes(x = n, ymin = first.q.mm * n, ymax = third.q.mm * n,
                    colour = 'MM-EP'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc2 * n,
                colour = 'SSC-A')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc2 * n, ymax = third.q.ssc2 * n,
                    colour = 'SSC-A'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  facet_wrap(~ K, labeller = 'label_both') + 
  scale_y_log10()
```

Given ground truth community labels, Algorithm 3 and the MLE-based plug-in 
estimators \cite{307cbeb9b1be48299388437423d94bf1} \cite{noroozi2019estimation} 
perform similarly, with root mean square error decaying at rate approximately 
$n^{-1/2}$. 

```{r lambda_est_k, cache = FALSE, fig.cap = 'Median and IQR RMSE from Algorithm 3 (red) compared against an MLE-based method (blue). Simulations were repeated 50 times for each sample size.', fig.width = 10}
rmse.df <- readr::read_csv('rmse-k.csv')

rmse.df %>% 
  na.omit() %>% 
  dplyr::group_by(K, n) %>% 
  dplyr::summarise(median.rmse = median(rmse),
                   q1.rmse = quantile(rmse, .25),
                   q3.rmse = quantile(rmse, .75),
                   median.rmse.mle = median(rmse.mle),
                   q1.rmse.mle = quantile(rmse.mle, .25),
                   q3.rmse.mle = quantile(rmse.mle, .75)) %>% 
  dplyr::ungroup() %>% 
  ggplot() +
  geom_line(aes(x = n, y = median.rmse, colour = 'Algorithm 3')) + 
  geom_errorbar(aes(x = n, ymin = q1.rmse, ymax = q3.rmse,
                    colour = 'Algorithm 3'), width = .1) + 
  geom_line(aes(x = n, y = median.rmse.mle, colour = 'MLE-based')) + 
  geom_errorbar(aes(x = n, ymin = q1.rmse.mle, ymax = q3.rmse.mle,
                   colour = 'MLE-based'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  labs(y = 'RMSE', colour = NULL) + 
  facet_wrap(~ K, labeller = 'label_both') + 
  scale_y_log10() + 
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096))
```

## Imbalanced communities

Simulations performed in this section are similar to those in the previous 
section with the exception of the mixture parameters 
$\{\alpha_1, ..., \alpha_K\}$ used to draw community labels from the 
multinomial distribution. For these examples, we set the following parameters:

* Number of vertices $n = 128, 256, 512, 1024, 2048, 4096$
* Number of underlying communities $K = 2, 3, 4$
* Mixture parameters $\alpha_k = \frac{k^{-1}}{\sum_{l=1}^K l^{-1}}$ for 
$k = 1, ..., K$
* Community labels 
$z_k \iid Multinomial(\alpha_1, ..., \alpha_K)$
* Within-group popularities $\lambda^{(kk)} \iid Beta(2, 1)$
* Between-group popularities 
$\lambda^{(kl)} \iid Beta(1, 2)$ for $k \neq l$

$50$ simulations were performed for each $(n, K)$ pair. 

Fig. \ref{fig:clust_err_k_imba} and \ref{fig:clust_err_ct_sim_imba} show 
similar results as in the balanced communities case, with both OSE and 
SSC resulting in no misclustered vertices for a 
sufficiently large sample size. However, Fig. \ref{fig:lambda_est_k_imba} 
suggests that while Algorithm 3 retains $\sqrt{n}$ efficiency, the MLE-based 
plug-in estimator is less efficient for this setup. 

```{r clust_err_k_imba, fig.cap = 'IQR of clustering error using OSE (blue) compared against SSC on the ASE of A (purple), MM (red), and SSC on the adjacency matrix (green). Communities are imbalanced. Simulations were repeated 50 times for each sample size.', fig.width = 10}
clustering.df <- readr::read_csv('clustering-k-imbalanced.csv')
ssc.df <- readr::read_csv('clustering-ssc-k-imbalanced.csv')
clustering.df %>%
  dplyr::group_by(n, K) %>%
  dplyr::summarise(med.err = median(error),
                   first.q = quantile(error, .25),
                   third.q = quantile(error, .75),
                   med.err.ssc = median(error.ssc),
                   first.q.ssc = quantile(error.ssc, .25),
                   third.q.ssc = quantile(error.ssc, .75),
                   med.err.mm = median(error.mm),
                   first.q.mm = quantile(error.mm, .25),
                   third.q.mm = quantile(error.mm, .75)) %>%
  dplyr::ungroup() %>% 
  dplyr::inner_join(
    ssc.df %>% 
      dplyr::group_by(n, K) %>% 
      dplyr::summarise(med.err.ssc2 = median(error.ssc2),
                       first.q.ssc2 = quantile(error.ssc2, .25),
                       third.q.ssc2 = quantile(error.ssc2, .75)) %>% 
      dplyr::ungroup()
  ) %>% 
  ggplot() +
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096)) +
  # scale_x_continuous(breaks = c(128, 256, 512, 1024, 2048, 4096)) + 
  labs(y = 'community detection error rate', 
       colour = NULL) +
  geom_line(aes(x = n, y = med.err,
                colour = 'OSC')) +
  geom_errorbar(aes(x = n, ymin = first.q, ymax = third.q,
                    colour = 'OSC'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc,
                colour = 'SSC-ASE')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc, ymax = third.q.ssc,
                    colour = 'SSC-ASE'), width = .1) + 
  geom_line(aes(x = n, y = med.err.mm,
                colour = 'MM-EP')) + 
  geom_errorbar(aes(x = n, ymin = first.q.mm, ymax = third.q.mm,
                    colour = 'MM-EP'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc2,
                colour = 'SSC-A')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc2, ymax = third.q.ssc2,
                    colour = 'SSC-A'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  facet_wrap(~ K, labeller = 'label_both') + 
  scale_y_log10()
```

```{r clust_err_ct_sim_imba, fig.cap = 'IQR of counts of misclustered vertices using OSE (blue) compared against SSC on the ASE of A (purple), MM (red), and SSC on the adjacency matrix (green). Communities are imbalanced. Simulations were repeated 50 times for each sample size.', cache = FALSE, fig.width = 10}
clustering.df <- readr::read_csv('clustering-k-imbalanced.csv')
ssc.df <- readr::read_csv('clustering-ssc-k-imbalanced.csv')
clustering.df %>%
  dplyr::group_by(n, K) %>%
  dplyr::summarise(med.err = median(error),
                   first.q = quantile(error, .25),
                   third.q = quantile(error, .75),
                   med.err.ssc = median(error.ssc),
                   first.q.ssc = quantile(error.ssc, .25),
                   third.q.ssc = quantile(error.ssc, .75),
                   med.err.mm = median(error.mm),
                   first.q.mm = quantile(error.mm, .25),
                   third.q.mm = quantile(error.mm, .75)) %>%
  dplyr::ungroup() %>% 
  dplyr::inner_join(
    ssc.df %>% 
      dplyr::group_by(n, K) %>% 
      dplyr::summarise(med.err.ssc2 = median(error.ssc2),
                       first.q.ssc2 = quantile(error.ssc2, .25),
                       third.q.ssc2 = quantile(error.ssc2, .75)) %>% 
      dplyr::ungroup()
  ) %>% 
  ggplot() +
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096)) +
  # scale_x_continuous(breaks = c(128, 256, 512, 1024, 2048, 4096)) + 
  labs(y = 'community detection error count', 
       colour = NULL) +
  geom_line(aes(x = n, y = med.err * n,
                colour = 'OSC')) +
  geom_errorbar(aes(x = n, ymin = first.q * n, ymax = third.q * n,
                    colour = 'OSC'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc * n,
                colour = 'SSC-ASE')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc * n, ymax = third.q.ssc * n,
                    colour = 'SSC-ASE'), width = .1) + 
  geom_line(aes(x = n, y = med.err.mm * n,
                colour = 'MM-EP')) + 
  geom_errorbar(aes(x = n, ymin = first.q.mm * n, ymax = third.q.mm * n,
                    colour = 'MM-EP'), width = .1) + 
  geom_line(aes(x = n, y = med.err.ssc2 * n,
                colour = 'SSC-A')) + 
  geom_errorbar(aes(x = n, ymin = first.q.ssc2 * n, ymax = third.q.ssc2 * n,
                    colour = 'SSC-A'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  facet_wrap(~ K, labeller = 'label_both') + 
  scale_y_log10()
```

```{r lambda_est_k_imba, cache = FALSE, fig.cap = 'Median and IQR RMSE from Algorithm 3 (red) compared against an MLE-based method (blue). Simulations were repeated 50 times for each sample size.', fig.width = 10}
rmse.df <- readr::read_csv('rmse-k-imbalanced.csv')

rmse.df %>% 
  na.omit() %>% 
  dplyr::group_by(K, n) %>% 
  dplyr::summarise(median.rmse = median(rmse),
                   q1.rmse = quantile(rmse, .25),
                   q3.rmse = quantile(rmse, .75),
                   median.rmse.mle = median(rmse.mle),
                   q1.rmse.mle = quantile(rmse.mle, .25),
                   q3.rmse.mle = quantile(rmse.mle, .75)) %>% 
  dplyr::ungroup() %>% 
  ggplot() +
  geom_line(aes(x = n, y = median.rmse, colour = 'Algorithm 3')) + 
  geom_errorbar(aes(x = n, ymin = q1.rmse, ymax = q3.rmse,
                    colour = 'Algorithm 3'), width = .1) + 
  geom_line(aes(x = n, y = median.rmse.mle, colour = 'MLE-based')) + 
  geom_errorbar(aes(x = n, ymin = q1.rmse.mle, ymax = q3.rmse.mle,
                   colour = 'MLE-based'), width = .1) + 
  scale_colour_brewer(palette = 'Set1') + 
  labs(y = 'RMSE', colour = NULL) + 
  facet_wrap(~ K, labeller = 'label_both') + 
  scale_y_log10() + 
  scale_x_log10(breaks = c(128, 256, 512, 1024, 2048, 4096))
```

## Additional experiments

Using the same set of parameters for generating $P$ and $A$ as in the 
balanced communities examples for $K = 2$, we generated one instance of $A$ for
each $n$ and constructed $B$ according to Algorithm 3 to verify that 
as $n \to \infty$, $(\hat{v}_i)^\top \hat{v_j} \stackrel{P}{\to} 0$ for $i, j$ 
in different clusters. Furthermore, the distribution of these inner products 
should be approximately normal. 

```{r, fig.cap = 'Between-cluster inner products of the eigenvectors of $A$ for varying sample sizes.', fig.height = 2, fig.width = 4}
densities.df <- readr::read_csv('densities.csv')

ggplot(densities.df) + 
  geom_density(aes(x = inner.prods * n, colour = factor(n))) + 
  labs(x = expression(n * v[i]^T * v[j]),
       colour = 'n') + 
  scale_x_continuous(breaks = seq(-5, 5)) + 
  scale_colour_brewer(palette = 'Set1')
```

# Real data examples

```{r}
import::here(cluster.pabm, ssc, plot.A, .from = '~/dev/pabm-grdpg/functions.R')

n.edges <- 20566

butterfly <- R.matlab::readMat('data/Raw_butterfly_network.mat')

A <- butterfly$W.butterfly0
z <- butterfly$labels

keep <- c(6, 2, 9, 4)
A <- A[z %in% keep, z %in% keep]
z <- z[z %in% keep]
n <- length(z)
K <- 4

delta <- quantile(A[upper.tri(A)], 1 - n.edges / (n * (n - 1) / 2))
A <- (A > delta) * 1

z.hat <- cluster.pabm(A, K)

# z.hat.mm <- A %>% 
#   igraph::graph_from_adjacency_matrix(mode = 'undirected') %>% 
#   igraph::cluster_louvain() %>% 
#   igraph::membership()

# ari <- round(fossil::adj.rand.index(z, z.hat) * 100)
```

In the first real data example, we applied OSE to the Leeds Butterfly 
dataset \cite{Wang_2018} consisting of visual similarity measurements among 832 
butterflies across 10 species. The graph was modified to match the example 
from \citet{noroozi2019estimation}: Only the 4 most frequent species were 
considered, and the similarities were discretized to $\{0, 1\}$ via 
thresholding. Fig. \ref{fig:butterfly} shows a sorted adjacency matrix sorted 
by the resultant clustering. 

Comparing against the ground truth species labels, OSE achieves an 
accuracy of 63\% and an adjusted Rand index of 73\%. In comparison, 
\citet{noroozi2019estimation} achieved an adjusted Rand index of 73\% using 
sparse subspace clustering on the same dataset.

```{r butterfly, fig.height = 2, fig.width = 2, fig.cap = 'Adjacency matrix of the Leeds Butterfly dataset after sorting by the clustering outputted by OSE.', cache = FALSE}
plot.A(A, z.hat)
```

```{r}
# this portion is from the code provided by Sengupta and Chen
# the data were generated by scripts from S&C, then saved as RDS files

bmp <- readRDS('data/bmp.rds')
A.bmp <- bmp$A
z.bmp <- bmp$z

pb <- readRDS('data/pb.rds')
A.pb <- pb$A
z.pb <- pb$z

dblp <- readRDS('data/dblp.rds')
A.dblp <- dblp$A
z.dblp <- dblp$z
```

```{r, cache = TRUE}
zhat.bmp <- cluster.pabm(A.bmp, 2, use.all = TRUE, normalize = FALSE)
bmp.acc <- mean(zhat.bmp == z.bmp)
bmp.acc <- ifelse(bmp.acc < .5, 1 - bmp.acc, bmp.acc)

zhat.pb <- cluster.pabm(A.pb, 2, use.all = TRUE, normalize = FALSE, 
                        p = 2, q = 0, d.eigenmap = 2)
pb.acc <- mean(zhat.pb == z.pb)
pb.acc <- ifelse(pb.acc < .5, 1 - pb.acc, pb.acc)

zhat.dblp <- cluster.pabm(A.dblp, 2, use.all = TRUE, normalize = FALSE)
dblp.acc <- mean(zhat.dblp == z.dblp)
dblp.acc <- ifelse(dblp.acc < .5, 1 - dblp.acc, dblp.acc)

zhat.bmp.ssc <- ssc(A.bmp, 2, 1e-3, normalize = TRUE)
bmp.acc.ssc <- mean(zhat.bmp.ssc == z.bmp)
bmp.acc.ssc <- ifelse(bmp.acc.ssc < .5, 1 - bmp.acc.ssc, bmp.acc.ssc)

zhat.pb.ssc <- ssc(A.pb, 2, .008, normalize = TRUE)
pb.acc.ssc <- mean(zhat.pb.ssc == z.pb)
pb.acc.ssc <- ifelse(pb.acc.ssc < .5, 1 - pb.acc.ssc, pb.acc.ssc)

zhat.dblp.ssc <- ssc(A.dblp, 2, 1e-2, normalize = FALSE)
dblp.acc.ssc <- mean(zhat.dblp.ssc == z.dblp)
dblp.acc.ssc <- ifelse(dblp.acc.ssc < .5, 1 - dblp.acc.ssc, dblp.acc.ssc)

```

In the second example, we applied OSE to the British MPs Twitter 
network \cite{greene2013producing}, the Political Blogs network 
\cite{10.1145/1134271.1134277}, and the DBLP network \cite{NIPS2009_3855} 
\cite{10.1007/978-3-642-15880-3_42}. For this data analysis, we subsetted the 
data as described by 
\citet{307cbeb9b1be48299388437423d94bf1} for their analysis of the same 
networks. Our methods underperformed compared to modularity maximization, 
although performance is comparable. In 
addition, OSE's runtime is much lower than that of modularity 
maximization.

```{r}
dplyr::tibble(Network = c('British MPs', 'Political blogs', 'DBLP'),
              Vertices = c(nrow(A.bmp), nrow(A.pb), nrow(A.dblp)),
              `Error (Mod. Max.)` = c(round(1 / 329, 3),
                                      round(61 / 1222, 3),
                                      round(62 / 2203, 3)),
              `Error (SSC)` = round(1 - c(bmp.acc.ssc,
                                          pb.acc.ssc,
                                          dblp.acc.ssc), 3),
              `Error (OSE)` = round(1 - c(bmp.acc, 
                                                  pb.acc,
                                                  dblp.acc), 3)) %>% 
  knitr::kable(caption = paste('Community detection error rates', 
                               'for modularity maximization,',
                               'sparse subspace clustering, ',
                               'and OSE.'))
```

```{r mp, cache = FALSE, fig.width = 8, fig.height = 2, fig.cap = 'Adjacency matrices of (from left to right) the British MPs, Political Blogs, and DBLP networks after sorting by the clustering outputted by OSE.', fig.dpi = 10}
gridExtra::grid.arrange(plot.A(A.bmp, zhat.bmp),
                        plot.A(A.pb, zhat.pb),
                        plot.A(A.dblp, zhat.dblp),
                        ncol = 3)
```

In the third example, we consider the Karantaka villages data studied by 
\citet{DVN/U3BIHX_2013}. For this example, we chose the `visitgo` networks from 
villages 12, 31, and 46 at the household level. The label of interest is the 
religious affiliation. The networks were truncated to religions "1" and "2", 
and vertices of degree 0 were removed. 

```{r, cache = FALSE, fig.cap = 'Adjacency matrix of the Karnataka villages data, arranged by the clustering produced by OSE (left). The villages studied here are, from left to right, 12, 31, and 46.', fig.height = 2, fig.width = 8, eval = TRUE}
K <- 2
hh.12 <- readRDS('data/village-hh-12.rds')
hh.31 <- readRDS('data/village-hh-31.rds')
hh.46 <- readRDS('data/village-hh-46.rds')

# this part of code taken from sengupta & chen
setwd('~/dev/pabm-grdpg/codes_and_data')
require(MASS);require("clue")
source("EPalgosim.R") 
source("functions.R")
mm <- function(A.hh) {
  b.can = EPalgo(A.hh,eps=0) # EP algorithm (no perturbation)
  Q.PA.can = rep(NA, ncol(b.can))	# array to store Q values
  for (i in 1:ncol(b.can)){
    #check if any cluster is empty
    foo = rep(NA, 2)
    for (clus in 1:2) {foo[clus]=sum(b.can[,i]==clus)}
    if (min(foo)==0) {stop('Empty groups are not allowed')} 
    Q.PA.can[i] = Q.PA(A.hh, b=b.can[,i])   # fit PABM
  } # end of i for loop
  foo1 = order(-Q.PA.can)[1] 
  return(b.can[,foo1])   # community assignment that maximises Q.PA
}

A.hh.12 <- hh.12$A
z.hh.12 <- hh.12$z
zhat.hh.12 <- cluster.pabm(A.hh.12, K,
                           use.all = TRUE, normalize = FALSE, p = 3, q = 1)
zhat.hh.ssc.12 <- ssc(A.hh.12, K, lambda = .1)
zhat.hh.mm.12 <- mm(A.hh.12)

A.hh.31 <- hh.31$A
z.hh.31 <- hh.31$z
zhat.hh.31 <- cluster.pabm(A.hh.31, K,
                           use.all = TRUE, normalize = FALSE, p = 3, q = 1)
zhat.hh.ssc.31 <- ssc(A.hh.31, K, lambda = .05)
zhat.hh.mm.31 <- mm(A.hh.31)

A.hh.46 <- hh.46$A
z.hh.46 <- hh.46$z
zhat.hh.46 <- cluster.pabm(A.hh.46, K,
                           use.all = TRUE, normalize = FALSE, p = 2, q = 0)
zhat.hh.ssc.46 <- ssc(A.hh.46, K, lambda = .0005)
zhat.hh.mm.46 <- mm(A.hh.46)

gridExtra::grid.arrange(plot.A(A.hh.12, zhat.hh.12),
                        plot.A(A.hh.31, zhat.hh.31),
                        plot.A(A.hh.46, zhat.hh.46),
                        nrow = 1)
```

```{r, eval = TRUE}
hh.acc.12 <- mean(zhat.hh.12 == z.hh.12)
hh.acc.12 <- ifelse(hh.acc.12 < .5, 1 - hh.acc.12, hh.acc.12)
hh.acc.ssc.12 <- mean(zhat.hh.ssc.12 == z.hh.12)
hh.acc.ssc.12 <- ifelse(hh.acc.ssc.12 < .5, 1 - hh.acc.ssc.12, hh.acc.ssc.12)
hh.acc.mm.12 <- mean(zhat.hh.mm.12 == z.hh.12)
hh.acc.mm.12 <- ifelse(hh.acc.mm.12 < .5, 1 - hh.acc.mm.12, hh.acc.mm.12)

hh.acc.31 <- mean(zhat.hh.31 == z.hh.31)
hh.acc.31 <- ifelse(hh.acc.31 < .5, 1 - hh.acc.31, hh.acc.31)
hh.acc.ssc.31 <- mean(zhat.hh.ssc.31 == z.hh.31)
hh.acc.ssc.31 <- ifelse(hh.acc.ssc.31 < .5, 1 - hh.acc.ssc.31, hh.acc.ssc.31)
hh.acc.mm.31 <- mean(zhat.hh.mm.31 == z.hh.31)
hh.acc.mm.31 <- ifelse(hh.acc.mm.31 < .5, 1 - hh.acc.mm.31, hh.acc.mm.31)

hh.acc.46 <- mean(zhat.hh.46 == z.hh.46)
hh.acc.46 <- ifelse(hh.acc.46 < .5, 1 - hh.acc.46, hh.acc.46)
hh.acc.ssc.46 <- mean(zhat.hh.ssc.46 == z.hh.46)
hh.acc.ssc.46 <- ifelse(hh.acc.ssc.46 < .5, 1 - hh.acc.ssc.46, hh.acc.ssc.46)
hh.acc.mm.46 <- mean(zhat.hh.mm.46 == z.hh.46)
hh.acc.mm.46 <- ifelse(hh.acc.mm.46 < .5, 1 - hh.acc.mm.46, hh.acc.mm.46)

knitr::kable(dplyr::tibble(
  Method = c('OSE', 'SSC', 'Mod. Max.'),
  `Error (Village 12)` = round(1 - c(hh.acc.12, hh.acc.ssc.12, hh.acc.mm.12), 3),
  `Error (Village 31)` = round(1 - c(hh.acc.31, hh.acc.ssc.31, hh.acc.mm.31), 3),
  `Error (Village 46)` = round(1 - c(hh.acc.46, hh.acc.ssc.46, hh.acc.mm.46), 3)),
  caption = paste('Community detection error rates for identifying',
                  'household religion.'))
```

# Discussion

# Proofs

**Proof of Theorem 1**. 
This is given by straightforward matrix multiplication. It suffices to show 
that 

$$X U I_{3, 1} U^\top X^\top = 
\begin{bmatrix} 
  \lambda^{(11)} (\lambda^{(11)})^\top & \lambda^{(12)} (\lambda^{(21)})^\top \\
  \lambda^{(21)} (\lambda^{(12)})^\top & \lambda^{(22)} (\lambda^{(22)})^\top
\end{bmatrix}$$

*Remark*. While we can just perform the matrix multiplication to show the 
equivalence, it is more illustrative to look at a few intermediate steps. Note 
that the product of the three inner matrices results in a permutation matrix 
with fixed points at positions $1$ and $4$ and a cycle of order 2 swappoing 
positions $2$ and $3$: 

$$U I_{3, 1} U^\top = \begin{bmatrix} 
  1 & 0 & 0 & 0 \\ 
  0 & 0 & 1 & 0 \\ 
  0 & 1 & 0 & 0 \\ 
  0 & 0 & 0 & 1 
\end{bmatrix} = \Pi$$

Since $U$ is orthonormal and $I_{3, 1}$ is diagonal, $\Pi = U I_{3, 1} U^\top$ 
is a spectral decomposition of this permutation matrix. Note that the two fixed 
points result in eigenvalues of $+1$ with corresponding eigenvectors $e_i$ 
where $i = 1, 4$ corresponding to the locations of the fixed points, and the 
cycle of order two results in two eigenvalues $\pm 1$ with corresponding 
eigenvectors $(e_i \pm e_j) / \sqrt{2}$ where $i = 2, j = 3$, pair that is 
swapped.

**Proof of Theorem 2**. 
Let 
$$\Lambda^{(k)} = 
\begin{bmatrix} \lambda^{(k,1)} & \cdots & \lambda^{(k, K)} \end{bmatrix}
\in \mathbb{R}^{n_k \times K}$$

\begin{equation}
X = \blockdiag(\Lambda^{(1)}, ..., \Lambda^{(K)}) \in \mathbb{R}^{n \times K^2}
\end{equation}

$$L^{(k)} = \blockdiag(\lambda^{(1k)}, ..., \lambda^{(Kk)}) \in 
\mathbb{R}^{n \times K}$$

$$Y = \begin{bmatrix} L^{(1)} & \cdots & L^{(K)} \end{bmatrix} \in 
\mathbb{R}^{n \times K^2}$$

Then $P = X Y^\top$.

Similar to the $K = 2$ case, we have $Y = X \Pi$ for a permutation matrix
$\Pi$, resulting in $P = X \Pi X^\top$.  
The permutation described by $\Pi$ has $K$ fixed points, which correspond to 
$K$ eigenvalues equal to $1$ with corresponding eigenvectors $e_k$ where 
$k = r (K + 1) + 1$ for $r = 0, ..., K - 1$. It also has 
$\binom{K}{2} = K (K - 1) / 2$ cycles of order $2$. Each cycle corresponds to 
a pair of eigenvalues $+1$ and $-1$ and a pair of eigenvectors 
$(e_s + e_t) / \sqrt{2}$ and $(e_s - e_t) / \sqrt{2}$.

Then $\Pi$ has $K (K + 1) / 2$ eigenvalues equal to $1$ and $K (K - 1) / 2$ 
eigenvalues equal to $-1$. $\Pi$ has the decomposed form 

\begin{equation}
\Pi = U I_{K (K + 1) / 2, K (K - 1) / 2} U^\top
\end{equation}

The edge probability matrix then can be written as:

\begin{equation}
P = X U I_{p, q} (X U)^\top
\end{equation}

\begin{equation}
p = K (K + 1) / 2
\end{equation}

\begin{equation}
q = K (K - 1) / 2
\end{equation}

and we can describe the PABM with $K$ communities as a GRDPG with latent 
positions $X U$ with signature $\big( K (K + 1) / 2, K (K - 1) / 2 \big)$.

**Lemma 1.** 
Let $P = V D V^\top$ be the spectral decomposition of the edge probability 
matrix for a PABM. Then $V V^\top = X (X^\top X)^{-1} X^\top$ where $X$ is 
defined as in equation (7).

*Proof of Lemma 1*. 
By Theorem 2, $P = X U I_{p, q} U^\top X^\top$, where $X$ is defined as in 
equation (7) and $p$ and $q$ are defined as in equations (10) and (11).
Alternatively, the spectral decomposition can be written as 
$P = V D V^\top = V |D|^{1/2} I_{p, q} |D|^{1/2} V^\top$ for the same $(p, q)$ 
and $|\cdot|^{1/2}$ is applied entry-wise. Thus for some 
$Q \in \mathbb{O}(p, q)$, 

$$X U Q = V |D|^{1/2}$$

Therefore, using the fact that $U U^\top = I$ and $V^\top V = I$,

$$(V |D|^{1/2}) ((V |D|^{1/2})^\top (V |D|^{1/2}))^{-1} (V |D|^{1/2})^\top = 
(X U Q) ((X U Q)^\top (X U Q))^{-1} (X U Q)^\top$$

The righthand side becomes

$$\begin{split}
  (X U Q) ((X U Q)^\top (X U Q))^{-1} (X U Q)^\top & 
    = X U Q Q^{-1} U^\top (X^\top X)^{-1} U (Q^\top)^{-1} Q^\top U^\top X^\top \\
  & = X U U^\top (X^\top X)^{-1} U U^\top X^\top \\
  & = X (X^\top X)^{-1} X^\top
\end{split}$$

The lefthand side becomes:

$$\begin{split}
  (V |D|^{1/2}) ((V |D|^{1/2})^\top (V |D|^{1/2}))^{-1} (V |D|^{1/2})^\top & =
    V |D|^{1/2} |D|^{-1/2} (V^\top V)^{-1} |D|^{-1/2} |D|^{1/2} V^\top \\
  & = V V^\top
\end{split}$$

**Proof of Theorem 3**.
By Lemma 1, $V V^\top = X (X^\top X)^{-1} X^\top$ where $X$ is defined as 
in Theorem 2. Since $X$ is block diagonal with each block corresponding 
to one community, $X (X^\top X)^{-1} X^\top$ is also a block diagonal matrix 
with each block corresponding to a community and zeros elsewhere. Therefore, 
if vertices $i$ and $j$ belong to different communities, then the $ij$^th^ 
element of $X (X^\top X)^{-1} X^\top = V V^\top = B$ is 0.

**Proof of Theorem 5**.

**Proof of Theorem 6**. 
Let $P$ and $A$ be organized by community such that the elements of blocks 
$P^{(kl)}$ and $A^{(kl)}$ correspond to the edges between communities $k$ and 
$l$. 

*Case $k = l$*. $P^{(kk)}$ and $A^{(kk)}$ represent within-community edge 
probabilities and edges for community $k$.  
By definition, 
$P^{(kk)} = \lambda^{(kk)} (\lambda^{(kk)})^\top$. This implies that the 
signular value decomposition 
$P^{(kk)} = \sigma_{kk}^2 u^{(kk)} (u^{(kk)})^\top$ has one singular value and 
one pair of singular vectors ($P^{(kk)}$ is symmetric, so the left and right 
singular vectors are identical). Then $\lambda^{(kk)} = \sigma_{kk} u^{(kk)}$.  
Let $\hat{U}^{(kk)} \hat{\Sigma}^{(kk)} (\hat{U}^{(kk)})^\top$ be the singular 
value decomposition of $A^{(kk)}$, and let 
$\hat{\sigma}_{kk}^2 \hat{u}^{(kk)} (\hat{u}^{(kk)})^\top$ be its 
one-dimensional approximation. Define 
$\hat{\lambda}^{(kk)} = \hat{\sigma}_{kk} \hat{u}^{(kk)}$. Then 
$\hat{\lambda}^{(kk)}$ is the adjacency spectral embedding approximation of 
$\lambda^{(kk)}$.  
Then by Theorem 5 from \citet{rubindelanchy2017statistical}, the adjacency 
spectral embedding $\hat{\lambda}^{(kk)}$ approximates $\lambda^{(kk)}$ at rate 
$\frac{(\log n_k)^c}{\sqrt{n_k}}$.

*Case $k \neq l$*. $P^{(kl)}$ and $A^{(kl)}$ represent edge probabilities and 
edges between communities $k$ and $l$. Note that $P^{(kl)} = (P^{(lk)})^\top$.  
By definition, $P^{(kl)} = \lambda^{(kl)} (\lambda^{(lk)})^\top$. As in the 
$k = l$ case, we note that the singular value decomposition 
$P^{(kl)} = \sigma_{kl}^2 u^{(kl)} (v^{(kl)})^\top$ is one-dimensional and 
$\lambda^{(kl)} = \sigma_{kl} u^{(kl)}$. (We can also note that the SVD of 
$P^{(lk)} = \sigma_{kl}^2 v^{(kl)} (u^{(kl)})^\top$, i.e., 
$\sigma_{kl} = \sigma_{lk}$, $u^{(kl)} = v^{(lk)}$, and 
$v^{(kl)} = u^{(lk)}$.)  
Now consider the Hermitian dilation 

$$M^{(kl)} = 2 \begin{bmatrix} 0 & P^{(kl)} \\ P^{(lk)} & 0 \end{bmatrix}$$

which is a symmetric $(n_k + n_l) \times (n_k + n_l)$ matrix. It can be shown 
that the eigendecomposition of $M^{(kl)}$ is

$$M^{(kl)} = 
\begin{bmatrix} u^{(kl)} & -u^{(kl)} \\ v^{(kl)} & v^{(kl)} \end{bmatrix} \times 
\begin{bmatrix} \sigma^2_{kl} & 0 \\ 0 & -\sigma^2_{kl} \end{bmatrix} \times
\begin{bmatrix} u^{(kl)} & -u^{(kl)} \\ v^{(kl)} & v^{(kl)} \end{bmatrix}^\top$$

Thus treating $M^{(kl)}$ as the edge probability matrix of a GRDPG, we have 
latent positions in $\mathbb{R}^2$ given by 

$$\begin{bmatrix} 
  \sigma_{kl} u^{(kl)} & \sigma_{kl} u^{(kl)} \\ 
  \sigma_{kl} v^{(kl)} & -\sigma_{kl} v^{(kl)} 
\end{bmatrix} = 
\begin{bmatrix} 
  \lambda^{(kl)} & \lambda^{(kl)} \\ 
  \lambda^{(lk)} & -\lambda^{(lk)} 
\end{bmatrix}$$

Now consider 

$$\hat{M}^{(kl)} = \begin{bmatrix} 0 & A^{(kl)} \\ A^{(lk)} & 0 \end{bmatrix}$$ 
Then $\hat{M}^{(kl)} = M^{(kl)} + E'$ where 

$$E' = \begin{bmatrix} 0 & E \\ E^\top & 0 \end{bmatrix}$$

and $E$ is the 
$n_k \times n_l$ matrix of independent noise (to generate the Bernoulli entries 
in $A^{(kl)}$.  
Then $\hat{M}^{(kl)}$ is an adjacency matrix drawn from $M^{(kl)}$, so its adjacency 
spectral embedding, given by 

$$\begin{bmatrix} 
  \hat{\lambda}^{(kl)} & \hat{\lambda}^{(kl)} \\ 
  \hat{\lambda}^{(lk)} & -\hat{\lambda}^{(lk)} 
\end{bmatrix}$$

where each $\hat{\lambda}^{(kl)}$ is defined as in Algorithm 3, approximates 
the latent positions of $M^{(kl)}$ up to indefinite orthogonal transformation 
by the rate given in Theorem 5 of \citet{rubindelanchy2017statistical}.  
In this case, the indefinite orthogonal transformation $W_*$ in the GRDPG 
result \cite{rubindelanchy2017statistical} is of the form 
$U^\top \hat{U}$. The eigenvalues of $M$ are distinct since the signature 
for this GRDPG is $(1, 1)$, and $U^\top \hat{U}$ is block diagonal, 
resulting in $W_* \stackrel{P}{\to} I$. Therefore, the adjacency spectral embedding of $\hat{M}^{(kl)}$ is 
a direct estimation of the specific latent positions outlined for $M^{(kl)}$, 
up to sign flip. 

# References